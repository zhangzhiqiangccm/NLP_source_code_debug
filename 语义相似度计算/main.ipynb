{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting transformers==4.0.1\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 18.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (1.19.1)\n",
      "Collecting packaging (from transformers==4.0.1)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/3c/77/e2362b676dc5008d81be423070dd9577fa03be5da2ba1105811900fda546/packaging-21.0-py3-none-any.whl (40kB)\n",
      "\u001b[K     |████████████████████████████████| 40kB 16.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (4.45.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (0.0.43)\n",
      "Collecting tokenizers==0.9.4 (from transformers==4.0.1)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/fb/36/59e4a62254c5fcb43894c6b0e9403ec6f4238cc2422a003ed2e6279a1784/tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 13.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (2020.7.14)\n",
      "Collecting filelock (from transformers==4.0.1)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (2.22.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->transformers==4.0.1) (2.4.7)\n",
      "Requirement already satisfied: click in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (7.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (0.14.1)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->transformers==4.0.1) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->transformers==4.0.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->transformers==4.0.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->transformers==4.0.1) (1.25.6)\n",
      "Installing collected packages: packaging, tokenizers, filelock, transformers\n",
      "Successfully installed filelock-3.0.12 packaging-21.0 tokenizers-0.9.4 transformers-4.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers==4.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![文本相似度](https://img-blog.csdnimg.cn/136dde6917294a3aa8200b7b853d9133.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![train.json](https://img-blog.csdnimg.cn/img_convert/d0484fd918be5f239de10499fb07879c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![test.json](https://img-blog.csdnimg.cn/img_convert/84e411156968837c4071bf0f22a76c2b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "def read_data(path):\n",
    "    sentence_a = []\n",
    "    sentence_b = []\n",
    "    labels = []\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in tqdm(f.readlines(), desc='Reading data'):\n",
    "            line = json.loads(line)\n",
    "            sentence_a.append(line['sentence1'])\n",
    "            sentence_b.append(line['sentence2'])\n",
    "            labels.append(int(line['label']))\n",
    "    \n",
    "    df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns=['text_a', 'text_b', 'labels'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading data: 100%|██████████| 34334/34334 [00:00<00:00, 239315.65it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = read_data('data/data100821/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>蚂蚁借呗等额还款可以换成先息后本吗</td>\n",
       "      <td>借呗有先息到期还本吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>蚂蚁花呗说我违约一次</td>\n",
       "      <td>蚂蚁花呗违约行为是什么</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>帮我看一下本月花呗账单有没有结清</td>\n",
       "      <td>下月花呗账单</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>蚂蚁借呗多长时间综合评估一次</td>\n",
       "      <td>借呗得评估多久</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我的花呗账单是***，还款怎么是***</td>\n",
       "      <td>我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>蚂蚁借呗的额度可以从申请不</td>\n",
       "      <td>蚂蚁借呗节假日可以借款吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>商家怎么开通花呗支付</td>\n",
       "      <td>为什么无法开通花呗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>用花呗怎么买不了车票</td>\n",
       "      <td>我是问用蚂蚁花呗，为什么买不了火车票</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>如升级为网商贷用户而借呗的欠款怎么还</td>\n",
       "      <td>我借呗已还清，为什么升级不了网商贷</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>花呗不消费有没有年费</td>\n",
       "      <td>花呗不用就不会产生费用</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>支付宝系统点我的里面没有花呗这一项</td>\n",
       "      <td>我下载支付宝怎么没有花呗的</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>用花呗淘宝购物，花呗已经还清了，但是东西不想要申请退款了，钱会退到哪去</td>\n",
       "      <td>在淘宝买衣服用花呗支付 然后申请退款了 这笔钱退到哪里</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>花呗标示的商品</td>\n",
       "      <td>花呗识别的商品</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>花呗自动从余额宝扣款，需要我自己设置吗</td>\n",
       "      <td>支付宝余额会自动还花呗吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>花呗提前结清会影响使用吗</td>\n",
       "      <td>我的花呗分期不能提前结清</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>花呗逾期一天怎么办</td>\n",
       "      <td>花呗逾期一天会影响芝麻信用吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>花呗消费超过额度有什么影响吗</td>\n",
       "      <td>花呗额度成负数有啥影响吗</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>还款还清了，为什么花呗账单显示还要还款</td>\n",
       "      <td>花呗全额还清怎么显示没有还款</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>我的花呗背限额了吗</td>\n",
       "      <td>花呗临时额度有限额吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>花呗为什么会几个月没有提升额度</td>\n",
       "      <td>我这个支付宝怎么没有花呗额度</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>花呗一次性付款有限制吗</td>\n",
       "      <td>解除花呗支付限制</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>为什么我开通不了花呗</td>\n",
       "      <td>我一直想买苹果***p，没钱，想分期付款，除了花呗，还有什么可以</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>可以用其他的银行卡还蚂蚁借呗的钱吗</td>\n",
       "      <td>蚂蚁借呗昨天借的钱今天就到还款日</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>花呗分期还款会影响芝麻信用分吗</td>\n",
       "      <td>花呗额度为付影响信用度吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>花呗不可以用密码开通吗</td>\n",
       "      <td>花呗可以用银行卡开通吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>花呗分期提前还款要滞纳金吗 手续费怎么算</td>\n",
       "      <td>花呗分期退款手续费</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>手机关闭花宝贝怎么做</td>\n",
       "      <td>怎样扫描手机支付宝</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>港澳人士能用花呗吗</td>\n",
       "      <td>港澳通行证可以用花呗吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>借呗周期有***个月吗</td>\n",
       "      <td>借呗之前最长不是***个月吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>借呗一般是多久评估</td>\n",
       "      <td>大概多久评估一次借呗条件</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>么，这个是不是我现在不能不能用花呗</td>\n",
       "      <td>我的花呗不能用了是吧</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>这个蚂蚁花呗能恢复正常用不</td>\n",
       "      <td>我的蚂蚁花呗 怎么用不了</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>蚂蚁借呗提前还分期提前还一部分，超过分期，不够欠的总额</td>\n",
       "      <td>蚂蚁借呗可以分期借不</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>把你们花呗的电话给我</td>\n",
       "      <td>给花呗打电话</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>为什么我的花呗不能买车险</td>\n",
       "      <td>我在花呗买的车险，为什么不送单</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>要是不用花呗 还需要还吗</td>\n",
       "      <td>不用花呗里面的钱 是不是就可以不还款了</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>当前交易不支持花呗付款怎么回事</td>\n",
       "      <td>该付款不支持花呗是什么意思</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>花呗分期是从什么时候开始</td>\n",
       "      <td>花呗分期的钱什么时候扣除</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>我的有花呗额度挺多</td>\n",
       "      <td>我的花呗总额度是不是降下来了</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>花呗收款开通报错</td>\n",
       "      <td>商家用一个营业执照可以开通几个花呗收款</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>我之前手机绑定的支付宝账户，用的花呗，现在手机号码已经不用了，就是我手机号码，现在解绑可以解吗</td>\n",
       "      <td>个人申请的支付宝证号可以申请开通花呗和信用卡收款码的</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>我昨天用花呗付了小黄车定金，退回来了</td>\n",
       "      <td>现在花呗能付小黄车押金</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>我的花呗是另外一个手机号，但现在手机号不用了</td>\n",
       "      <td>我换手机号了，花呗怎么办</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>借呗信用到多少可以借款</td>\n",
       "      <td>达到多少额度可以申请借呗</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>昨天晚上借呗借了***，怎么还没到账</td>\n",
       "      <td>申请的蚂蚁借呗怎么还没到账了</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>网商贷换回借呗贷……晕</td>\n",
       "      <td>网商贷要怎么换借呗</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>账单都还清楚了！怎么花呗不能用了</td>\n",
       "      <td>为什么我的花呗还款后还不能用</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>我能查看用花呗买什么东西吗</td>\n",
       "      <td>花呗都能购买那些东西</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>花呗分期提前还完不可以吗</td>\n",
       "      <td>花呗分期能提前全部还清</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>花呗不是申请的吗，是系统自动开通的</td>\n",
       "      <td>花呗是不能关闭么</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_a  \\\n",
       "0                                 蚂蚁借呗等额还款可以换成先息后本吗   \n",
       "1                                        蚂蚁花呗说我违约一次   \n",
       "2                                  帮我看一下本月花呗账单有没有结清   \n",
       "3                                    蚂蚁借呗多长时间综合评估一次   \n",
       "4                               我的花呗账单是***，还款怎么是***   \n",
       "5                                     蚂蚁借呗的额度可以从申请不   \n",
       "6                                        商家怎么开通花呗支付   \n",
       "7                                        用花呗怎么买不了车票   \n",
       "8                                如升级为网商贷用户而借呗的欠款怎么还   \n",
       "9                                        花呗不消费有没有年费   \n",
       "10                                支付宝系统点我的里面没有花呗这一项   \n",
       "11              用花呗淘宝购物，花呗已经还清了，但是东西不想要申请退款了，钱会退到哪去   \n",
       "12                                          花呗标示的商品   \n",
       "13                              花呗自动从余额宝扣款，需要我自己设置吗   \n",
       "14                                     花呗提前结清会影响使用吗   \n",
       "15                                        花呗逾期一天怎么办   \n",
       "16                                   花呗消费超过额度有什么影响吗   \n",
       "17                              还款还清了，为什么花呗账单显示还要还款   \n",
       "18                                        我的花呗背限额了吗   \n",
       "19                                  花呗为什么会几个月没有提升额度   \n",
       "20                                      花呗一次性付款有限制吗   \n",
       "21                                       为什么我开通不了花呗   \n",
       "22                                可以用其他的银行卡还蚂蚁借呗的钱吗   \n",
       "23                                  花呗分期还款会影响芝麻信用分吗   \n",
       "24                                      花呗不可以用密码开通吗   \n",
       "25                             花呗分期提前还款要滞纳金吗 手续费怎么算   \n",
       "26                                       手机关闭花宝贝怎么做   \n",
       "27                                        港澳人士能用花呗吗   \n",
       "28                                      借呗周期有***个月吗   \n",
       "29                                        借呗一般是多久评估   \n",
       "30                                么，这个是不是我现在不能不能用花呗   \n",
       "31                                    这个蚂蚁花呗能恢复正常用不   \n",
       "32                      蚂蚁借呗提前还分期提前还一部分，超过分期，不够欠的总额   \n",
       "33                                       把你们花呗的电话给我   \n",
       "34                                     为什么我的花呗不能买车险   \n",
       "35                                     要是不用花呗 还需要还吗   \n",
       "36                                  当前交易不支持花呗付款怎么回事   \n",
       "37                                     花呗分期是从什么时候开始   \n",
       "38                                        我的有花呗额度挺多   \n",
       "39                                         花呗收款开通报错   \n",
       "40  我之前手机绑定的支付宝账户，用的花呗，现在手机号码已经不用了，就是我手机号码，现在解绑可以解吗   \n",
       "41                               我昨天用花呗付了小黄车定金，退回来了   \n",
       "42                           我的花呗是另外一个手机号，但现在手机号不用了   \n",
       "43                                      借呗信用到多少可以借款   \n",
       "44                               昨天晚上借呗借了***，怎么还没到账   \n",
       "45                                      网商贷换回借呗贷……晕   \n",
       "46                                 账单都还清楚了！怎么花呗不能用了   \n",
       "47                                    我能查看用花呗买什么东西吗   \n",
       "48                                     花呗分期提前还完不可以吗   \n",
       "49                                花呗不是申请的吗，是系统自动开通的   \n",
       "\n",
       "                                   text_b  labels  \n",
       "0                              借呗有先息到期还本吗       0  \n",
       "1                             蚂蚁花呗违约行为是什么       0  \n",
       "2                                  下月花呗账单       0  \n",
       "3                                 借呗得评估多久       0  \n",
       "4   我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元       1  \n",
       "5                            蚂蚁借呗节假日可以借款吗       0  \n",
       "6                               为什么无法开通花呗       0  \n",
       "7                      我是问用蚂蚁花呗，为什么买不了火车票       0  \n",
       "8                       我借呗已还清，为什么升级不了网商贷       0  \n",
       "9                             花呗不用就不会产生费用       0  \n",
       "10                          我下载支付宝怎么没有花呗的       1  \n",
       "11            在淘宝买衣服用花呗支付 然后申请退款了 这笔钱退到哪里       0  \n",
       "12                                花呗识别的商品       1  \n",
       "13                           支付宝余额会自动还花呗吗       0  \n",
       "14                           我的花呗分期不能提前结清       0  \n",
       "15                         花呗逾期一天会影响芝麻信用吗       0  \n",
       "16                           花呗额度成负数有啥影响吗       1  \n",
       "17                         花呗全额还清怎么显示没有还款       1  \n",
       "18                             花呗临时额度有限额吗       0  \n",
       "19                         我这个支付宝怎么没有花呗额度       0  \n",
       "20                               解除花呗支付限制       0  \n",
       "21       我一直想买苹果***p，没钱，想分期付款，除了花呗，还有什么可以       0  \n",
       "22                       蚂蚁借呗昨天借的钱今天就到还款日       0  \n",
       "23                           花呗额度为付影响信用度吗       0  \n",
       "24                            花呗可以用银行卡开通吗       0  \n",
       "25                              花呗分期退款手续费       0  \n",
       "26                              怎样扫描手机支付宝       0  \n",
       "27                            港澳通行证可以用花呗吗       0  \n",
       "28                         借呗之前最长不是***个月吗       0  \n",
       "29                           大概多久评估一次借呗条件       1  \n",
       "30                             我的花呗不能用了是吧       1  \n",
       "31                           我的蚂蚁花呗 怎么用不了       0  \n",
       "32                             蚂蚁借呗可以分期借不       0  \n",
       "33                                 给花呗打电话       1  \n",
       "34                        我在花呗买的车险，为什么不送单       0  \n",
       "35                    不用花呗里面的钱 是不是就可以不还款了       1  \n",
       "36                          该付款不支持花呗是什么意思       1  \n",
       "37                           花呗分期的钱什么时候扣除       1  \n",
       "38                         我的花呗总额度是不是降下来了       0  \n",
       "39                    商家用一个营业执照可以开通几个花呗收款       0  \n",
       "40             个人申请的支付宝证号可以申请开通花呗和信用卡收款码的       0  \n",
       "41                            现在花呗能付小黄车押金       0  \n",
       "42                           我换手机号了，花呗怎么办       0  \n",
       "43                           达到多少额度可以申请借呗       1  \n",
       "44                         申请的蚂蚁借呗怎么还没到账了       1  \n",
       "45                              网商贷要怎么换借呗       1  \n",
       "46                         为什么我的花呗还款后还不能用       1  \n",
       "47                             花呗都能购买那些东西       1  \n",
       "48                            花呗分期能提前全部还清       1  \n",
       "49                               花呗是不能关闭么       0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading data: 100%|██████████| 4316/4316 [00:00<00:00, 257307.56it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_df = read_data('data/data100821/dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_df text_a\n",
    "(train_df.text_a.str.len() + train_df.text_b.str.len()).hist(bins=20);\n",
    "(dev_df.text_a.str.len() + dev_df.text_b.str.len()).hist(bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 64能容纳99%的句子长度\n",
    "(train_df.text_a.str.len() + train_df.text_b.str.len()).quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "config = {\n",
    "        'train_file_path': 'data/data100821/train.json',\n",
    "        'dev_file_path': 'data/data100821/dev.json',\n",
    "        'test_file_path': 'data/data100821/test.json',\n",
    "        'embedding_file_path': 'data/data100821/sgns.weibo.word.bz2',\n",
    "        'train_val_ratio': 0.1,\n",
    "        'vocab_size': 30000,\n",
    "        'batch_size': 64,\n",
    "        'max_seq_len':64,\n",
    "        'num_epochs': 1,\n",
    "        'learning_rate': 1e-3,\n",
    "        'device': 'cpu',\n",
    "        'logging_step': 200,\n",
    "        'seed': 2021\n",
    "    } \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    config['device'] = 'cuda'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 处理文件，生层词表放回关于train_df[train.json + dev.json],test_df\n",
    "from collections import Counter\n",
    "import jieba\n",
    "import bz2\n",
    "def preprocess(config):\n",
    "    def convert2df(file_path, dataset='train'):\n",
    "        sentence_a = []\n",
    "        sentence_b = []\n",
    "        labels = []\n",
    "        with open(file_path, 'r', encoding='utf8') as f:\n",
    "            for line in tqdm(f.readlines(), desc=f'Reading {dataset} data'):\n",
    "                line = json.loads(line)\n",
    "                sentence_a.append(line['sentence1'])\n",
    "                sentence_b.append(line['sentence2'])\n",
    "                if dataset != 'test':\n",
    "                    labels.append(int(line['label']))\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "\n",
    "                tokens = list(jieba.cut(sentence_a[-1])) + list(jieba.cut(sentence_b[-1]))\n",
    "                token_counter.update(tokens)\n",
    "        df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns=['text_a', 'text_b', 'labels'])\n",
    "        return df\n",
    "    \n",
    "    token_counter = Counter()\n",
    "\n",
    "    train_df = convert2df(config['train_file_path'], 'train')\n",
    "    dev_df = convert2df(config['dev_file_path'], 'dev')\n",
    "    test_df = convert2df(config['test_file_path'], 'test')\n",
    "\n",
    "    train_df = train_df.append(dev_df)\n",
    "    vocab = set(token for token, _ in token_counter.most_common(config['vocab_size']))\n",
    "    return train_df, test_df, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading train data:   0%|          | 0/34334 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.863 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Reading train data: 100%|██████████| 34334/34334 [00:06<00:00, 5333.41it/s]\n",
      "Reading dev data: 100%|██████████| 4316/4316 [00:00<00:00, 6108.27it/s]\n",
      "Reading test data: 100%|██████████| 3861/3861 [00:00<00:00, 6193.95it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, vocab = preprocess(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " '/',\r",
       " '半',\r",
       " '无发',\r",
       " '有借有还',\r",
       " '锁',\r",
       " '壮态',\r",
       " '能花',\r",
       " '保持',\r",
       " '哈',\r",
       " '不选',\r",
       " '赌博',\r",
       " '成负会',\r",
       " '钱要',\r",
       " '王者',\r",
       " '登过',\r",
       " '跳过',\r",
       " '晚',\r",
       " '挖宝',\r",
       " '回比',\r",
       " '意在',\r",
       " '能升',\r",
       " '会变',\r",
       " '考拉',\r",
       " '十一点',\r",
       " \"'\",\r",
       " '这张',\r",
       " '商',\r",
       " '妹',\r",
       " '仟',\r",
       " '切换',\r",
       " '八千',\r",
       " '察花',\r",
       " '将来',\r",
       " '全款',\r",
       " '年花',\r",
       " '人口',\r",
       " '出借',\r",
       " '帮网',\r",
       " '没法用',\r",
       " '森林',\r",
       " '人民',\r",
       " '淘气',\r",
       " '买不到',\r",
       " '完奖',\r",
       " '按花',\r",
       " '到货',\r",
       " '不敢',\r",
       " '天用',\r",
       " '发校',\r",
       " '借后',\r",
       " '原样',\r",
       " '多一分',\r",
       " '$',\r",
       " '承担',\r",
       " '未能',\r",
       " '更欢',\r",
       " '预先',\r",
       " '三千',\r",
       " 'zf',\r",
       " 'c',\r",
       " '账户',\r",
       " '唯品',\r",
       " '跳',\r",
       " '几多',\r",
       " '首单',\r",
       " '撒换',\r",
       " '生活圈',\r",
       " '提上去',\r",
       " '核身',\r",
       " '哪时',\r",
       " '最短',\r",
       " '白屏',\r",
       " '最少',\r",
       " '无语',\r",
       " '办法',\r",
       " '纹身',\r",
       " '见长',\r",
       " '七十',\r",
       " '买手机',\r",
       " '查有',\r",
       " '还进',\r",
       " '几年',\r",
       " '爱人',\r",
       " '及',\r",
       " '药店',\r",
       " '返佣',\r",
       " '其',\r",
       " '日交',\r",
       " '咋成',\r",
       " '公用',\r",
       " '三十块',\r",
       " '先帮',\r",
       " '我邦定',\r",
       " '通花唄',\r",
       " '张券',\r",
       " '唱歌',\r",
       " '合约',\r",
       " '改完',\r",
       " '无奈',\r",
       " '让我拿',\r",
       " '情形',\r",
       " '代发',\r",
       " '得到',\r",
       " '封',\r",
       " '譬如说',\r",
       " '没掉',\r",
       " '单独',\r",
       " '佰迪乐',\r",
       " '不下',\r",
       " '被关',\r",
       " '对象',\r",
       " '透额',\r",
       " '借借',\r",
       " '日出',\r",
       " '能先',\r",
       " '反正',\r",
       " '越',\r",
       " '消掉',\r",
       " '还降',\r",
       " '没用',\r",
       " '支护',\r",
       " '讲网',\r",
       " '能设',\r",
       " '这会',\r",
       " '车用',\r",
       " '退',\r",
       " '失约',\r",
       " '网校',\r",
       " '时侯',\r",
       " '扣押',\r",
       " '之间',\r",
       " '开个',\r",
       " '问下',\r",
       " '查不出',\r",
       " '行驶证',\r",
       " '码有',\r",
       " '全收',\r",
       " '码且',\r",
       " '货未',\r",
       " '一二点',\r",
       " '二月',\r",
       " '越用越少',\r",
       " '或是',\r",
       " 'v',\r",
       " '减钱',\r",
       " '兑',\r",
       " '用续',\r",
       " '有钱人',\r",
       " '申通',\r",
       " '春节',\r",
       " '受到',\r",
       " '中等',\r",
       " '冻解',\r",
       " '最高值',\r",
       " '作预',\r",
       " '平台',\r",
       " '率',\r",
       " '一不小心',\r",
       " '大概',\r",
       " '消',\r",
       " '把款',\r",
       " '裤子',\r",
       " '坑',\r",
       " '准备',\r",
       " '依赖',\r",
       " '啥时候',\r",
       " '半个',\r",
       " '不有',\r",
       " '怎去',\r",
       " '限额',\r",
       " '用花唄',\r",
       " '活跃度',\r",
       " '该乡',\r",
       " '短息',\r",
       " '收入',\r",
       " '梦',\r",
       " '用户名',\r",
       " '交電費',\r",
       " '跟花',\r",
       " '～',\r",
       " '上个月',\r",
       " '当花',\r",
       " '内要',\r",
       " '花闭',\r",
       " '几率',\r",
       " '182',\r",
       " '居民',\r",
       " '二满',\r",
       " '期至',\r",
       " '来用',\r",
       " '用后',\r",
       " '（',\r",
       " '煤',\r",
       " '氣費',\r",
       " '三十多块',\r",
       " '提下花',\r",
       " '决定',\r",
       " '存卡能',\r",
       " '医药费',\r",
       " '达买',\r",
       " '专',\r",
       " '计费',\r",
       " '绑定',\r",
       " '转让',\r",
       " '余生',\r",
       " '只分',\r",
       " '发票',\r",
       " '某',\r",
       " '捆绑',\r",
       " '双倍',\r",
       " '那笔',\r",
       " '可不可以',\r",
       " '十几元',\r",
       " '全是',\r",
       " '献爱心',\r",
       " '找个',\r",
       " '微钱',\r",
       " '挨',\r",
       " '骗',\r",
       " '借备',\r",
       " '对多花',\r",
       " '妨碍',\r",
       " '借过',\r",
       " '卖花',\r",
       " '手头',\r",
       " '扣出',\r",
       " '换为',\r",
       " '授权',\r",
       " '片',\r",
       " '货品',\r",
       " '过大',\r",
       " '乱',\r",
       " '日未',\r",
       " '个人信用',\r",
       " '几种',\r",
       " '弃用',\r",
       " '史用',\r",
       " '有款',\r",
       " '互通',\r",
       " '更改',\r",
       " '放贷',\r",
       " '五百四十',\r",
       " '买件',\r",
       " '办过',\r",
       " '常熟市',\r",
       " '一百元',\r",
       " '贷款额度',\r",
       " '莪',\r",
       " '提提',\r",
       " '司法',\r",
       " '它',\r",
       " '说代',\r",
       " '请求',\r",
       " '用光',\r",
       " '返利',\r",
       " '开来',\r",
       " '个人资料',\r",
       " '已换',\r",
       " '券包',\r",
       " '买过',\r",
       " '几分钟',\r",
       " '咋会',\r",
       " '没期',\r",
       " '不受',\r",
       " '权是',\r",
       " '嗳',\r",
       " '吃饭',\r",
       " '利是',\r",
       " '空号',\r",
       " '发花',\r",
       " '没够',\r",
       " '一同',\r",
       " '现息',\r",
       " '先结',\r",
       " '扩大',\r",
       " '成长',\r",
       " '专户',\r",
       " '．',\r",
       " '未买',\r",
       " '宝里',\r",
       " '票价',\r",
       " '转接',\r",
       " '跳高',\r",
       " '173',\r",
       " '没额额',\r",
       " '以上',\r",
       " '团团',\r",
       " '定时',\r",
       " '知不知道',\r",
       " '不能',\r",
       " '吃完饭',\r",
       " '比如',\r",
       " '13',\r",
       " '数是',\r",
       " '重花',\r",
       " '一并',\r",
       " '办证',\r",
       " '餐饮店',\r",
       " '有笔',\r",
       " '登号',\r",
       " '农商',\r",
       " '该期',\r",
       " '授信',\r",
       " '不取',\r",
       " '有个',\r",
       " '开销',\r",
       " '越短',\r",
       " '天猫付',\r",
       " '时说',\r",
       " '核对',\r",
       " '比例',\r",
       " '送人',\r",
       " '却是',\r",
       " '问下会',\r",
       " '神奇',\r",
       " '超期',\r",
       " '第八期',\r",
       " '乛',\r",
       " '土沟',\r",
       " '率利',\r",
       " 's',\r",
       " '乐换',\r",
       " '借条',\r",
       " '才刚',\r",
       " '连贯',\r",
       " '我',\r",
       " '打车',\r",
       " '偶想',\r",
       " '限制',\r",
       " '贷过',\r",
       " '排队',\r",
       " '更变',\r",
       " '火车站',\r",
       " '跪求',\r",
       " '原始',\r",
       " '天弘',\r",
       " '本身',\r",
       " '儿',\r",
       " '过开',\r",
       " '收钱',\r",
       " '几千块',\r",
       " '变得',\r",
       " '租赁',\r",
       " '完是',\r",
       " '能查',\r",
       " '券能',\r",
       " '码里花',\r",
       " '部分',\r",
       " '宝没放',\r",
       " '时用',\r",
       " '界定',\r",
       " '以后',\r",
       " '先用',\r",
       " '登录',\r",
       " '装后',\r",
       " 'huab',\r",
       " '高中',\r",
       " '路径',\r",
       " '没过',\r",
       " '施华洛',\r",
       " '境外',\r",
       " '码花',\r",
       " '二唯码',\r",
       " '玩',\r",
       " '举报',\r",
       " '随着',\r",
       " '中途',\r",
       " '形象',\r",
       " '上下',\r",
       " '账到',\r",
       " '开密付',\r",
       " '如禾',\r",
       " '一般',\r",
       " '一百块',\r",
       " '公开',\r",
       " '有利',\r",
       " '骑',\r",
       " '解开',\r",
       " '至少',\r",
       " '费费',\r",
       " '提取',\r",
       " '一分',\r",
       " '优先',\r",
       " '球',\r",
       " '我拿花',\r",
       " '粉丝',\r",
       " '提不上',\r",
       " '成网',\r",
       " '手里',\r",
       " '物品',\r",
       " '透',\r",
       " '相电',\r",
       " '调回去',\r",
       " '款己',\r",
       " '我行',\r",
       " '充优酷',\r",
       " '家装',\r",
       " '如才',\r",
       " '抵付',\r",
       " '刷新',\r",
       " '信用度',\r",
       " '问题',\r",
       " '自助',\r",
       " '大型',\r",
       " '会花',\r",
       " '普通',\r",
       " '三四天',\r",
       " '某些',\r",
       " '开错',\r",
       " '我要',\r",
       " '上卡',\r",
       " '帮帮',\r",
       " '点解',\r",
       " '疯狂',\r",
       " '周卡',\r",
       " '定票',\r",
       " '三笔',\r",
       " '连接',\r",
       " '台胞证',\r",
       " '能密付',\r",
       " '乐租',\r",
       " '宝跟花',\r",
       " '划走',\r",
       " '我连点',\r",
       " '实名制',\r",
       " '撤',\r",
       " '降額',\r",
       " '初始化',\r",
       " '费',\r",
       " '几次',\r",
       " '享用',\r",
       " '换货',\r",
       " '积分换',\r",
       " '打不开',\r",
       " '国外',\r",
       " '菜单',\r",
       " '六十岁',\r",
       " '权',\r",
       " '迟俩',\r",
       " '先查',\r",
       " '喊花',\r",
       " '代付',\r",
       " '变更',\r",
       " '多日',\r",
       " '装修',\r",
       " '通',\r",
       " '上少',\r",
       " '不加',\r",
       " '购物车',\r",
       " '拼',\r",
       " '日用',\r",
       " '刮奖',\r",
       " '白猫',\r",
       " '有算',\r",
       " '地',\r",
       " '实用',\r",
       " '不花用',\r",
       " '恶意',\r",
       " '现要',\r",
       " '整错',\r",
       " '令',\r",
       " '钱表',\r",
       " '全新',\r",
       " '第二个',\r",
       " '一进',\r",
       " '问用',\r",
       " '还会少',\r",
       " '套装',\r",
       " '慢点',\r",
       " '实际上',\r",
       " '跟主',\r",
       " '以外',\r",
       " '恢复正常',\r",
       " '改价后',\r",
       " '猫',\r",
       " '开了个',\r",
       " '越大',\r",
       " '迟',\r",
       " '从摩拜',\r",
       " '领取',\r",
       " '少退',\r",
       " '自动',\r",
       " '订期',\r",
       " '看满',\r",
       " '钻石',\r",
       " '排',\r",
       " '收集',\r",
       " '地铁',\r",
       " '索要',\r",
       " '签收',\r",
       " '好几遍',\r",
       " '最迟',\r",
       " 'hahaha',\r",
       " '不论',\r",
       " '好乐迪',\r",
       " '二维',\r",
       " '压',\r",
       " '买房子',\r",
       " '回事',\r",
       " 'qqcom',\r",
       " '点卷',\r",
       " '无言',\r",
       " '骗钱',\r",
       " '挺升',\r",
       " '就',\r",
       " '解控',\r",
       " '他们',\r",
       " '宁',\r",
       " 'tm',\r",
       " '有返',\r",
       " '抵用券',\r",
       " '酒店',\r",
       " '十几块',\r",
       " '可到',\r",
       " '不认',\r",
       " '四万',\r",
       " '沃尔玛',\r",
       " '瑜伽',\r",
       " '试运行',\r",
       " '升回去',\r",
       " '调回来',\r",
       " '苹果',\r",
       " '转给',\r",
       " '而是',\r",
       " '乱来',\r",
       " '转入',\r",
       " 'huo',\r",
       " '四月',\r",
       " '或换',\r",
       " '密码锁',\r",
       " '仅',\r",
       " '卡咯',\r",
       " '要分',\r",
       " '同等',\r",
       " '负有',\r",
       " '评分标准',\r",
       " '有效',\r",
       " '没长',\r",
       " '次信',\r",
       " '完善',\r",
       " '搁',\r",
       " '非复',\r",
       " '除外',\r",
       " '创业基金',\r",
       " '跳不出',\r",
       " '都还没',\r",
       " '名额',\r",
       " '我领',\r",
       " '随便',\r",
       " '一想',\r",
       " '我务点',\r",
       " '容易',\r",
       " '选呀',\r",
       " '笔回',\r",
       " '混付',\r",
       " '巴士',\r",
       " '八个',\r",
       " '至花',\r",
       " '增高',\r",
       " '删不掉',\r",
       " '没关系',\r",
       " '整',\r",
       " '担是',\r",
       " '一号',\r",
       " '安',\r",
       " '印象',\r",
       " '查以',\r",
       " '我姐',\r",
       " '已满',\r",
       " '想',\r",
       " '贝壳',\r",
       " '礼包',\r",
       " '结款',\r",
       " '开奖',\r",
       " '贷借',\r",
       " '厂商',\r",
       " '找下',\r",
       " '看用',\r",
       " '相同',\r",
       " '点过',\r",
       " '个',\r",
       " '辛苦',\r",
       " 'shi',\r",
       " '二手',\r",
       " '减了',\r",
       " '黄',\r",
       " '什我花',\r",
       " '哪发',\r",
       " '意',\r",
       " '太紧',\r",
       " '多花',\r",
       " '取出',\r",
       " '调',\r",
       " '一条',\r",
       " '结清',\r",
       " '码收花',\r",
       " '量',\r",
       " '但',\r",
       " '会进',\r",
       " '木有',\r",
       " '使用率',\r",
       " '放花',\r",
       " '波动',\r",
       " '一个月',\r",
       " '在建',\r",
       " '已用',\r",
       " '放宽',\r",
       " '出现',\r",
       " '姐花',\r",
       " '≥',\r",
       " '开得',\r",
       " '体到',\r",
       " '没交',\r",
       " '甚么',\r",
       " '要用',\r",
       " '请款',\r",
       " '法律承认',\r",
       " '单立',\r",
       " '消花',\r",
       " '里',\r",
       " '经常',\r",
       " '全部一次',\r",
       " 'a',\r",
       " '近期',\r",
       " '到子',\r",
       " '收到',\r",
       " '提生',\r",
       " '单集',\r",
       " '点券',\r",
       " '多借',\r",
       " '最低',\r",
       " '提伸',\r",
       " '至今',\r",
       " '我信',\r",
       " '会降',\r",
       " '折能',\r",
       " '这个',\r",
       " '纸花',\r",
       " '刚好',\r",
       " '样能',\r",
       " '干通',\r",
       " '三万元',\r",
       " '余下',\r",
       " '耽误',\r",
       " '确要',\r",
       " '会员',\r",
       " '老刷',\r",
       " '本息',\r",
       " '钱后',\r",
       " '五年',\r",
       " '工作日',\r",
       " '结开',\r",
       " '降级',\r",
       " '元花',\r",
       " '银泰包',\r",
       " '会开',\r",
       " '填行成',\r",
       " '合钱',\r",
       " '具体',\r",
       " '日借',\r",
       " '请以',\r",
       " '咋天',\r",
       " '九九',\r",
       " '汇花',\r",
       " '贴码',\r",
       " '借货',\r",
       " '期要',\r",
       " '车票',\r",
       " '钱够',\r",
       " '还好',\r",
       " '因',\r",
       " '版',\r",
       " '充了',\r",
       " '多收',\r",
       " '分能',\r",
       " '为啥',\r",
       " '负额',\r",
       " '龄',\r",
       " '现示',\r",
       " '能差',\r",
       " '结洼',\r",
       " '日时',\r",
       " '微信里',\r",
       " '作为',\r",
       " '给付',\r",
       " '时花',\r",
       " '有效期',\r",
       " '戚用',\r",
       " '升花',\r",
       " '国际',\r",
       " '黄车',\r",
       " '记错',\r",
       " '新闻',\r",
       " '次月免',\r",
       " '免期',\r",
       " '花超',\r",
       " '手上',\r",
       " '玩意儿',\r",
       " '臨時',\r",
       " '网页',\r",
       " '提款',\r",
       " '流程',\r",
       " '火锅',\r",
       " '占',\r",
       " '确退',\r",
       " '方或式',\r",
       " '便',\r",
       " '分号',\r",
       " '信任度',\r",
       " '一万八',\r",
       " '情圈',\r",
       " '张',\r",
       " '小说',\r",
       " '期待',\r",
       " '金币',\r",
       " '花罗',\r",
       " '靠',\r",
       " '借下',\r",
       " '核实',\r",
       " '已开',\r",
       " '孩子',\r",
       " '单价',\r",
       " '分对',\r",
       " '停花',\r",
       " '飞机票',\r",
       " '开户',\r",
       " '包不',\r",
       " '押金',\r",
       " '十几分钟',\r",
       " '查询',\r",
       " '何为',\r",
       " '街',\r",
       " '交付',\r",
       " '万达',\r",
       " '等于',\r",
       " '什样',\r",
       " '多会涨',\r",
       " '已清',\r",
       " '一两天',\r",
       " '丶',\r",
       " '地址',\r",
       " '昨天晚上',\r",
       " '额是',\r",
       " '太长',\r",
       " '不知情',\r",
       " '福卡',\r",
       " '钱划',\r",
       " '玩意',\r",
       " '半月',\r",
       " '相差',\r",
       " '事能',\r",
       " '设备',\r",
       " '用不着',\r",
       " '泥',\r",
       " '帮人',\r",
       " '里边儿',\r",
       " '手花',\r",
       " '租错',\r",
       " '集分',\r",
       " '信用',\r",
       " '时选',\r",
       " '宝就会',\r",
       " '奇怪',\r",
       " '好换',\r",
       " '蚂仪',\r",
       " '改嘛',\r",
       " '领',\r",
       " '转盘',\r",
       " '先',\r",
       " '没充',\r",
       " '以致',\r",
       " '提把',\r",
       " '我试',\r",
       " '点多',\r",
       " '今天下午',\r",
       " '整么关',\r",
       " '后百变',\r",
       " '网点',\r",
       " '先要',\r",
       " '延迟',\r",
       " '用不来',\r",
       " '贷好',\r",
       " '应响',\r",
       " '脸通',\r",
       " '这里',\r",
       " '养殖',\r",
       " '月初',\r",
       " '除夕',\r",
       " '两千多元',\r",
       " '管它',\r",
       " '合为',\r",
       " '九',\r",
       " '月息',\r",
       " '带付',\r",
       " '不扫',\r",
       " '八百',\r",
       " '调到',\r",
       " '江干',\r",
       " '烦人',\r",
       " '宝没钱',\r",
       " '能永',\r",
       " '一千块',\r",
       " '新人',\r",
       " '钱',\r",
       " '组合',\r",
       " '一付',\r",
       " '各',\r",
       " '\\ufeff',\r",
       " '喃',\r",
       " '陇南',\r",
       " '理解',\r",
       " '启动',\r",
       " '巴斯',\r",
       " '购物款',\r",
       " '借次',\r",
       " '卷',\r",
       " '冻结',\r",
       " '差到',\r",
       " '足够',\r",
       " '心形',\r",
       " '借给',\r",
       " '提额充',\r",
       " '发旧',\r",
       " 'oppor',\r",
       " '减除',\r",
       " '付宝',\r",
       " '热线',\r",
       " '几百块',\r",
       " '借多',\r",
       " '元摩拜',\r",
       " '消费者',\r",
       " '作',\r",
       " '局限',\r",
       " '五个',\r",
       " '买断',\r",
       " '提不起',\r",
       " '转移',\r",
       " '不成',\r",
       " '求开',\r",
       " '清晰',\r",
       " '┃',\r",
       " '还光',\r",
       " '一千',\r",
       " '有收',\r",
       " '油站',\r",
       " '扣会',\r",
       " '关掉',\r",
       " '抽奖券',\r",
       " '约',\r",
       " '数字',\r",
       " '加多少',\r",
       " '债务',\r",
       " '正式',\r",
       " '我花臂',\r",
       " '回花',\r",
       " '用网',\r",
       " '凭证',\r",
       " '扫描枪',\r",
       " '几久',\r",
       " '个人',\r",
       " '图',\r",
       " '随机',\r",
       " '由',\r",
       " '条款',\r",
       " '删去',\r",
       " '点亮',\r",
       " '石化',\r",
       " '我同',\r",
       " '弄掉',\r",
       " '中能',\r",
       " '点评',\r",
       " '有货',\r",
       " '招牌',\r",
       " '同样',\r",
       " '到付',\r",
       " 'note',\r",
       " '扣应',\r",
       " '多重',\r",
       " '之外',\r",
       " '吐乱',\r",
       " '未出',\r",
       " '事',\r",
       " '二维码',\r",
       " '开同',\r",
       " '我换花',\r",
       " '一個',\r",
       " '转发',\r",
       " '后面',\r",
       " '..',\r",
       " '转号',\r",
       " '钱以',\r",
       " '接过',\r",
       " '一弄',\r",
       " '抢',\r",
       " '马蚁',\r",
       " '深林',\r",
       " '拉面',\r",
       " '敬请',\r",
       " '狂欢节',\r",
       " '返',\r",
       " '贸易',\r",
       " '不光',\r",
       " '起款',\r",
       " '换款',\r",
       " '开支',\r",
       " '账上',\r",
       " '美团动',\r",
       " '时扫',\r",
       " 'fang',\r",
       " '工商',\r",
       " '三元',\r",
       " '常',\r",
       " '下款',\r",
       " '第一位',\r",
       " '赎回',\r",
       " '灰',\r",
       " '不久',\r",
       " 'shids',\r",
       " '元多',\r",
       " '一分钱',\r",
       " '信贷',\r",
       " '储存卡',\r",
       " '二百三十多',\r",
       " '沒到',\r",
       " '文易',\r",
       " '共享',\r",
       " '信号',\r",
       " '有用吗',\r",
       " '有网',\r",
       " '实品',\r",
       " '可延长',\r",
       " '现实',\r",
       " '日息',\r",
       " '便利店',\r",
       " '美团点',\r",
       " '少换',\r",
       " '语气',\r",
       " '重复',\r",
       " '有应',\r",
       " '价位',\r",
       " '多扣',\r",
       " 'ma',\r",
       " '修好',\r",
       " '最快',\r",
       " '时确',\r",
       " '用餐',\r",
       " '開通',\r",
       " '出错',\r",
       " '最低标准',\r",
       " '元天',\r",
       " '能订',\r",
       " '鸡饲料',\r",
       " '再试',\r",
       " '钱克',\r",
       " '两款',\r",
       " '祢好',\r",
       " '贷中',\r",
       " '从',\r",
       " '入网',\r",
       " '当面',\r",
       " '代开',\r",
       " '气借',\r",
       " '票退',\r",
       " '每',\r",
       " '素材',\r",
       " '鼓励',\r",
       " '负起',\r",
       " '取不来',\r",
       " '天猫店',\r",
       " '天在',\r",
       " '钱转',\r",
       " '服装店',\r",
       " '对不起',\r",
       " '起升',\r",
       " '计次',\r",
       " '店铺',\r",
       " '还入',\r",
       " '日会',\r",
       " '信',\r",
       " '军校',\r",
       " '联通公司',\r",
       " '机',\r",
       " '重要',\r",
       " '两倍',\r",
       " '有折',\r",
       " 'qianq',\r",
       " '用脸',\r",
       " '绑花',\r",
       " '已借',\r",
       " '日包',\r",
       " '折券',\r",
       " '选泽',\r",
       " '开办',\r",
       " '收戗',\r",
       " '吵',\r",
       " '招联',\r",
       " '接着',\r",
       " '荣盛',\r",
       " '几月',\r",
       " ...}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TextCNN 词表对应成向量\n",
    "def get_embedding(vocab, embedding_file_path):\n",
    "    print('processing embedding file ...')\n",
    "\n",
    "    token2embedding = {}\n",
    "\n",
    "    with bz2.open(embedding_file_path) as f:\n",
    "\n",
    "        token_vectors = f.readlines()\n",
    "        meta_info = token_vectors[0].split()\n",
    "        print(f'{meta_info[0]} tokens in embedding file in total, vector size is {meta_info[-1]}')\n",
    "\n",
    "        for line in tqdm(token_vectors[1:]):\n",
    "            line = line.split()\n",
    "            token = line[0].decode('utf8')\n",
    "\n",
    "            vector = line[1:]\n",
    "            if token in vocab:\n",
    "                token2embedding[token] = [float(num) for num in vector]\n",
    "\n",
    "    token2idx = {token: idx for idx, token in enumerate(token2embedding.keys(), 4)}\n",
    "    UNK, PAD, BOS, EOS = '<unk>', '<pad>', '<bos>', '<eos>'\n",
    "    token2idx[PAD] = 0\n",
    "    token2idx[UNK] = 1\n",
    "    token2idx[BOS] = 2\n",
    "    token2idx[EOS] = 3\n",
    "    idx2token = {idx: token for token, idx in token2idx.items()}\n",
    "    idx2embedding = {token2idx[token]: embedding for token, embedding in token2embedding.items()}\n",
    "    idx2embedding[0] = [.0] * int(meta_info[-1])\n",
    "    idx2embedding[1] = [.0] * int(meta_info[-1])\n",
    "    idx2embedding[2] = np.random.random(int(meta_info[-1])).tolist()\n",
    "    idx2embedding[3] = np.random.random(int(meta_info[-1])).tolist()\n",
    "    emb_mat = [idx2embedding[idx] for idx in range(len(idx2embedding))]\n",
    "\n",
    "    return torch.tensor(emb_mat, dtype=torch.float), token2idx, len(vocab) + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing embedding file ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4083/195202 [00:00<00:14, 13060.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'195202' tokens in embedding file in total, vector size is b'300'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 13583/195202 [00:00<00:08, 20754.04it/s]100%|██████████| 195202/195202 [00:02<00:00, 70533.61it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, token2idx, config['vocab_size'] = get_embedding(vocab, config['embedding_file_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def tokenizer(sent, token2id):\n",
    "    # .get() 找到返回 token的id, 没找到就返回 1 1->UNK\n",
    "    ids = [token2id.get(token, 1) for token in jieba.cut(sent)]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(data_df, train_val_ratio, token2id, mode='train'):\n",
    "    if mode == 'train':\n",
    "        X_train, y_train = defaultdict(list), []\n",
    "        X_val, y_val = defaultdict(list), []\n",
    "        num_val = int(len(data_df) * train_val_ratio)\n",
    "    else:\n",
    "        X_test, y_test = defaultdict(list), []\n",
    "\n",
    "    for i, row in tqdm(data_df.iterrows(), desc=f'Preprocessing {mode} data', total=len(data_df)):\n",
    "        # -------------  new ---------------------------#\n",
    "        text_left = row[0]\n",
    "        text_right = row[1]\n",
    "        label = row[2]\n",
    "\n",
    "        inputs_a = tokenizer(text_left, token2id=token2idx)\n",
    "        inputs_b = tokenizer(text_right, token2id=token2idx)\n",
    "\n",
    "        if mode == 'train':\n",
    "            if i<num_val:\n",
    "                X_val['text_left'].append(inputs_a)\n",
    "                X_val['text_right'].append(inputs_b)\n",
    "                y_val.append(label)\n",
    "            else:\n",
    "                X_train['text_left'].append(inputs_a)\n",
    "                X_train['text_right'].append(inputs_b)\n",
    "                y_train.append(label)  \n",
    "\n",
    "        else:\n",
    "            X_test['text_left'].append(inputs_a)\n",
    "            X_test['text_right'].append(inputs_b)\n",
    "            y_test.append(label)             \n",
    "\n",
    "        # -------------  new ---------------------------#\n",
    "\n",
    "    if mode == 'train':\n",
    "        label2id = {label: i for i, label in enumerate(np.unique(y_train))}\n",
    "        id2label = {i: label for label, i in label2id.items()}\n",
    "        y_train = torch.tensor([label2id[label] for label in y_train], dtype=torch.long)\n",
    "        y_val = torch.tensor([label2id[label] for label in y_val], dtype=torch.long)\n",
    "        return X_train, y_train, X_val, y_val, label2id, id2label\n",
    "    else:\n",
    "        y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "        return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing train data: 100%|██████████| 38650/38650 [00:10<00:00, 3726.01it/s]\n",
      "Preprocessing test data: 100%|██████████| 3861/3861 [00:01<00:00, 3764.93it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, label2id, id2label = read_data(train_df, config['train_val_ratio'], token2idx, mode='train')\n",
    "X_test, y_test = read_data(test_df, config['train_val_ratio'], token2idx, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class AFQMCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        super(AFQMCDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ------------ new -----------------#\n",
    "        data = (self.x['text_left'][idx], \n",
    "                self.x['text_right'][idx], \n",
    "                self.y[idx])\n",
    "        return data\n",
    "        # ------------ new -----------------#\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "TextCNN中collete_fn函数\n",
    "def collete_fn(examples):\n",
    "    input_ids_list = []\n",
    "    labels =[]\n",
    "    for example in examples:\n",
    "        input_ids_list.append(example['input_ids'])\n",
    "        labels.append(example['label'])\n",
    "    \n",
    "    # 对齐操作 -- 找到 input_ids_list 中 最长的 句子， 执行短句子补齐\n",
    "    # 1. 找到 input_ids_list 中 最长的 句子\n",
    "    max_length = max(len(input_ids) for input_ids in input_ids_list) \n",
    "    # 2. 定义一个 input_ids_tensor, 我们要把 每个 input_ids 放入 tensor 中\n",
    "    input_ids_tensor = torch.zeros((len(labels), max_length), dtype=torch.long)\n",
    "    for i, input_ids in enumerate(input_ids_list):\n",
    "        # 得到当前句子的长度\n",
    "        seq_len = len(input_ids)\n",
    "        # 第i个句子，填充 seq_len 这么长\n",
    "        input_ids_tensor[i, :seq_len] = torch.tensor(input_ids, dtype=torch.long)\n",
    "    \n",
    "    return {\n",
    "        'input_ids' : input_ids_tensor,\n",
    "        'labels' : torch.tensor(labels, dtype=torch.long)\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2, 3], [4, 5, 6], 1) ([777, 888, 999], [321, 543, 654], 0)\n",
      "<zip object at 0x7f4c5defedc0>\n",
      "[([1, 2, 3], [777, 888, 999]), ([4, 5, 6], [321, 543, 654]), (1, 0)]\n"
     ]
    }
   ],
   "source": [
    "# 假设 传入 Collator一共两条数据\n",
    "# 每条数据 data = (sentence1, sentence2, label])\n",
    "datas = [([1,2,3],[4,5,6],1),([777,888,999],[321,543,654],0)]\n",
    "print(*datas)\n",
    "print(zip(*datas))\n",
    "print(list(zip(*datas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# __call__\n",
    "class MyClass():\n",
    "    def __call__(self):\n",
    "        print('__call__方法被调用')\n",
    "        return 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__call__方法被调用\n"
     ]
    }
   ],
   "source": [
    "obj = MyClass()\n",
    "res = obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 从 AFQMCDataset 输出的 data  = (sentence1, sentence2, label])\n",
    "# 1. 将元组中属于sentence1的放在一起，属于sentence2的放在一起，属于label的放在一起\n",
    "# 2. 对齐操作，找到sentence1, sentence2,最长的句子，执行短句子补齐\n",
    "# 3. 定义一个tensor，把数据放里面\n",
    "\n",
    "class Collator:\n",
    "    def __init__(self, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def get_max_seq_len(self, ids_list):\n",
    "        cur_max_seq_len = max(len(input_id) for input_id in ids_list)\n",
    "        max_seq_len = min(self.max_seq_len, cur_max_seq_len)\n",
    "        return max_seq_len\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_and_truncate(text_ids_list, max_seq_len):\n",
    "        input_ids = torch.zeros((len(text_ids_list), max_seq_len), dtype=torch.long)\n",
    "        for i, text_ids in enumerate(text_ids_list):\n",
    "            seq_len = min(len(text_ids), max_seq_len)\n",
    "            input_ids[i, :seq_len] = torch.tensor(text_ids[:seq_len], dtype=torch.long)\n",
    "        \n",
    "        return input_ids\n",
    "\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        # 1. 将元组中属于sentence1的放在一起，属于sentence2的放在一起，属于label的放在一起\n",
    "        text_ids_left_list, text_ids_right_list, labels_list = list(zip(*examples))\n",
    "\n",
    "        # 2.1 找到 text_ids_left_list, text_ids_right_list 最长的句子长度\n",
    "        max_text_left_length = self.get_max_seq_len(text_ids_left_list)\n",
    "        max_text_right_length = self.get_max_seq_len(text_ids_right_list)\n",
    "\n",
    "        # 2.2 执行短暂句子补齐, 3.定义一个tensor，把数据放里面\n",
    "        text_left_ids = self.pad_and_truncate(text_ids_left_list, max_text_left_length)\n",
    "        text_right_ids = self.pad_and_truncate(text_ids_right_list, max_text_right_length)\n",
    "        labels = torch.tensor(labels_list, dtype=torch.long)\n",
    "        \n",
    "        data_list = [text_left_ids, text_right_ids, labels]\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def build_dataloader(train_df, test_df, config, vocab):\n",
    "    X_train, y_train, X_val, y_val, label2id, id2label = read_data(train_df, config['train_val_ratio'], vocab, mode='train')\n",
    "    X_test, y_test = read_data(test_df, config['train_val_ratio'], vocab, mode='test')\n",
    "\n",
    "    train_dataset = AFQMCDataset(X_train, y_train)\n",
    "    val_dataset = AFQMCDataset(X_val, y_val)\n",
    "    test_dataset = AFQMCDataset(X_test, y_test)\n",
    "    \n",
    "    # -----------------new -----------------------#\n",
    "    collate_fn = Collator(config['max_seq_len'])\n",
    "    # -----------------new -----------------------#\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=config['batch_size'],\n",
    "                                  num_workers=4, shuffle=True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=config['batch_size'],\n",
    "                                num_workers=4, shuffle=False, collate_fn=collate_fn)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=config['batch_size'],\n",
    "                                 num_workers=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return id2label, test_dataloader, train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing train data: 100%|██████████| 38650/38650 [00:10<00:00, 3690.11it/s]\n",
      "Preprocessing test data: 100%|██████████| 3861/3861 [00:01<00:00, 3676.12it/s]\n"
     ]
    }
   ],
   "source": [
    "id2label, test_dataloader, train_dataloader, val_dataloader = build_dataloader(train_df, test_df, config, token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in train_dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def evaluation(config, model, val_dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    val_loss = 0.\n",
    "    val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            # batch [([sent_left_id1],[sent_left_id1]..),([sent_right_id2],[sent_right_id2]..),([label],[label]..)]\n",
    "            # -----------new ----------------#\n",
    "            labels.append(batch[-1])\n",
    "            batch = [item.to(config['device']) for item in batch]\n",
    "            loss, logits = model(batch)[:2]\n",
    "            # -----------new ----------------#\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            # 返回逻辑值最大的位置，要么0，要么1\n",
    "            preds.append(logits.argmax(dim=-1).detach().cpu())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    # -----------new ----------------#\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    # -----------new ----------------#\n",
    "    return avg_val_loss, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TextCNN 改写\n",
    "from tqdm import trange\n",
    "from transformers import AdamW\n",
    "\n",
    "def train(model, config, id2label, train_dataloader, val_dataloader):\n",
    "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    model.to(config['device'])\n",
    "    epoch_iterator = trange(config['num_epochs'])\n",
    "\n",
    "    global_steps = 0\n",
    "    train_loss = 0.\n",
    "    logging_loss = 0.\n",
    "\n",
    "    for epoch in epoch_iterator:\n",
    "\n",
    "        train_iterator = tqdm(train_dataloader, desc='Training', total=len(train_dataloader))\n",
    "        model.train()\n",
    "        for batch in train_iterator:\n",
    "            # -----------new ----------------#\n",
    "            batch = [item.to(config['device']) for item in batch]\n",
    "            loss = model(batch)[0]\n",
    "            # -----------new ----------------#\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            global_steps += 1\n",
    "\n",
    "            if global_steps % config['logging_step'] == 0:\n",
    "                print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
    "                logging_loss = train_loss\n",
    "\n",
    "                # -----------new ----------------#\n",
    "                avg_val_loss, f1, acc = evaluation(config, model, val_dataloader)\n",
    "                # -----------new ----------------#\n",
    "\n",
    "                print_log = f'>>> training loss: {print_train_loss:.4f}, valid loss: {avg_val_loss:.4f}, ' \\\n",
    "                            f'valid f1 score: {f1:.4f}, valid acc: {acc:.4f}'\n",
    "                print(print_log)\n",
    "                model.train()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(config, id2label, model, test_dataloader):\n",
    "    test_iterator = tqdm(test_dataloader, desc='Predicting', total=len(test_dataloader))\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iterator:\n",
    "            batch = [item.to(config['device']) for item in batch]\n",
    "            logits = model(batch)[1]\n",
    "            test_preds.append(logits.argmax(dim=-1).detach().cpu())\n",
    "    test_preds = torch.cat(test_preds, dim=0).numpy()\n",
    "    test_preds = [id2label[id_] for id_ in test_preds]\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "现在是休息时间，看录播的同学可以跳过哦～: 100%|██████████| 900/900 [15:02<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for i in tqdm(range(60*15), desc=\"现在是休息时间，看录播的同学可以跳过哦～\"):\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预备知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5., 5., 5., 5.],\n",
      "         [6., 6., 6., 6.],\n",
      "         [7., 7., 7., 7.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [2., 2., 2., 2.],\n",
      "         [3., 3., 3., 3.]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 定义一个 [2,3,4] -> [batch_size, seq_len, embedding_dim]  一个batch有两个句子，每个句子3个词，每个词的维度为4\n",
    "a = torch.tensor([[[5., 5., 5., 5.],[6., 6., 6., 6.],[7., 7., 7., 7.]], [[1., 1., 1., 1.],[2., 2., 2., 2.],[3., 3., 3., 3.]]])\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# [2, 1, 4]\n",
    "ones = torch.ByteTensor([[[1, 1, 0, 0]],[[0, 1, 1, 0]]])\n",
    "print(ones.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5., 5., 0., 0.],\n",
      "         [6., 6., 0., 0.],\n",
      "         [7., 7., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 1., 0.],\n",
      "         [0., 2., 2., 0.],\n",
      "         [0., 3., 3., 0.]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 相当于将词向量 某些维度 清0\n",
    "print(ones * a) # ones. [2,1,4] -> [2,3,4] \n",
    "print((ones * a).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### masked_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# [2, 3, 1]\n",
    "mask = torch.ByteTensor([[[1],[1],[0]],[[0],[1],[1]]])\n",
    "print(mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5., 5., 5., 5.],\n",
      "         [6., 6., 6., 6.],\n",
      "         [7., 7., 7., 7.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [2., 2., 2., 2.],\n",
      "         [3., 3., 3., 3.]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 定义一个 [2,3,4] -> [batch_size, seq_len, embedding_dim]  一个batch有两个句子，每个句子3个词，每个词的维度为4\n",
    "a = torch.tensor([[[5., 5., 5., 5.],[6., 6., 6., 6.],[7., 7., 7., 7.]], [[1., 1., 1., 1.],[2., 2., 2., 2.],[3., 3., 3., 3.]]])\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
      "         [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
      "         [ 7.0000e+00,  7.0000e+00,  7.0000e+00,  7.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
      "         [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
      "         [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022071458/work/aten/src/ATen/native/LegacyDefinitions.cpp:41: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "# a [2,3,4] mask[2,3,1]\n",
    "#是将 mask 中 为1 的 元素所在的索引，在 a 中 相同索引处替换为value\n",
    "# 把某个词向量给mask\n",
    "b = a.masked_fill(mask, value=torch.tensor(-1e7))\n",
    "print(b)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# [2, 1, 4]\n",
    "ones = torch.ByteTensor([[[1, 1, 0, 0]],[[0, 1, 1, 0]]])\n",
    "print(ones.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0000e+07, -1.0000e+07,  5.0000e+00,  5.0000e+00],\n",
      "         [-1.0000e+07, -1.0000e+07,  6.0000e+00,  6.0000e+00],\n",
      "         [-1.0000e+07, -1.0000e+07,  7.0000e+00,  7.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.0000e+07, -1.0000e+07,  1.0000e+00],\n",
      "         [ 2.0000e+00, -1.0000e+07, -1.0000e+07,  2.0000e+00],\n",
      "         [ 3.0000e+00, -1.0000e+07, -1.0000e+07,  3.0000e+00]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022071458/work/aten/src/ATen/native/LegacyDefinitions.cpp:41: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "mask = ones # [2, 1, 4]\n",
    "b = a.masked_fill(mask, value=torch.tensor(-1e7))\n",
    "print(b)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ESIM模型 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![ESIM](https://img-blog.csdnimg.cn/img_convert/1adb67ec46e87da23fa042f298ff88bb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![ESIM2](https://img-blog.csdnimg.cn/img_convert/6cfe48bd15e9616c0d92eaeebfa50e7b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![pytorch LSTM](https://img-blog.csdnimg.cn/5697d3a5bfec44039e4c1b407c4ec924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![stackRNN](https://img-blog.csdnimg.cn/02b1b24b629c4defabb888776f9d3f57.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Local Inference Modeling](https://img-blog.csdnimg.cn/c8afbd13e0da49a080fd8207e31eac8c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Local inference collected over sequences](https://img-blog.csdnimg.cn/759a1006a9354deeabdb8e0a7bc5f20a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Local inference 3](https://img-blog.csdnimg.cn/176f587346994ceaa2d6e0d3393a69b4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![The composition layer](https://img-blog.csdnimg.cn/bb58ab8d53454eb2aaf450b5378bcb74.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Pooling](https://img-blog.csdnimg.cn/66a661431da8440d9cd0efdb3aeac523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![all of gongshi](https://img-blog.csdnimg.cn/652165f9f0584ac683c0df8d412514be.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_config = {\n",
    "        'embedding': embedding_matrix,  # torch.Size([5251, 300])\n",
    "        'freeze_emb': True,\n",
    "        'hidden_size': 256,\n",
    "        'dropout': 0.3,\n",
    "        'num_layers': 2,\n",
    "        'concat_layers': True,\n",
    "        'rnn_type': 'lstm',\n",
    "        'num_labels': len(id2label)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNNDropout(nn.Dropout):\n",
    "    # 将词向量 某些维度 清0\n",
    "    # sequences_batch [B, L, D]\n",
    "    def forward(self, sequences_batch):\n",
    "        # ones [B, D]\n",
    "        ones = sequences_batch.data.new_ones(sequences_batch.shape[0], sequences_batch.shape[-1])\n",
    "\n",
    "        # 随机 mask ones\n",
    "        # dropout_mask [B, D]\n",
    "        dropout_mask = nn.functional.dropout(ones, self.p, self.training, inplace=False)\n",
    "       \n",
    "        return dropout_mask.unsqueeze(1) * sequences_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class StackedBRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers,\n",
    "                 dropout_rate=0, dropout_output=False, rnn_type=nn.LSTM, \n",
    "                 concat_layers=False):\n",
    "        super().__init__()\n",
    "        self.dropout_output = dropout_output\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers = num_layers\n",
    "        self.concat_layers = concat_layers\n",
    "        self.rnns = nn.ModuleList()\n",
    "        # 共有两层LSTM\n",
    "        for i in range(num_layers):\n",
    "            input_size = input_size if i == 0 else 2*hidden_size\n",
    "            self.rnns.append(rnn_type(input_size, hidden_size, num_layers=1, bidirectional=True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x (B, L, D) -> (L, B, D)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        outputs = [x]\n",
    "        for i in range(self.num_layers):\n",
    "            rnn_input = outputs[-1]\n",
    "\n",
    "            if self.dropout_rate > 0:\n",
    "                rnn_input = F.dropout(rnn_input, p=self.dropout_rate, training=self.training)\n",
    "            \n",
    "            # self.rnn[i](rnn_input) (output, (h_n, c_n))\n",
    "            rnn_output = self.rnns[i](rnn_input)[0]\n",
    "            outputs.append(rnn_output)\n",
    "        \n",
    "        # outputs [x, output0, output1]\n",
    "        if self.concat_layers:\n",
    "            output = torch.cat(outputs[1:], 2)\n",
    "        else:\n",
    "            output = outputs[-1]\n",
    "        \n",
    "        # output (L, B, D) -> (B, L, D)\n",
    "        output = output.transpose(0, 1)\n",
    "\n",
    "        if self.dropout_output and self.dropout_rate > 0:\n",
    "            output = F.dropout(output, p=self.dropout_rate, training=self.training)\n",
    "        \n",
    "        # 进行 transpose之后，tensor在内存中不连续， contiguous将output内存连续\n",
    "        return output.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BidirectionalAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # v1 [B, L, H]\n",
    "        # v1_mask [B, L]\n",
    "        # v2 [B, R, H]\n",
    "        # v2_mask [B, R]\n",
    "    def forward(self, v1, v1_mask, v2, v2_mask):\n",
    "        # v2:a v1:b \n",
    "\n",
    "        # 1.计算矩阵相似度\n",
    "        # similarity_matrix [B, L, R]\n",
    "        similarity_matrix = v1.bmm(v2.transpose(2, 1).contiguous())\n",
    "\n",
    "        # 2.计算attention时没有必要计算pad=0, 要进行mask操作 3.进行softmax\n",
    "        # 将similarity_matrix v1中pad对应的权重给mask\n",
    "        # [B, L, R]\n",
    "        v2_v1_attn = F.softmax(\n",
    "            similarity_matrix.masked_fill(\n",
    "                v1_mask.unsqueeze(2), -1e7), dim=1)\n",
    "\n",
    "        # 将similarity_matrix v2中pad对应的权重给mask\n",
    "        # [B, L, R]\n",
    "        v1_v2_attn = F.softmax(\n",
    "            similarity_matrix.masked_fill(\n",
    "                v2_mask.unsqueeze(1), -1e7),dim=2)\n",
    "\n",
    "        # 4.计算attention\n",
    "        # [B, L, R] @ [B, R, H] \n",
    "        # 句子a 对b的影响 [B, L, H]\n",
    "        # attented_v1 [B, L, H]\n",
    "        attented_v1 = v1_v2_attn.bmm(v2)\n",
    "\n",
    "        # 句子b 对a的影响 \n",
    "        # v2_v1_attn [B, L, R] -> [B, R, L] @[B, L, H] -> [B, R, H]\n",
    "        # attented_v2 [B, R, H]\n",
    "        attented_v2 = v2_v1_attn.transpose(1,2).bmm(v1)\n",
    "\n",
    "        # attented_v1 将v1对应的pad填充为0\n",
    "        # attented_v2 将v2对应的pad填充为0\n",
    "        attented_v1.masked_fill(v1_mask.unsqueeze(2), 0)\n",
    "        attented_v2.masked_fill(v2_mask.unsqueeze(2), 0)\n",
    "        return attented_v1, attented_v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ESIM(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # -----------------------   input encoding  ---------------------#\n",
    "        rnn_mapping = {'lstm': nn.LSTM, 'gru': nn.GRU}\n",
    "        self.embedding = nn.Embedding.from_pretrained(config['embedding'], freeze=config['freeze_emb'])\n",
    "\n",
    "        self.rnn_dropout = RNNDropout(p=config['dropout'])\n",
    "        rnn_size = config['hidden_size']\n",
    "\n",
    "        if config['concat_layers']:\n",
    "            rnn_size //= config['num_layers']\n",
    "\n",
    "        self.input_encoding = StackedBRNN(input_size=config['embedding'].size(1),\n",
    "                                          hidden_size=rnn_size // 2,\n",
    "                                          num_layers=config['num_layers'],\n",
    "                                          rnn_type=rnn_mapping[config['rnn_type']],\n",
    "                                          concat_layers=config['concat_layers'])\n",
    "\n",
    "        # -----------------------   input encoding  ---------------------#\n",
    "\n",
    "        # -----------------------   Local inference collected over sequences  ---------------------#\n",
    "        self.attention = BidirectionalAttention()\n",
    "        # -----------------------   Local inference collected over sequences  ---------------------#\n",
    "\n",
    "\n",
    "        # -----------------------   the compositon layer  ---------------------#\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(4 * config['hidden_size'], config['hidden_size']),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "        self.composition = StackedBRNN(input_size=config['hidden_size'],\n",
    "                                      hidden_size=rnn_size // 2,\n",
    "                                      num_layers=config['num_layers'],\n",
    "                                      rnn_type=rnn_mapping[config['rnn_type']],\n",
    "                                      concat_layers=config['concat_layers'])\n",
    "\n",
    "\n",
    "        # -----------------------   the compositon layer  ---------------------#\n",
    "\n",
    "\n",
    "        self.classification = nn.Sequential(\n",
    "            nn.Dropout(p=config['dropout']),\n",
    "            nn.Linear(4 * config['hidden_size'], config['hidden_size']),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=config['dropout']))\n",
    "            \n",
    "        self.out = nn.Linear(config['hidden_size'], config['num_labels'])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: [sentence1_tensor, sentence2_tensor, labels_tensor]\n",
    "        # B: batch_size\n",
    "        # L = 'inputs left'  sequence length\n",
    "        # R = 'inputs right'  sequence length\n",
    "        # D = embedding size\n",
    "        # H = hidden size\n",
    "\n",
    "        # -----------------------   input encoding  ---------------------#\n",
    "        # query sentence1_tensor\n",
    "        # doc sentence2_tensor\n",
    "   \n",
    "        # query [B, L]\n",
    "        # doc [B, R] \n",
    "        query, doc = inputs[0].long(), inputs[1].long()\n",
    "        \n",
    "        # 判断 query，doc中的每一个数是不是0， 是1则表示该位置是pad\n",
    "        # query：[2,3,4,5,0,0,0] -> query_mask：[0,0,0,0,1,1,1]\n",
    "        # query_mask [B, L]\n",
    "        # doc_mask [B, R]\n",
    "        query_mask = (query == 0)\n",
    "        doc_mask = (doc == 0)\n",
    "        \n",
    "        # query [B, L, D]\n",
    "        # doc [B, R, D]\n",
    "        query = self.embedding(query)\n",
    "        doc = self.embedding(doc)\n",
    "        \n",
    "        # query [B, L, D]\n",
    "        # doc [B, R, D]\n",
    "        query = self.rnn_dropout(query)\n",
    "        doc = self.rnn_dropout(doc)\n",
    "        \n",
    "        # query [B, L, H]\n",
    "        # doc [B, R, H]\n",
    "        query = self.input_encoding(query)\n",
    "        doc = self.input_encoding(doc)\n",
    "        # -----------------------   input encoding  ---------------------#\n",
    "\n",
    "        # 1.计算矩阵相似度\n",
    "        # 2.计算attention时没有必要计算pad=0, 要进行mask操作\n",
    "        # 3.进行softmax\n",
    "        # 4.计算attention\n",
    "\n",
    "        # -----------------------   Local inference collected over sequences  ---------------------#\n",
    "        # query [B, L, H]\n",
    "        # query_mask [B, L]\n",
    "        # doc [B, R, H]\n",
    "        # doc_mask [B, R]\n",
    "        attended_query, attended_doc = self.attention(query, query_mask, doc, doc_mask)\n",
    "        # -----------------------   Local inference collected over sequences  ---------------------#\n",
    "\n",
    "        # -----------------------  Enhancement of local inference information ---------------------#\n",
    "        # enhanced_query [B, L, 4*h]\n",
    "        # enhanced_doc [B, R, 4*h]\n",
    "        enhanced_query = torch.cat([query, \n",
    "                                    attended_query, \n",
    "                                    query-attended_query, \n",
    "                                    query*attended_query], \n",
    "                                    dim=-1)\n",
    "        \n",
    "        enhanced_doc = torch.cat([doc, \n",
    "                                  attended_doc, \n",
    "                                  doc-attended_doc, \n",
    "                                  doc*attended_doc], \n",
    "                                  dim=-1)\n",
    "        \n",
    "        # -----------------------  Enhancement of local inference information ---------------------#\n",
    "         \n",
    "\n",
    "        # -----------------------   the compositon layer  ---------------------#\n",
    "        # projected_query [B, L, H]\n",
    "        # projected_doc [B, R, H]\n",
    "        projected_query = self.projection(enhanced_query)\n",
    "        projected_doc = self.projection(enhanced_doc)\n",
    "        \n",
    "        # projected_query [B, L, H]\n",
    "        # projected_doc [B, R, H]\n",
    "        query = self.composition(projected_query)\n",
    "        doc = self.composition(projected_doc)\n",
    "        # -----------------------   the compositon layer  ---------------------#\n",
    "\n",
    "        # -----------------------   Pooling  ---------------------#\n",
    "        # query_mask， doc_mask. 判断 query，doc中的每一个数是不是0， 是1则表示该位置是pad\n",
    "        # reverse_query_mask 0的位置代表pad\n",
    "        # reverse_query_mask [B, L]\n",
    "        # reverse_doc_mask [B, R]\n",
    "        reverse_query_mask = 1. - query_mask.float()\n",
    "        reverse_doc_mask = 1. - doc_mask.float()\n",
    "\n",
    "        query_avg = torch.sum(query * reverse_query_mask.unsqueeze(2),dim=1)/ (torch.sum(reverse_query_mask, dim=1, keepdim=True) + 1e-8)\n",
    "        doc_avg = torch.sum(doc * reverse_doc_mask.unsqueeze(2),dim=1)/ (torch.sum(reverse_doc_mask, dim=1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        # 防止取出pad\n",
    "        query = query.masked_fill(query_mask.unsqueeze(2), -1e7)\n",
    "        doc = doc.masked_fill(doc_mask.unsqueeze(2), -1e7)\n",
    "        \n",
    "        \n",
    "        query_max, _ = query.max(dim=1)\n",
    "        doc_max, _ = doc.max(dim=1)\n",
    "        \n",
    "        # v [B, 4*H]\n",
    "        v = torch.cat([query_avg, query_max, doc_avg, doc_max], dim=-1)\n",
    "        # -----------------------   Pooling  ---------------------#\n",
    "\n",
    "        # -----------------------   prediction  ---------------------#\n",
    "        # hidden [B, H]\n",
    "        hidden = self.classification(v)\n",
    "\n",
    "        out = self.out(hidden)\n",
    "        outputs = (out, )\n",
    "        # -----------------------   prediction  ---------------------#\n",
    "\n",
    "        if len(inputs) == 3:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(out, inputs[-1])\n",
    "            outputs = (loss, ) + outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ESIM(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Training:   0%|          | 0/484 [00:00<?, ?it/s]\u001b[AException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4c7d0274d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4c7d0274d0>\n",
      "AssertionError: can only join a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "\n",
      "Training:   0%|          | 1/484 [00:00<08:02,  1.00it/s]\u001b[AException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4c7d0274d0>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4c7d0274d0>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "    w.join()\n",
      "    w.join()\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "AssertionError: can only join a child process\n",
      "\n",
      "Training:   0%|          | 2/484 [00:01<05:57,  1.35it/s]\u001b[A\n",
      "Training:   1%|          | 5/484 [00:01<04:13,  1.89it/s]\u001b[A\n",
      "Training:   2%|▏         | 9/484 [00:01<03:00,  2.63it/s]\u001b[A\n",
      "Training:   2%|▏         | 12/484 [00:01<02:10,  3.61it/s]\u001b[A\n",
      "Training:   3%|▎         | 15/484 [00:01<01:35,  4.90it/s]\u001b[A\n",
      "Training:   4%|▍         | 19/484 [00:01<01:10,  6.56it/s]\u001b[A\n",
      "Training:   5%|▍         | 22/484 [00:01<00:53,  8.56it/s]\u001b[A\n",
      "Training:   5%|▌         | 25/484 [00:01<00:43, 10.66it/s]\u001b[A\n",
      "Training:   6%|▌         | 28/484 [00:02<00:34, 13.16it/s]\u001b[A\n",
      "Training:   6%|▋         | 31/484 [00:02<00:28, 15.77it/s]\u001b[A\n",
      "Training:   7%|▋         | 35/484 [00:02<00:24, 18.50it/s]\u001b[A\n",
      "Training:   8%|▊         | 38/484 [00:02<00:21, 20.62it/s]\u001b[A\n",
      "Training:   9%|▊         | 42/484 [00:02<00:19, 22.63it/s]\u001b[A\n",
      "Training:   9%|▉         | 45/484 [00:02<00:18, 23.66it/s]\u001b[A\n",
      "Training:  10%|▉         | 48/484 [00:02<00:18, 23.67it/s]\u001b[A\n",
      "Training:  11%|█         | 51/484 [00:02<00:17, 24.98it/s]\u001b[A\n",
      "Training:  11%|█         | 54/484 [00:02<00:16, 26.21it/s]\u001b[A\n",
      "Training:  12%|█▏        | 57/484 [00:03<00:16, 25.95it/s]\u001b[A\n",
      "Training:  13%|█▎        | 61/484 [00:03<00:15, 27.24it/s]\u001b[A\n",
      "Training:  13%|█▎        | 65/484 [00:03<00:14, 28.77it/s]\u001b[A\n",
      "Training:  14%|█▍        | 68/484 [00:03<00:15, 27.53it/s]\u001b[A\n",
      "Training:  15%|█▍        | 71/484 [00:03<00:15, 27.08it/s]\u001b[A\n",
      "Training:  15%|█▌        | 74/484 [00:03<00:14, 27.80it/s]\u001b[A\n",
      "Training:  16%|█▌        | 77/484 [00:03<00:14, 27.64it/s]\u001b[A\n",
      "Training:  17%|█▋        | 80/484 [00:03<00:14, 27.52it/s]\u001b[A\n",
      "Training:  17%|█▋        | 83/484 [00:03<00:14, 27.58it/s]\u001b[A\n",
      "Training:  18%|█▊        | 86/484 [00:04<00:14, 27.53it/s]\u001b[A\n",
      "Training:  18%|█▊        | 89/484 [00:04<00:14, 27.43it/s]\u001b[A\n",
      "Training:  19%|█▉        | 92/484 [00:04<00:13, 28.15it/s]\u001b[A\n",
      "Training:  20%|█▉        | 95/484 [00:04<00:14, 26.40it/s]\u001b[A\n",
      "Training:  20%|██        | 98/484 [00:04<00:14, 27.05it/s]\u001b[A\n",
      "Training:  21%|██        | 102/484 [00:04<00:13, 28.09it/s]\u001b[A\n",
      "Training:  22%|██▏       | 106/484 [00:04<00:13, 28.56it/s]\u001b[A\n",
      "Training:  23%|██▎       | 109/484 [00:04<00:13, 28.48it/s]\u001b[A\n",
      "Training:  23%|██▎       | 113/484 [00:05<00:12, 28.83it/s]\u001b[A\n",
      "Training:  24%|██▍       | 117/484 [00:05<00:12, 28.99it/s]\u001b[A\n",
      "Training:  25%|██▍       | 120/484 [00:05<00:12, 28.14it/s]\u001b[A\n",
      "Training:  25%|██▌       | 123/484 [00:05<00:13, 26.30it/s]\u001b[A\n",
      "Training:  26%|██▌       | 126/484 [00:05<00:13, 27.11it/s]\u001b[A\n",
      "Training:  27%|██▋       | 129/484 [00:05<00:13, 26.96it/s]\u001b[A\n",
      "Training:  27%|██▋       | 132/484 [00:05<00:12, 27.24it/s]\u001b[A\n",
      "Training:  28%|██▊       | 136/484 [00:05<00:12, 28.08it/s]\u001b[A\n",
      "Training:  29%|██▊       | 139/484 [00:05<00:12, 28.34it/s]\u001b[A\n",
      "Training:  30%|██▉       | 143/484 [00:06<00:11, 28.98it/s]\u001b[A\n",
      "Training:  30%|███       | 146/484 [00:06<00:11, 28.93it/s]\u001b[A\n",
      "Training:  31%|███       | 149/484 [00:06<00:12, 27.45it/s]\u001b[A\n",
      "Training:  31%|███▏      | 152/484 [00:06<00:11, 27.93it/s]\u001b[A\n",
      "Training:  32%|███▏      | 155/484 [00:06<00:11, 27.89it/s]\u001b[A\n",
      "Training:  33%|███▎      | 159/484 [00:06<00:11, 28.64it/s]\u001b[A\n",
      "Training:  33%|███▎      | 162/484 [00:06<00:11, 28.11it/s]\u001b[A\n",
      "Training:  34%|███▍      | 165/484 [00:06<00:11, 28.35it/s]\u001b[A\n",
      "Training:  35%|███▍      | 169/484 [00:07<00:10, 29.25it/s]\u001b[A\n",
      "Training:  36%|███▌      | 173/484 [00:07<00:10, 29.72it/s]\u001b[A\n",
      "Training:  36%|███▋      | 176/484 [00:07<00:10, 28.22it/s]\u001b[A\n",
      "Training:  37%|███▋      | 179/484 [00:07<00:11, 27.51it/s]\u001b[A\n",
      "Training:  38%|███▊      | 183/484 [00:07<00:10, 28.55it/s]\u001b[A\n",
      "Training:  38%|███▊      | 186/484 [00:07<00:10, 28.74it/s]\u001b[A\n",
      "Training:  39%|███▉      | 189/484 [00:07<00:10, 28.87it/s]\u001b[A\n",
      "Training:  40%|███▉      | 192/484 [00:07<00:10, 29.03it/s]\u001b[A\n",
      "Training:  40%|████      | 195/484 [00:07<00:09, 28.98it/s]\u001b[A\n",
      "Training:  41%|████      | 198/484 [00:08<00:10, 27.52it/s]\u001b[A\n",
      "\n",
      "Evaluation:   0%|          | 0/121 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:   1%|          | 1/121 [00:00<00:49,  2.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:   6%|▌         | 7/121 [00:00<00:33,  3.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  12%|█▏        | 14/121 [00:00<00:22,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  17%|█▋        | 20/121 [00:00<00:15,  6.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  22%|██▏       | 27/121 [00:00<00:10,  8.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  28%|██▊       | 34/121 [00:00<00:07, 12.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  34%|███▍      | 41/121 [00:01<00:05, 15.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  40%|███▉      | 48/121 [00:01<00:03, 20.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  46%|████▋     | 56/121 [00:01<00:02, 26.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  52%|█████▏    | 63/121 [00:01<00:01, 32.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  58%|█████▊    | 70/121 [00:01<00:01, 38.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  64%|██████▎   | 77/121 [00:01<00:01, 43.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  69%|██████▉   | 84/121 [00:01<00:00, 48.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  76%|███████▌  | 92/121 [00:01<00:00, 53.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  82%|████████▏ | 99/121 [00:01<00:00, 56.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  88%|████████▊ | 106/121 [00:02<00:00, 58.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  93%|█████████▎| 113/121 [00:02<00:00, 61.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation: 100%|██████████| 121/121 [00:02<00:00, 51.39it/s]\u001b[A\u001b[A\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "Training:  42%|████▏     | 201/484 [00:10<01:17,  3.66it/s]\u001b[A\n",
      "Training:  42%|████▏     | 204/484 [00:10<00:56,  4.96it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> training loss: 0.6204, valid loss: 0.6381, valid f1 score: 0.3992, valid acc: 0.6644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:  43%|████▎     | 207/484 [00:10<00:42,  6.56it/s]\u001b[A\n",
      "Training:  43%|████▎     | 210/484 [00:10<00:32,  8.46it/s]\u001b[A\n",
      "Training:  44%|████▍     | 213/484 [00:10<00:25, 10.75it/s]\u001b[A\n",
      "Training:  45%|████▍     | 216/484 [00:11<00:20, 13.07it/s]\u001b[A\n",
      "Training:  45%|████▌     | 219/484 [00:11<00:17, 15.49it/s]\u001b[A\n",
      "Training:  46%|████▌     | 223/484 [00:11<00:14, 18.15it/s]\u001b[A\n",
      "Training:  47%|████▋     | 226/484 [00:11<00:12, 20.27it/s]\u001b[A\n",
      "Training:  47%|████▋     | 229/484 [00:11<00:11, 22.04it/s]\u001b[A\n",
      "Training:  48%|████▊     | 232/484 [00:11<00:10, 23.72it/s]\u001b[A\n",
      "Training:  49%|████▊     | 235/484 [00:11<00:09, 24.96it/s]\u001b[A\n",
      "Training:  49%|████▉     | 239/484 [00:11<00:09, 26.12it/s]\u001b[A\n",
      "Training:  50%|█████     | 243/484 [00:12<00:08, 27.44it/s]\u001b[A\n",
      "Training:  51%|█████     | 246/484 [00:12<00:08, 27.89it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "best_model = train(model, config, id2label, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(config, id2label, best_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.8.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
