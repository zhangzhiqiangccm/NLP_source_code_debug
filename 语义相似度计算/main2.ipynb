{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./words.vector.gz'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 移动文件\n",
    "# import shutil\n",
    "# shutil.move(\"data/data101045/words.vector.gz\",\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. 基础的文本数据增强（EDA）[同义词替换!]\n",
    "2. 闭包数据增强\n",
    "3. 无监督数据增强（UDA）\n",
    "4. 对偶数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据增强 （Data Augmentation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "数据增强：通过有限数据来产生更多的等价数据，人工扩展训练集的技术"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting synonyms\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/e9/33/dc5b383d986071cf77fefae0839db32ea05242f69319a6a044a9359f575f/synonyms-3.16.0.tar.gz (10.7MB)\n",
      "\u001b[K     |████████████████████████████████| 10.7MB 17.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from synonyms) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from synonyms) (1.19.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from synonyms) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from synonyms) (0.20.0)\n",
      "Building wheels for collected packages: synonyms\n",
      "  Building wheel for synonyms (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for synonyms: filename=synonyms-3.16.0-cp37-none-any.whl size=10832785 sha256=11e02e419dbacde24a3e357f2e42ec8f506a9d90b1acb412d3f663f0eb12f03d\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/cf/62/50/96a495f5ed067aaf5425e9debf31980c31ecb1bbe27f37f862\n",
      "Successfully built synonyms\n",
      "Installing collected packages: synonyms\n",
      "Successfully installed synonyms-3.16.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -U synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! cp 'words.vector.gz' '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/synonyms/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jieba] default dict file path ../data/vocab.txt\n",
      "[jieba] default dict file path ../data/vocab.txt\n",
      "[jieba] load default dict ../data/vocab.txt ...\n",
      "[jieba] load default dict ../data/vocab.txt ...\n",
      ">> Synonyms load wordseg dict [/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/synonyms/data/vocab.txt] ... \n",
      ">> Synonyms on loading stopwords [/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/synonyms/data/stopwords.txt] ...\n",
      "[Synonyms] on loading vectors [/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/synonyms/data/words.vector.gz] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['爸爸', '妈妈', '爷爷', '奶奶', '老婆', '姐姐', '爸妈', '老公', '老爸', '外婆'],\n",
       " [1.0,\n",
       "  0.9197904,\n",
       "  0.8369896,\n",
       "  0.83030784,\n",
       "  0.80404544,\n",
       "  0.79813045,\n",
       "  0.79792285,\n",
       "  0.7714373,\n",
       "  0.77116084,\n",
       "  0.75844836])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms.nearby('爸爸')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## EDA（Easy Data Augmentation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![UDA1](https://img-blog.csdnimg.cn/c5ffcca4482c4c42beb6f1215e37657c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### EDA：用于提高文本分类任务性能的简单数据增强技术。 EDA 由四个简单但功能强大的操作组成：同义词替换、随机插入、随机交换和随机删除。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![EDA2](https://img-blog.csdnimg.cn/3fc3cec8b4d24c30939a92c1fac93b35.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 之前的工作已经提出了一些 NLP 中数据增强的技术，回译通过将句子翻译成法语然后再翻译成英语来生成新数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![EDA3](https://img-blog.csdnimg.cn/50c22b4212714b509ce053ff921d6bdd.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 对于训练集中的给定句子，随机选择并执行以下操作之一：\n",
    "* 同义词替换（SR）：从句子中随机选择 n 个不是停用词的词。 用随机选择的同义词之一替换这些单词中的每一个。\n",
    "* 随机插入 (RI)：在句子中随机找到一个词，并找出其同义词，且该同义词不是停用词。 将该同义词插入句子中的随机位置。 这样做n次。\n",
    "* 随机交换（RS）：随机选择句子中的两个单词并交换它们的位置。 这样做n次。\n",
    "* 随机删除（RD）：以概率 p 随机删除句子中的每个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = {word.strip() for word in open('baidu_stopwords.txt', 'r', encoding='utf8').readlines()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 同义词替换（SR）：从句子中随机选择 n 个不是停用词的词。 用随机选择的同义词之一替换这些单词中的每一个。\n",
    "# import random\n",
    "# def get_synonyms(word):\n",
    "#     # (['mama'],['0.9'])\n",
    "#     sys = set(synonyms.nearby(word)[0])\n",
    "#     if word in sys:\n",
    "#         sys.remove(word)\n",
    "#     return list(sys)\n",
    "\n",
    "# # 给力  ([],[])\n",
    "# def synonym_replacement(words, n):\n",
    "#     new_words = words.copy()\n",
    "#     # 不在停用词表\n",
    "#     random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "#     random.shuffle(random_word_list)\n",
    "#     num_replaced = 0\n",
    "#     for random_word in random_word_list:\n",
    "#         synonyms = get_synonyms(random_word)\n",
    "#         if len(synonyms) >=1 :\n",
    "#             synonym = random.choice(list(synonyms))\n",
    "#             new_words = [synonym if word == random_word else word for word in new_words]\n",
    "#             num_replaced += 1\n",
    "#         if num_replaced >= n:\n",
    "#             break\n",
    "    \n",
    "#     # new_words = [ ]\n",
    "#     sentence = ' '.join(new_words)\n",
    "#     new_words = sentence.split(' ')\n",
    "#     return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 存在一种情况  actor -> film star\n",
    "sentence = ['in', 'actor']\n",
    "new_words = ['in', 'film', 'star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 随机删除（RD）：以概率 p 随机删除句子中的每个单词。\n",
    "def random_deletion(words, p):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    \n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r>p:\n",
    "            new_words.append(word)\n",
    "    \n",
    "    if len(new_words) == 0:\n",
    "        random_int = random.randint(0, len(words)-1)\n",
    "        return [words[random_int]]\n",
    "    \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 随机交换（RS）：随机选择句子中的两个单词并交换它们的位置。 这样做n次。\n",
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "    random_idx_2 = random_idx_1\n",
    "\n",
    "    counter = 0\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "    return new_words\n",
    "\n",
    "def random_swap(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 随机插入 (RI)：在句子中随机找到一个词，并找出其同义词，且该同义词不是停用词。 将该同义词插入句子中的随机位置。 这样做n次。\n",
    "\n",
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1:\n",
    "        # 在句子中随机找到一个词\n",
    "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return \n",
    "    \n",
    "    random_synonym = synonyms[0]\n",
    "    # 取出要插入的位置\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "def random_insert(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "# num_aug 增加了几条数据\n",
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "    words = synonyms.seg(sentence)[0]\n",
    "    num_words = len(words)\n",
    "    augmented_sentence = []\n",
    "    # 3\n",
    "    num_new_per_technique = int(num_aug/4) + 1\n",
    "\n",
    "    n_sr = max(1, int(alpha_sr*num_words))\n",
    "    n_ri = max(1, int(alpha_ri*num_words))\n",
    "    n_rs = max(1, int(alpha_rs*num_words))\n",
    "    \n",
    "    # 同义词替换\n",
    "    # for _ in range(num_new_per_technique):\n",
    "    #     a_words = synonym_replacement(words, n_sr)\n",
    "    #     # a_words []\n",
    "    #     print(a_words)\n",
    "    #     augmented_sentence.append(' '.join(a_words))\n",
    "    \n",
    "    # 随机插入\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_insert(words, n_ri)\n",
    "        augmented_sentence.append(' '.join(a_words))\n",
    "\n",
    "    # 随机交换\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_swap(words, n_rs)\n",
    "        augmented_sentence.append(' '.join(a_words))\n",
    "    \n",
    "    # 随机删除\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_deletion(words, p_rd)\n",
    "        augmented_sentence.append(' '.join(a_words))\n",
    "    \n",
    "    shuffle(augmented_sentence)\n",
    "\n",
    "    if num_aug>=1:\n",
    "        augmented_sentence = augmented_sentence[:num_aug]\n",
    "    \n",
    "    else: #num_aug<1\n",
    "        keep_prob = num_aug/len(augmented_sentence)\n",
    "        # random_delete\n",
    "        augmented_sentence = [s for s in augmented_sentence if random.uniform(0,1)<keep_prob]\n",
    "    \n",
    "    return augmented_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['目前', '华为', '部分', '型号', '的', '手机', '产品', '出现', '货', '少', '的', '现象'], ['t', 'nr', 'n', 'n', 'uj', 'n', 'n', 'v', 'n', 'a', 'uj', 'n'])\n"
     ]
    }
   ],
   "source": [
    "words = synonyms.seg('目前华为部分型号的手机产品出现货少的现象')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['若 月 15 日 以来 的 台积电 、 ， 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 ， 相关 许可证 ， 都 无法 供应 芯片 的 华为 ， 而 中芯国际 等 将 手机 企业 ， 也 因 采用 美国 技术 高通 而 无法 供货 给 华为 。 目前 华为 部分 型号 给 手机 产品 出现 货 少 的 现象 ， 9 该 形势 持续 下去 ， 华为 芯片 业务 国产 遭受 重创 。',\n",
       " '9 采用 15 日 以来 ， 台积电 ， 高通 、 三星 等 华为 的 将 合作伙伴 ， 只要 没有 美国 若 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 月 美国 技术 ， 而 无法 供货 给 华为 。 华为 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 重创 该 形势 持续 下去 、 目前 手机 的 重要 遭受 业务 。',\n",
       " '9 月 15 日 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 给 华为 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 相关人员 等 EMC 中央处理器 反倒 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 初期 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 猪仔 。 目前 华为 部分 型号 的 贵 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月 15 日 ， 台积电 、 高通 、 等 华为 的 重要 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 而 中芯国际 等 国产 芯片 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 目前 华为 部分 的 手机 产品 货 少 的 现象 ， 若 该 形势 下去 ， 华为 手机 业务 遭受 重创 。',\n",
       " '9 业务 15 日 以来 ， 台积电 、 高通 都 三星 等 华为 的 将 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 、 无法 因 芯片 给 华为 部分 而 中芯国际 等 国产 芯片 企业 ， 也 供应 采用 美国 技术 ， 而 无法 供货 给 华为 ， 目前 华为 ， 型号 的 手机 产品 出现 货 少 的 现象 。 若 该 形势 ， 下去 持续 华为 手机 月 重要 遭受 重创 。',\n",
       " '9 月 15 日 以来 ， 过程中将 台积电 、 高通 、 三星 等 反倒 华为 的 重要 经常出现 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 过程中将 给 华为 。 目前 华为 部分 型号 诸如此类 的 手机 产品 出现 货 少 的 放手 现象 ， 若 如 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 的 重要 合作伙伴 ， 只要 没有 的 相关 许可证 ， 都 供应 芯片 给 华为 ， 而 中芯国际 国产 芯片 ， 也 因 采用 美国 技术 ， 而 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 若 该 形势 持续 下去 ， 华为 业务 将 遭受 重创 。',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 商业伙伴 的 重要 合作伙伴 澳洲 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 在我看来 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 反倒 。 目前 华为 部分 型号 的 iPhone4 手机 产品 宏碁 出现 货 少 的 现象 ， 若 NVIDIA 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda('9月15日以来，台积电、高通、三星等华为的重要合作伙伴，只要没有美国的相关许可证，都无法供应芯片给华为，而中芯国际等国产芯片企业，也因采用美国技术，而无法供货给华为。目前华为部分型号的手机产品出现货少的现象，若该形势持续下去，华为手机业务将遭受重创。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 闭包数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "数据集中每条数据有两个句子 \\\n",
    "a, b, 1\\\n",
    "a, c, 1\\\n",
    "a, d, 0\\\n",
    "a~b, a~c => b~c\\\n",
    "a~b, ad不相似 => bd不相似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "def parse_data(path, data_type='train'):\n",
    "    sentence_a = []\n",
    "    sentence_b = []\n",
    "    labels = []\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
    "            line = json.loads(line)\n",
    "            sentence_a.append(line['sentence1'])\n",
    "            sentence_b.append(line['sentence2'])\n",
    "            if data_type != 'test':\n",
    "                labels.append(int(line['label']))\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns=['text_a', 'text_b', 'labels'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 203895.10it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = parse_data('data/data100821/train.json', data_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>蚂蚁借呗等额还款可以换成先息后本吗</td>\n",
       "      <td>借呗有先息到期还本吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>蚂蚁花呗说我违约一次</td>\n",
       "      <td>蚂蚁花呗违约行为是什么</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>帮我看一下本月花呗账单有没有结清</td>\n",
       "      <td>下月花呗账单</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>蚂蚁借呗多长时间综合评估一次</td>\n",
       "      <td>借呗得评估多久</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我的花呗账单是***，还款怎么是***</td>\n",
       "      <td>我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text_a                                 text_b  labels\n",
       "0    蚂蚁借呗等额还款可以换成先息后本吗                             借呗有先息到期还本吗       0\n",
       "1           蚂蚁花呗说我违约一次                            蚂蚁花呗违约行为是什么       0\n",
       "2     帮我看一下本月花呗账单有没有结清                                 下月花呗账单       0\n",
       "3       蚂蚁借呗多长时间综合评估一次                                借呗得评估多久       0\n",
       "4  我的花呗账单是***，还款怎么是***  我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元       1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为什么不能用蚂蚁借呗\n",
      "           text_a           text_b  labels\n",
      "3227   为什么不能用蚂蚁借呗           开通蚂蚁借呗       0\n",
      "30332  为什么不能用蚂蚁借呗  为什么我的淘宝提示可以蚂蚁借呗       0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2 entries, 3227 to 30332\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text_a  2 non-null      object\n",
      " 1   text_b  2 non-null      object\n",
      " 2   labels  2 non-null      int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 64.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for data in train_df.groupby(by=['text_a']):\n",
    "    # print(type(data))\n",
    "    # print(len(data))\n",
    "    # print(data[0]) # text_a句子\n",
    "    # print('-------')\n",
    "    # print(data[1]) # 以text_a为主键返回的dataFrame\n",
    "    if len(data[1])==2:\n",
    "        print(data[0])\n",
    "        print(data[1])\n",
    "        print(data[1].info())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "a : 为什么我开通不了花呗 b: 我一直想买苹果 p，没钱，想分期付款，除了花呗，还有什么可以  c:我开通不了蚂蚁花呗 \n",
    "ab 不相似， ac相似，=》bc不相似\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def aug_group_by_a(df):\n",
    "    aug_data = defaultdict(list)\n",
    "    # 以text_a中的句子为 a\n",
    "    for g, data in df.groupby(by=['text_a']):\n",
    "        if len(data)<2:\n",
    "            continue\n",
    "        for i in range(len(data)):\n",
    "            for j in range(i+1, len(data)):\n",
    "                # 取出b的值，a,b的label\n",
    "                row_i_text = data.iloc[i, 1]\n",
    "                row_i_label = data.iloc[i, 2]\n",
    "\n",
    "                # 取出c的值，a,c的label\n",
    "                row_j_text = data.iloc[j, 1]\n",
    "                row_j_label = data.iloc[j, 2]\n",
    "\n",
    "                if row_i_label == row_j_label == 0:\n",
    "                    continue\n",
    "                \n",
    "                aug_label = 1 if row_i_label == row_j_label == 1 else 0 \n",
    "\n",
    "                aug_data['text_a'].append(row_i_text)\n",
    "                aug_data['text_b'].append(row_j_text)\n",
    "                aug_data['labels'].append(aug_label)\n",
    "    return pd.DataFrame(aug_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               text_a                   text_b  labels\n",
      "0    我一直想买苹果***p，没钱，想分期付款，除了花呗，还有什么可以                我开通不了蚂蚁花呗       0\n",
      "1    我一直想买苹果***p，没钱，想分期付款，除了花呗，还有什么可以  你直接帮我查一下 确实开通不了花呗吗？我的账户       0\n",
      "2                          为什么不可以开通花呗                我开通不了蚂蚁花呗       0\n",
      "3                          为什么不可以开通花呗  你直接帮我查一下 确实开通不了花呗吗？我的账户       0\n",
      "4                            我为何打不开花呗                我开通不了蚂蚁花呗       0\n",
      "..                                ...                      ...     ...\n",
      "223                           你是人工服务吗                   需求人工客服       0\n",
      "224                           你是人工服务吗                   联系人工客服       1\n",
      "225                            需求人工客服                   联系人工客服       0\n",
      "226                     身份证过期可以用蚂蚁借呗吗           身份证过期可以注册蚂蚁借呗吗       0\n",
      "227                            人工关闭花呗     一个帐户的花呗关，另一个帐户的花呗怎么开       0\n",
      "\n",
      "[228 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "aug_train_a = aug_group_by_a(train_df)\n",
    "print(aug_train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "现在是休息时间，看录播的同学可以跳过哦～: 100%|██████████| 900/900 [15:02<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(60*15), desc='现在是休息时间，看录播的同学可以跳过哦～'):\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## UDA（Unsupervised Data Augmentation for Consistency Training）用于一致性训练的无监督数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " ![UDA1](https://img-blog.csdnimg.cn/c9cb603261a1497c8093ee669b7923f2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![UDA2](https://img-blog.csdnimg.cn/3e1a474a04564517bf8dafd3c017ec82.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 一致性训练\n",
    "### 数据增强 是 创建 逼近 真实的训练数据，并且不改变其标签\n",
    "### a, b 1  a c 1 -> b, c 1\n",
    "### 增强前和增强后 标签 应该保持一致，利用这个特性训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![UDA4](https://img-blog.csdnimg.cn/fa4442776d67471ab8e5c707d8ff6752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "$q(\\hat{x}|x)$ 根据原始example $x$ 绘制 $\\hat{x}$ 并且要求$x$与$\\hat{x}$标签相同\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![UDA3](https://img-blog.csdnimg.cn/519f920759584cb6aeefce3d978974dd.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![unsupervised data augmentation](https://img-blog.csdnimg.cn/86f3aad1042b4bf381d662e9c3b48f0a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " 半监督学习利用无标签样本增强模型鲁棒性\n",
    " * 给定输入x, 计算输出分布$p_{\\theta}(y|x)$, 同时，给输入x进行数据增强，计算出分布$p_{\\theta}(y|\\hat x)$.\n",
    " * 给两个分布计算KL散度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![UDA5](https://img-blog.csdnimg.cn/9d10da70d1d0467e93ef5bb1267ac87f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![code](https://img-blog.csdnimg.cn/d97f35fd41e0485185f40d50f4fd8e8d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bucket_sampler import SortedSampler, BucketBatchSampler\n",
    "from EMA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "config = {\n",
    "        'train_file_path': 'data/data100821/train.json',\n",
    "        'dev_file_path': 'data/data100821/dev.json',\n",
    "        'test_file_path': 'data/data100821/test.json',\n",
    "        'output_path': '.',\n",
    "        'model_path': 'data/data94445',\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 1,\n",
    "        'max_seq_len': 64,\n",
    "        'learning_rate': 2e-5,\n",
    "        'weight_decay': 0.01,\n",
    "        'use_bucket': True,\n",
    "        'bucket_multiplier': 200,\n",
    "        'unsup_data_ratio': 1.5,\n",
    "        'uda_softmax_temp': 0.4,\n",
    "        'uda_confidence_threshold': 0.8,\n",
    "        'device': 'cuda',\n",
    "        'n_gpus': 0,\n",
    "        'logging_step': 400,\n",
    "        'ema_start_step': 500,\n",
    "        'ema_start': False,\n",
    "        'seed': 2021\n",
    "    }\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    config['device'] = 'cpu'\n",
    "else:\n",
    "    config['n_gpus'] = torch.cuda.device_count()\n",
    "    config['batch_size'] *= config['n_gpus']\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting transformers==4.0.1\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 15.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging (from transformers==4.0.1)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/3c/77/e2362b676dc5008d81be423070dd9577fa03be5da2ba1105811900fda546/packaging-21.0-py3-none-any.whl (40kB)\n",
      "\u001b[K     |████████████████████████████████| 40kB 19.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (2020.7.14)\n",
      "Collecting filelock (from transformers==4.0.1)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (2.22.0)\n",
      "Collecting tokenizers==0.9.4 (from transformers==4.0.1)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/fb/36/59e4a62254c5fcb43894c6b0e9403ec6f4238cc2422a003ed2e6279a1784/tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 14.6MB/s eta 0:00:01     |█████████████████▏              | 1.6MB 14.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (0.0.43)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (1.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (4.45.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->transformers==4.0.1) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->transformers==4.0.1) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->transformers==4.0.1) (1.25.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->transformers==4.0.1) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->transformers==4.0.1) (3.0.4)\n",
      "Requirement already satisfied: click in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (7.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (0.14.1)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
      "Installing collected packages: packaging, filelock, tokenizers, transformers\n",
      "Successfully installed filelock-3.0.12 packaging-21.0 tokenizers-0.9.4 transformers-4.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers==4.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(config['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
    "    inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens=True,\n",
    "                          return_token_type_ids=True, return_attention_mask=True)\n",
    "    \n",
    "    inputs['input_ids'].append(inputs_dict['input_ids'])\n",
    "    inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
    "    inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
    "    inputs['labels'].append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 对偶数据增强\n",
    "sentence1: a\n",
    "sentence2: b\n",
    "a,b, 1 变成 b,a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 无监督BERT输入\n",
    "def build_unsup_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
    "    lr_inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens=True,\n",
    "                          return_token_type_ids=True, return_attention_mask=True)\n",
    "                     \n",
    "    rl_inputs_dict = tokenizer.encode_plus(sentence_b, sentence_a, add_special_tokens=True,\n",
    "                          return_token_type_ids=True, return_attention_mask=True)\n",
    "    \n",
    "    inputs['input_ids'].append((lr_inputs_dict['input_ids'], rl_inputs_dict['input_ids']))\n",
    "    inputs['token_type_ids'].append((lr_inputs_dict['token_type_ids'],rl_inputs_dict['token_type_ids']))\n",
    "    inputs['attention_mask'].append((lr_inputs_dict['attention_mask'],rl_inputs_dict['attention_mask']))\n",
    "    inputs['labels'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(config, tokenizer):\n",
    "    train_df = parse_data(config['train_file_path'], data_type='train')\n",
    "    dev_df = parse_data(config['dev_file_path'], data_type='dev')\n",
    "    test_df = parse_data(config['test_file_path'], data_type='test')\n",
    "\n",
    "    data_df = {'train': train_df, 'dev': dev_df, 'test': test_df}\n",
    "    processed_data = {}\n",
    "    unsup_data = defaultdict(list)\n",
    "    for data_type, df in data_df.items():\n",
    "        inputs = defaultdict(list)\n",
    "        if data_type == 'train':\n",
    "            reversed_inputs = defaultdict(list)\n",
    "        \n",
    "        for i, row in tqdm(df.iterrows(), desc=f'Preprocessing {data_type} data', total=len(df)):\n",
    "            label=0 if data_type == 'test' else row[2]\n",
    "            sentence_a, sentence_b = row[0], row[1]\n",
    "            build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer)\n",
    "            \n",
    "            # 测试时刻增强\n",
    "            if data_type.startswith('test'):\n",
    "                build_bert_inputs(inputs, label, sentence_b, sentence_a, tokenizer)\n",
    "            \n",
    "            # train, dev, test\n",
    "            build_unsup_bert_inputs(unsup_data, label, sentence_a, sentence_b, tokenizer)\n",
    "        \n",
    "        processed_data[data_type] = inputs\n",
    "    \n",
    "    processed_data['unsup_data'] = unsup_data\n",
    "    return processed_data\n",
    "\n",
    "# processed_data\n",
    "# {\n",
    "#    'train':,\n",
    "#    'dev':,\n",
    "#    'test':,\n",
    "#    'unsup_data':   # 数据量最大的\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 196398.23it/s]\n",
      "Reading dev data: 100%|██████████| 4316/4316 [00:00<00:00, 174533.51it/s]\n",
      "Reading test data: 100%|██████████| 3861/3861 [00:00<00:00, 172694.01it/s]\n",
      "Preprocessing train data: 100%|██████████| 34334/34334 [01:03<00:00, 544.42it/s]\n",
      "Preprocessing dev data: 100%|██████████| 4316/4316 [00:07<00:00, 561.62it/s]\n",
      "Preprocessing test data: 100%|██████████| 3861/3861 [00:09<00:00, 413.86it/s]\n"
     ]
    }
   ],
   "source": [
    "data = read_data(config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class AFQMCDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        super(AFQMCDataset, self).__init__()\n",
    "        self.data_dict = data_dict\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = (self.data_dict['input_ids'][index], self.data_dict['token_type_ids'][index],\n",
    "                self.data_dict['attention_mask'][index], self.data_dict['labels'][index])\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "# 无监督BERT输入\n",
    "def build_unsup_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
    "    lr_inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens=True,\n",
    "                          return_token_type_ids=True, return_attention_mask=True)\n",
    "                     \n",
    "    rl_inputs_dict = tokenizer.encode_plus(sentence_b, sentence_a, add_special_tokens=True,\n",
    "                          return_token_type_ids=True, return_attention_mask=True)\n",
    "    \n",
    "    inputs['input_ids'].append((lr_inputs_dict['input_ids'], rl_inputs_dict['input_ids']))\n",
    "    inputs['token_type_ids'].append((lr_inputs_dict['token_type_ids'],rl_inputs_dict['token_type_ids']))\n",
    "    inputs['attention_mask'].append((lr_inputs_dict['attention_mask'],rl_inputs_dict['attention_mask']))\n",
    "    inputs['labels'].append(label)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, max_seq_len, tokenizer):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def pad_and_truncate(self, input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len):\n",
    "        input_ids = torch.zeros((len(input_ids_list), max_seq_len), dtype=torch.long)\n",
    "        token_type_ids = torch.zeros_like(input_ids)\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        for i in range(len(input_ids_list)):\n",
    "            seq_len = len(input_ids_list[i])\n",
    "            if seq_len<=max_seq_len:\n",
    "                input_ids[i,:seq_len] = torch.tensor(input_ids_list[i], dtype=torch.long)\n",
    "                token_type_ids[i,:seq_len] = torch.tensor(token_type_ids_list[i], dtype=torch.long)\n",
    "                attention_mask[i,:seq_len] = torch.tensor(attention_mask_list[i], dtype=torch.long)\n",
    "            \n",
    "            else:\n",
    "                input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len-1]+[self.tokenizer.sep_token_id], dtype=torch.long)\n",
    "                token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len], dtype=torch.long)\n",
    "                attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len], dtype=torch.long)\n",
    "            \n",
    "        labels = torch.tensor(labels_list, dtype=torch.long)\n",
    "        return input_ids, token_type_ids, attention_mask, labels\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        input_ids_list, token_type_ids_list, attention_mask_list, labels_list= list(zip(*examples))\n",
    "        cur_max_seq_len = max(len(input_ids) for input_ids in input_ids_list)\n",
    "        max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask, labels = self.pad_and_truncate(input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len)\n",
    "        data_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collate_fn = Collator(config['max_seq_len'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UDA\n",
    "class UnsupAFQMCDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        super(UnsupAFQMCDataset, self).__init__()\n",
    "        self.data_dict = data_dict\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.data_dict['input_ids'][index]\n",
    "        token_type_ids = self.data_dict['token_type_ids'][index]\n",
    "        attention_mask = self.data_dict['attention_mask'][index]\n",
    "        labels = self.data_dict['labels'][index]\n",
    "        return (input_ids[0], token_type_ids[0], attention_mask[0],\n",
    "                input_ids[1], token_type_ids[1], attention_mask[1],\n",
    "                labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UnsupCollator(Collator):\n",
    "    def __init__(self, max_seq_len, tokenizer):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "  \n",
    "\n",
    "    def __call__(self, examples):\n",
    "        (ab_input_ids_list, ab_token_type_ids_list, ab_attention_mask_list,\n",
    "        ba_input_ids_list, ba_token_type_ids_list, ba_attention_mask_list, labels_list) = list(zip(*examples))\n",
    "\n",
    "        cur_max_seq_len = max(len(input_ids) for input_ids in ab_input_ids_list)\n",
    "        max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
    "\n",
    "        # 分批整ab, ba\n",
    "        ab_input_ids, ab_token_type_ids, ab_attention_mask, labels = self.pad_and_truncate(ab_input_ids_list, ab_token_type_ids_list, ab_attention_mask_list, labels_list, max_seq_len)\n",
    "        ba_input_ids, ba_token_type_ids, ba_attention_mask, labels = self.pad_and_truncate(ba_input_ids_list, ba_token_type_ids_list, ba_attention_mask_list, labels_list, max_seq_len)\n",
    "\n",
    "        data_dict = {\n",
    "            'ab_input_ids': ab_input_ids,\n",
    "            'ab_token_type_ids': ab_token_type_ids,\n",
    "            'ab_attention_mask': ab_attention_mask,\n",
    "            'ba_input_ids': ba_input_ids,\n",
    "            'ba_token_type_ids': ba_token_type_ids,\n",
    "            'ba_attention_mask': ba_attention_mask,\n",
    "            'labels': labels\n",
    "\n",
    "        }\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "def build_dataloader(config, data, tokenizer):\n",
    "    train_dataset = AFQMCDataset(data['train'])\n",
    "    dev_dataset = AFQMCDataset(data['dev'])\n",
    "    test_dataset = AFQMCDataset(data['test'])\n",
    "\n",
    "    unsup_dataset = UnsupAFQMCDataset(data['unsup_data'])\n",
    "\n",
    "    collate_fn = Collator(config['max_seq_len'], tokenizer)\n",
    "    unsup_collate_fn = UnsupCollator(config['max_seq_len'], tokenizer)\n",
    "\n",
    "    if config['use_bucket']:\n",
    "        # 监督数据\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "        bucket_sampler = BucketBatchSampler(train_sampler, batch_size=config['batch_size'],\n",
    "                                           drop_last=False, sort_key=lambda x:len(train_dataset[x][0]),\n",
    "                                           bucket_size_multiplier=config['bucket_multiplier'])\n",
    "\n",
    "        train_dataloder = DataLoader(dataset=train_dataset, batch_sampler=bucket_sampler, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "        # 无监督数据\n",
    "        unsup_sampler = RandomSampler(unsup_dataset)\n",
    "        unsup_bucket_sampler = BucketBatchSampler(unsup_sampler, batch_size=int(config['batch_size']*config['unsup_data_ratio']),\n",
    "                                           drop_last=False, sort_key=lambda x:len(train_dataset[x][0]),\n",
    "                                           bucket_size_multiplier=config['bucket_multiplier'])\n",
    "        unsup_dataloader = DataLoader(dataset=unsup_dataset, batch_sampler=unsup_bucket_sampler, num_workers=4, collate_fn=unsup_collate_fn)\n",
    "    \n",
    "    else:\n",
    "        # 监督数据\n",
    "        train_dataloder = DataLoader(dataset=train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "        # 无监督数据\n",
    "        unsup_dataloader = DataLoader(dataset=unsup_dataset, batch_size=int(config['batch_size']*config['unsup_data_ratio']), shuffle=True,\n",
    "                                      num_workers=4, collate_fn=unsup_collate_fn)\n",
    "    \n",
    "    dev_dataloader = DataLoader(dataset=dev_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "    return unsup_dataloader, train_dataloder, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unsup_dataloader, train_dataloder, dev_dataloader, test_dataloader = build_dataloader(config, data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 6010, 6009,  955, 1446, 5543, 5314, 2769,  115,  115,  115,  674,\n",
      "         4638, 7583, 2428,  720, 8024, 6468, 6468,  102, 6010, 6009,  955, 1446,\n",
      "         6820,  749, 3621, 3300, 7583, 2428, 1408,  102],\n",
      "        [ 101, 2582, 3416, 1377,  809, 2990, 1184, 6820, 3378,  671, 3309, 4638,\n",
      "         5709, 1446,  102, 5709, 1446, 1146, 3309,  749,  809, 1400, 8024, 2990,\n",
      "         1184, 6820, 3621, 1377,  809, 1048, 1343,  102],\n",
      "        [ 101, 2769, 4638, 5709, 1446, 3300, 3867, 6589, 8024, 2769, 2682, 2990,\n",
      "         1184, 6820, 3926,  102, 5709, 1446, 2990, 1184, 6820, 3621, 8024, 6821,\n",
      "          702, 1216, 5543, 3766, 3300, 2823, 1168,  102],\n",
      "        [ 101, 2769, 6821, 3221, 5709, 1446, 4638, 3297,  856, 6820, 3621, 6848,\n",
      "         7555, 8024, 6820,  749,  115,  115,  115,  102, 5709, 1446, 8024, 3297,\n",
      "          856, 6820, 3621,  671, 4684, 1927, 6571,  102],\n",
      "        [ 101, 3118,  802, 2140, 1384, 4772, 2940,  749, 8024, 5709, 1446, 6820,\n",
      "         3221, 1333, 3341, 4638, 1384, 4772, 8024, 2582,  720, 3121,  102, 5709,\n",
      "         1446, 1963,  862, 2940, 3118,  802, 2140,  102],\n",
      "        [ 101,  115,  115,  115, 3299,  819, 4638, 5709, 1446, 2362, 1296,  679,\n",
      "         5543, 1146, 3309, 6820, 3621,  102, 5709, 1446, 6572, 1296, 2347, 1139,\n",
      "          852, 3221, 1146, 3309,  679,  749, 4638,  102],\n",
      "        [ 101, 5709, 1446, 1146, 3309, 1400,  678,  702, 3299, 1377,  809,  831,\n",
      "         1044, 6820, 1146, 3309, 4638, 1408,  102,  955, 1446, 6572, 1296, 1139,\n",
      "         3341, 1400, 1377,  809, 1146, 3309, 1408,  102],\n",
      "        [ 101, 5709, 1446, 3119, 7178, 4638,  753, 5335, 4772, 1469, 3119, 7178,\n",
      "         4772,  671, 3416, 1408,  102, 4509, 6435, 4638, 3119, 7178, 4772, 8024,\n",
      "         5543, 3119, 1166,  782, 5709, 1446, 1408,  102],\n",
      "        [ 101, 3221, 2828, 5709, 1446, 6572, 1296, 1146,  775, 5314, 1962, 1351,\n",
      "         2798,  833, 4692, 1168, 6913, 6435, 3221, 1408,  102, 1963,  862, 6913,\n",
      "         6435, 3301, 1351, 2376, 6820, 5709, 1446,  102],\n",
      "        [ 101, 2769, 6821, 3221,  697,  702, 3118,  802, 2140, 8024, 1372, 5543,\n",
      "          671,  702, 2458, 6858, 5709, 1446, 1408,  102, 2769, 6821,  702, 1384,\n",
      "         3766, 3300, 2458, 6858, 5709, 1446, 1408,  102],\n",
      "        [ 101, 7444, 6206, 4007, 6639,  784,  720, 3416, 4638, 3340,  816, 2798,\n",
      "         5543, 2881, 3300,  955, 1446,  102, 2769,  679, 5543,  886, 4500,  955,\n",
      "         1446, 8024, 2208,  784,  720, 3340,  816,  102],\n",
      "        [ 101, 5143, 5320,  784,  720, 3198,  952, 5543, 2612, 1908,  955, 1446,\n",
      "         7583, 2428,  102, 2769,  955, 1446, 6820, 3621,  749,  784,  720, 3198,\n",
      "          952, 7583, 2428, 1377,  809, 2612, 1908,  102],\n",
      "        [ 101, 4500,  955, 1446, 6821,  720,  719,  749, 8024, 1168, 4385, 1762,\n",
      "         6963,  679,  679, 1285, 7583, 2428,  102,  711,  784,  720, 2769,  955,\n",
      "         1446, 7583, 2428, 2990, 1285,  679,  749,  102],\n",
      "        [ 101, 5709, 1446, 6842, 3621, 6842, 1726, 7583, 2428, 3221, 1415, 6820,\n",
      "         7444, 6820, 3621,  102, 5709, 1446, 7027, 4638, 7178,  679, 1357, 3221,\n",
      "         1415, 5543, 6820, 5709, 1446, 4638, 3621,  102],\n",
      "        [ 101, 2791, 6587, 3633, 1762, 2144, 2821, 2496,  704,  886, 4500,  955,\n",
      "         1446,  833, 2512, 1510, 2791, 6587, 2144, 2821, 1408,  102,  955, 1446,\n",
      "         2512, 1510, 5296,  678, 6587, 3621, 1408,  102],\n",
      "        [ 101, 5709, 1446, 6842, 3621, 3227, 4850, 2768, 1216,  852, 3221,  865,\n",
      "         7583,  679, 1359,  102, 2769, 4638, 5709, 1446, 6842, 3621, 2768, 1216,\n",
      "          711,  784,  720, 7032, 7583,  679, 1359,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1])}\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloder:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def evaluation(config, model, val_dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    val_loss = 0.\n",
    "    val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            labels.append(batch['labels'])\n",
    "            batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
    "            batch_cuda['mode'] = 'val'\n",
    "            loss, logits = model(**batch_cuda)[:2]\n",
    "\n",
    "            if config['n_gpus'] > 1:\n",
    "                loss = loss.mean()\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds.append(logits.argmax(dim=-1).detach().cpu())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    f1 = metrics.f1_score(labels, preds)\n",
    "    acc = metrics.accuracy_score(labels, preds)\n",
    "    return avg_val_loss, f1, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![BertForSequenceClassification](https://img-blog.csdnimg.cn/65a419a6c58a4b07883a3c91084e7cde.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![BertForSequenceClassification1](https://img-blog.csdnimg.cn/0c2a5f0612aa4e42a1dfdf0e0352e0f2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "class BertForAFQMC(BertForSequenceClassification):\n",
    "    def forward(self, input_ids, token_type_ids, atttention_mask, labels=None, mode='train'):\n",
    "        outputs = self.bert(input_ids, token_type_ids=token_type_ids, atttention_mask=atttention_mask, output_hidden_states=True)\n",
    "        \n",
    "        # [batch_size, hidden_size]\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.drop(pooled_output)\n",
    "\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits, )\n",
    "\n",
    "        if mode == 'val':\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels.view(-1))\n",
    "\n",
    "            outputs = (loss, ) +outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![addtitional Training1](https://img-blog.csdnimg.cn/5916fe8ae028469bb877d15a1ac566de.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![UDA5](https://img-blog.csdnimg.cn/1ddf28077b88449aa84e0391149467e4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tsa_threshold(total_steps, global_steps):\n",
    "    return np.exp((global_steps / total_steps - 1) * 5) / 2 + 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "$$CE = -plogq$$   \n",
    "$$E = -plogp$$\n",
    "p-真实分布（已知） q-预测分布\n",
    "$$KLDiv(p||q) = \\sum_{i=1}^{N}p(x_{i})(logp(x_{i})- logq(x_{i}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![cross entropy](https://img-blog.csdnimg.cn/4572c78d76624c49b01b96a1cba42279.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![KL](https://img-blog.csdnimg.cn/189e4bc953904c199afbc7e6a11e5d9a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import trange\n",
    "import os\n",
    "def train(config, train_dataloader, dev_dataloader, unsup_dataloader=None):\n",
    "    model = BertForAFQMC.from_pretrained(config['model_path'])\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    model.to(config['device'])\n",
    "    total_steps = len(unsup_dataloader) * config['num_epochs']\n",
    "    epoch_iterator = trange(config['num_epochs'])\n",
    "    global_steps = 0\n",
    "    train_loss = 0.\n",
    "    logging_loss = 0.\n",
    "    best_acc = 0.\n",
    "    best_model_path = ''\n",
    "\n",
    "    if config['n_gpus'] > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    train_iterator = iter(train_dataloader)\n",
    "    for _ in epoch_iterator:\n",
    "        unsup_iterator = tqdm(unsup_dataloader, desc='Training', total=len(unsup_dataloader))\n",
    "        model.train()\n",
    "        for unsup_batch in unsup_iterator:\n",
    "            cur_bs = unsup_batch['ab_input_ids'].size(0)\n",
    "            try:\n",
    "                sup_batch = next(train_iterator)\n",
    "            except StopIteration:\n",
    "                train_iterator = iter(train_dataloader)\n",
    "                sup_batch = next(train_iterator)\n",
    "\n",
    "            grad_data, no_grad_data = get_data(sup_batch, unsup_batch, config)\n",
    "\n",
    "            unsup_loss_mask, unsup_probs = forward_no_grad(no_grad_data, config, model)\n",
    "\n",
    "            loss, tsa_threshold, unsup_loss, sup_loss = forward_with_grad(\n",
    "                unsup_loss_mask, unsup_probs, config, cur_bs, model, grad_data, total_steps, global_steps\n",
    "            )\n",
    "            \n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            if config['ema_start']:\n",
    "                ema.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            global_steps += 1\n",
    "\n",
    "            unsup_iterator.set_postfix_str(f'running training loss: {loss.item():.4f}')\n",
    "        \n",
    "            if global_steps % config['logging_step'] == 0:\n",
    "                if global_steps >= config['ema_start_step'] and not config['ema_start']:\n",
    "                    print('\\n>>> EMA starting ...')\n",
    "                    config['ema_start'] = True\n",
    "                    ema = EMA(model.module if hasattr(model, 'module') else model, decay=0.999)\n",
    "\n",
    "                print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
    "                logging_loss = train_loss\n",
    "\n",
    "                if config['ema_start']:\n",
    "                    ema.apply_shadow()\n",
    "                val_loss, f1, acc = evaluation(config, model, dev_dataloader)\n",
    "\n",
    "                print_log = f'\\n>>> training loss: {print_train_loss:.6f}, valid loss: {val_loss:.6f}, '\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    model_save_path = os.path.join(config['output_path'],\n",
    "                                                   f'checkpoint-{global_steps}-{acc:.6f}')\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                    model_to_save.save_pretrained(model_save_path)\n",
    "                    best_acc = acc\n",
    "                    best_model_path = model_save_path\n",
    "\n",
    "                print_log += f'valid f1: {f1:.6f}, valid acc: {acc:.6f}'\n",
    "\n",
    "                print(print_log)\n",
    "                model.train()\n",
    "                if config['ema_start']:\n",
    "                    ema.restore()\n",
    "\n",
    "    return model, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model, best_model_path = train(config, train_dataloader, dev_dataloader, unsup_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "def predict(config, model, test_dataloader):\n",
    "    test_iterator = tqdm(test_dataloader, desc='Predicting', total=len(test_dataloader))\n",
    "    test_preds = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iterator:\n",
    "            batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
    "            logits = model(**batch_cuda)[0]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            test_preds.append(probs[:, 1].detach().cpu())\n",
    "    \n",
    "\n",
    "    test_preds = torch.cat(test_preds)\n",
    "    test_preds = torch.stack(test_preds.split(2), dim=0).mean(dim=1).numpy()\n",
    "    submission_path = os.path.join(config['output_path'], 'submission.tsv')\n",
    "    test_df = pd.DataFrame(data={'prediction': test_preds})\n",
    "    test_df.to_csv(submission_path, index=False, header=False, encoding='utf8', sep='\\t')\n",
    "    with ZipFile(os.path.join(config['output_path'], 'submission.zip'), 'w') as myzip:\n",
    "        myzip.write(submission_path, 'submission.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(config, model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.8.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
