{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting torch==1.6.0\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/5d/5e/35140615fc1f925023f489e71086a9ecc188053d263d3594237281284d82/torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8MB)\n",
      "\u001b[K     |████████████████████████████████| 748.8MB 9.1MB/s eta 0:00:011��████████████████████▎  | 686.2MB 8.0MB/s eta 0:00:08\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from torch==1.6.0) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from torch==1.6.0) (1.19.1)\n",
      "Installing collected packages: torch\n",
      "  Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "Successfully installed torch-1.6.0\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting torchvision==0.7.0\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/4d/b5/60d5eb61f1880707a5749fea43e0ec76f27dfe69391cdec953ab5da5e676/torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.9MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9MB 13.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch==1.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from torchvision==0.7.0) (1.6.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from torchvision==0.7.0) (7.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from torchvision==0.7.0) (1.19.1)\n",
      "Requirement already satisfied: future in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from torch==1.6.0->torchvision==0.7.0) (0.18.0)\n",
      "Installing collected packages: torchvision\n",
      "  Found existing installation: torchvision 0.5.0\n",
      "    Uninstalling torchvision-0.5.0:\n",
      "      Successfully uninstalled torchvision-0.5.0\n",
      "Successfully installed torchvision-0.7.0\n"
     ]
    }
   ],
   "source": [
    "! pip install torchvision==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "config = {\n",
    "        'train_file_path': 'data/data100821/train.json',\n",
    "        'dev_file_path': 'data/data100821/dev.json',\n",
    "        'test_file_path': 'data/data100821/test.json',\n",
    "        'output_path': '.',\n",
    "        'model_path': 'data/data94445',\n",
    "        'batch_size': 64,\n",
    "        'num_epochs': 1,\n",
    "        'max_seq_len': 64,\n",
    "        'decay': 0.995,\n",
    "        'kd_coeff': 1.0,\n",
    "        'learning_rate': 2e-5,\n",
    "        'warmup_ratio': 0.05,\n",
    "        'weight_decay': 0.01,\n",
    "        'use_bucket': True,\n",
    "        'bucket_multiplier': 200,\n",
    "        'device': 'cuda',\n",
    "        'n_gpus': 0,\n",
    "        'use_amp': True,  \n",
    "        'logging_step': 400,\n",
    "        'ema_start_step': 500,\n",
    "        'ema_start': False,\n",
    "        'seed': 2021\n",
    "    }\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    config['device'] = 'cpu'\n",
    "else:\n",
    "    config['n_gpus'] = torch.cuda.device_count()\n",
    "    config['batch_size'] *= config['n_gpus']\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(60*15), desc='现在是休息时间，看录播的同学可以跳过哦～'):\n",
    "    sleep(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Requirement already satisfied: transformers==4.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (4.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (21.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (1.19.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (2.22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (2020.7.14)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (4.45.0)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (0.9.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers==4.0.1) (0.0.43)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->transformers==4.0.1) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->transformers==4.0.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->transformers==4.0.1) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->transformers==4.0.1) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->transformers==4.0.1) (2.8)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
      "Requirement already satisfied: click in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (7.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers==4.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(config['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "def parse_data(path, data_type='train'):\n",
    "    sentence_a = []\n",
    "    sentence_b = []\n",
    "    labels = []\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
    "            line = json.loads(line)\n",
    "            sentence_a.append(line['sentence1'])\n",
    "            sentence_b.append(line['sentence2'])\n",
    "            if data_type != 'test':\n",
    "                labels.append(int(line['label']))\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns=['text_a', 'text_b', 'labels'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
    "    inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens=True,\n",
    "                                        return_token_type_ids=True, return_attention_mask=True)\n",
    "    inputs['input_ids'].append(inputs_dict['input_ids'])\n",
    "    inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
    "    inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
    "    inputs['labels'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "def read_data(config, tokenizer):\n",
    "    train_df = parse_data(config['train_file_path'], data_type='train')\n",
    "    dev_df = parse_data(config['dev_file_path'], data_type='dev')\n",
    "    test_df = parse_data(config['test_file_path'], data_type='test')\n",
    "\n",
    "    data_df = {'train': train_df, 'dev': dev_df, 'test': test_df}\n",
    "    processed_data = {}\n",
    "\n",
    "    for data_type, df in data_df.items():\n",
    "        inputs = defaultdict(list)\n",
    "        for i, row in tqdm(df.iterrows(), desc=f'Preprocessing {data_type} data', total=len(df)):\n",
    "            label = row[2]\n",
    "            sentence_a, sentence_b = row[0], row[1]\n",
    "            build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer)\n",
    "\n",
    "        processed_data[data_type] = inputs\n",
    "\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 260985.27it/s]\n",
      "Reading dev data: 100%|██████████| 4316/4316 [00:00<00:00, 262955.07it/s]\n",
      "Reading test data: 100%|██████████| 3861/3861 [00:00<00:00, 267752.52it/s]\n",
      "Preprocessing train data: 100%|██████████| 34334/34334 [00:17<00:00, 1978.58it/s]\n",
      "Preprocessing dev data: 100%|██████████| 4316/4316 [00:02<00:00, 1972.19it/s]\n",
      "Preprocessing test data: 100%|██████████| 3861/3861 [00:01<00:00, 1936.16it/s]\n"
     ]
    }
   ],
   "source": [
    "data = read_data(config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, max_seq_len, tokenizer):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def pad_and_truncate(self, input_ids_list, token_type_ids_list,\n",
    "                         attention_mask_list, labels_list, max_seq_len):\n",
    "        input_ids = torch.zeros((len(input_ids_list), max_seq_len), dtype=torch.long)\n",
    "        token_type_ids = torch.zeros_like(input_ids)\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        for i in range(len(input_ids_list)):\n",
    "            seq_len = len(input_ids_list[i])\n",
    "            if seq_len <= max_seq_len:\n",
    "                input_ids[i, :seq_len] = torch.tensor(input_ids_list[i], dtype=torch.long)\n",
    "                token_type_ids[i, :seq_len] = torch.tensor(token_type_ids_list[i], dtype=torch.long)\n",
    "                attention_mask[i, :seq_len] = torch.tensor(attention_mask_list[i], dtype=torch.long)\n",
    "            else:\n",
    "                input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len - 1] + [self.tokenizer.sep_token_id],\n",
    "                                            dtype=torch.long)\n",
    "                token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len], dtype=torch.long)\n",
    "                attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len], dtype=torch.long)\n",
    "\n",
    "        labels = torch.tensor(labels_list, dtype=torch.long)\n",
    "        return input_ids, token_type_ids, attention_mask, labels\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        input_ids_list, token_type_ids_list, attention_mask_list, labels_list = list(zip(*examples))\n",
    "        cur_max_seq_len = max(len(input_id) for input_id in input_ids_list)\n",
    "        max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask, labels = self.pad_and_truncate(input_ids_list, token_type_ids_list,\n",
    "                                                                                  attention_mask_list, labels_list,\n",
    "                                                                                  max_seq_len)\n",
    "\n",
    "        data_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collate_fn = Collator(config['max_seq_len'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class AFQMCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dict):\n",
    "        super(AFQMCDataset, self).__init__()\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = (self.data_dict['input_ids'][index], self.data_dict['token_type_ids'][index],\n",
    "                self.data_dict['attention_mask'][index], self.data_dict['labels'][index])\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "class SortedSampler(Sampler):\n",
    "    \"\"\" Samples elements sequentially, always in the same order.\n",
    "\n",
    "    Args:\n",
    "        data (iterable): Iterable data.\n",
    "        sort_key (callable): Specifies a function of one argument that is used to extract a\n",
    "            numerical comparison key from each list element.\n",
    "\n",
    "    Example:\n",
    "        >>> list(SortedSampler(range(10), sort_key=lambda i: -i))\n",
    "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, sort_key):\n",
    "        super().__init__(data)\n",
    "        self.data = data\n",
    "        self.sort_key = sort_key\n",
    "        zip_ = [(i, self.sort_key(row)) for i, row in enumerate(self.data)]\n",
    "        zip_ = sorted(zip_, key=lambda r: r[1])\n",
    "        self.sorted_indexes = [item[0] for item in zip_]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sorted_indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler, SubsetRandomSampler\n",
    "import math\n",
    "class BucketBatchSampler(BatchSampler):\n",
    "    \"\"\" `BucketBatchSampler` toggles between `sampler` batches and sorted batches.\n",
    "\n",
    "    Typically, the `sampler` will be a `RandomSampler` allowing the user to toggle between\n",
    "    random batches and sorted batches. A larger `bucket_size_multiplier` is more sorted and vice\n",
    "    versa.\n",
    "\n",
    "    Background:\n",
    "        ``BucketBatchSampler`` is similar to a ``BucketIterator`` found in popular libraries like\n",
    "        ``AllenNLP`` and ``torchtext``. A ``BucketIterator`` pools together examples with a similar\n",
    "        size length to reduce the padding required for each batch while maintaining some noise\n",
    "        through bucketing.\n",
    "\n",
    "        **AllenNLP Implementation:**\n",
    "        https://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/bucket_iterator.py\n",
    "\n",
    "        **torchtext Implementation:**\n",
    "        https://github.com/pytorch/text/blob/master/torchtext/data/iterator.py#L225\n",
    "\n",
    "    Args:\n",
    "        sampler (torch.data.utils.sampler.Sampler):\n",
    "        batch_size (int): Size of mini-batch.\n",
    "        drop_last (bool): If `True` the sampler will drop the last batch if its size would be less\n",
    "            than `batch_size`.\n",
    "        sort_key (callable, optional): Callable to specify a comparison key for sorting.\n",
    "        bucket_size_multiplier (int, optional): Buckets are of size\n",
    "            `batch_size * bucket_size_multiplier`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 sampler,\n",
    "                 batch_size,\n",
    "                 drop_last,\n",
    "                 sort_key,\n",
    "                 bucket_size_multiplier=100):\n",
    "        super().__init__(sampler, batch_size, drop_last)\n",
    "        self.sort_key = sort_key\n",
    "        self.bucket_sampler = BatchSampler(sampler,\n",
    "                                           min(batch_size * bucket_size_multiplier, len(sampler)),\n",
    "                                           False)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for bucket in self.bucket_sampler:\n",
    "            sorted_sampler = SortedSampler(bucket, self.sort_key)\n",
    "            for batch in SubsetRandomSampler(\n",
    "                    list(BatchSampler(sorted_sampler, self.batch_size, self.drop_last))):\n",
    "                yield [bucket[i] for i in batch]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return math.ceil(len(self.sampler) / self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "def build_dataloader(config, data, collate_fn):\n",
    "    train_dataset = AFQMCDataset(data['train'])\n",
    "    dev_dataset = AFQMCDataset(data['dev'])\n",
    "    test_dataset = AFQMCDataset(data['test'])\n",
    "    \n",
    "    if config['use_bucket']:\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "        bucket_sampler = BucketBatchSampler(train_sampler, batch_size=config['batch_size'],\n",
    "                                            drop_last=False, sort_key=lambda x: len(train_dataset[x][0]),  # 以 input_id 长度作为排序的指标\n",
    "                                            bucket_size_multiplier=config['bucket_multiplier'])\n",
    "        train_dataloader = DataLoader(dataset=train_dataset, batch_sampler=bucket_sampler,\n",
    "                                      num_workers=4, collate_fn=collate_fn)\n",
    "    else:\n",
    "        train_dataloader = DataLoader(dataset=train_dataset, batch_size=config['batch_size'],\n",
    "                                      shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "    dev_dataloader = DataLoader(dataset=dev_dataset, batch_size=config['batch_size'],\n",
    "                                shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=config['batch_size'],\n",
    "                                 shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "    return train_dataloader, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataloader, dev_dataloader, test_dataloader = build_dataloader(config, data, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101,  955, 1446,  ..., 6820, 3621,  102],\n",
      "        [ 101, 5709, 1446,  ..., 5709, 1446,  102],\n",
      "        [ 101, 2769, 4638,  ...,  679,  749,  102],\n",
      "        ...,\n",
      "        [ 101, 5709, 1446,  ..., 3621, 1408,  102],\n",
      "        [ 101, 1555, 2157,  ..., 1555, 2157,  102],\n",
      "        [ 101, 2769, 3221,  ..., 3082,  868,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def evaluation(config, model, val_dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    val_loss = 0.\n",
    "    val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            labels.append(batch['labels'])\n",
    "            batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
    "            loss, logits = model(**batch_cuda)[:2]\n",
    "\n",
    "            if config['n_gpus'] > 1:\n",
    "                loss = loss.mean()\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds.append(logits.argmax(dim=-1).detach().cpu())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    f1 = metrics.f1_score(labels, preds)\n",
    "    acc = metrics.accuracy_score(labels, preds)\n",
    "    return avg_val_loss, f1, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 自集成 和 自蒸馏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![自监督](https://img-blog.csdnimg.cn/e0bff606718a480b84b26b37c02c1651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![自集成](https://img-blog.csdnimg.cn/e7013e8b74424092be6658ff9ff866bc.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![self1](https://img-blog.csdnimg.cn/9857a9457bba49a2953f06206a09b2cf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "self-ensemble 为了进一步降低集成模型的复杂度，从而使用了一种更高效的集成方法，该方法将多个基本模型 与 参数平均相结合，而不是保留多个基本模型。\\\n",
    "使用知识蒸馏来提高微调效率。当前的BERT模型（学生模型），自集成模型（教师模型）。教师模型是 之前几个时间步长 的学生模型的平均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/9da9862031d64d628b10f1d6b5644729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/1ae9a6f0c02e43789351952ef79d8f2c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/2484928631e64646a4d17bc3452bc315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "自集成模型，在单个训练阶段组合不同时间步长的中间模型。将每个时间步的BERT视为基础模型，并将它们组合成一个自集成模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/16055adb9e0f4668ac25025956364ef7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/3027d580da3d443ca50a38f674077bc2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/5e7a8e4517684d23a9175a61f9a83394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# EMA:指数移动平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### \\theta_{t} 取平均之后的参数  \\tilde\\theta_{t}当前时间步的参数\n",
    "$$\\theta_{t}   = 0.99\\theta_{t-1} + (1-0.99)\\tilde\\theta_{t}$$\n",
    "$$\\theta_{t-1} = 0.99\\theta_{t-2} + (1-0.99)\\tilde\\theta_{t-1}$$\n",
    "$$\\theta_{t-2} = 0.99\\theta_{t-3} + (1-0.99)\\tilde\\theta_{t-2}$$\n",
    "$$\\theta_{t-3} = 0.99*0 + (1-0.99)\\tilde\\theta_{t-3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "$$\\theta_{t}=0.99\\theta_{t-1} + (1-0.99)\\tilde\\theta_{t}$$\n",
    "$$\\theta_{t}=0.99\\theta_{t-1} + 0.01 * \\tilde\\theta_{t}$$\n",
    "$$\\theta_{t}=0.01*\\tilde\\theta_{t}+0.99\\theta_{t-1}$$\n",
    "$$\\theta_{t}=0.01*\\tilde\\theta_{t}+0.99(0.99\\theta_{t-2} + 0.01*\\tilde\\theta_{t-1})$$\n",
    "$$\\theta_{t}=0.01*\\tilde\\theta_{t}+0.01*0.99*\\tilde\\theta_{t-1}+0.99*0.99*\\theta_{t-2}$$\n",
    "$$\\theta_{t}=0.01*\\tilde\\theta_{t}+0.01*0.99*\\tilde\\theta_{t-1}+0.99*0.99*(0.99*\\theta_{t-3} + 0.01*\\tilde\\theta_{t-2})$$\n",
    "$$\\theta_{t}=0.01*\\tilde\\theta_{t}+0.01*0.99*\\tilde\\theta_{t-1}+0.01*0.99*0.99*\\tilde\\theta_{t-2}+0.99*0.99*0.99*\\theta_{t-3}$$\n",
    "$$\\theta_{t}=0.01*\\tilde\\theta_{t}+0.01*0.99*\\tilde\\theta_{t-1}+0.01*0.99*0.99*\\tilde\\theta_{t-2}+0.01*0.99*0.99*0.99*\\tilde\\theta_{t-3})$$\n",
    "$$\\theta_{t}=0.01*0.99^{0}*\\tilde\\theta_{t}+0.01*0.99^{1}*\\tilde\\theta_{t-1}+0.01*0.99^{2}*\\tilde\\theta_{t-2}+0.01*0.99^{3}*\\tilde\\theta_{t-3})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "$$\\theta_{t} = decay\\theta_{t-1} + (1-decay)\\tilde\\theta_{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "自蒸馏平均：\\\n",
    "计算两个loss CE 和 MSE\\\n",
    "MSE（当前时间步的参数，前几个时间步的参数求平均（不包括当前时间步））"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/a3c4790799e34d81940a380c102c2a24.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        self.register()\n",
    "    \n",
    "    # EMA初始化\n",
    "    def register(self):  \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                # 把当前模型参数保存到副本中\n",
    "                self.shadow[name] = param.data.clone()\n",
    "    \n",
    "    # 每一步都要更新\n",
    "    def update(self):  \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                # 更新的参数\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                # 保存平均参数\n",
    "                self.shadow[name] = new_average.clone()\n",
    "    \n",
    "    # 评估的时候用\n",
    "    def apply_shadow(self): \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                # 把当前参数备份\n",
    "                self.backup[name] = param.data\n",
    "                # 用维护的平均参数 替换当前模型的参数 进行模型评估\n",
    "                param.data = self.shadow[name]\n",
    "    \n",
    "    # 恢复到训练时的参数\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "$$ \\theta_{t} = decay\\theta_{t-1} + (1-decay)\\tilde\\theta_{t}$$\n",
    "$$ \\theta_{t} = decay\\theta_{t-1} + \\tilde\\theta_{t} - decay\\tilde\\theta_{t}$$\n",
    "$$ \\theta_{t} = decay(\\theta_{t-1} - \\tilde\\theta_{t}) + \\tilde\\theta_{t} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "one_minus_decay = 1.0 - decay\n",
    "for s_param, param in zip(kd_model.parameters(), parameters):\n",
    "     # 指数移动平均\n",
    "     s_param.sub_(one_minus_decay * (s_param - param))\n",
    "```\n",
    "s_param：平均参数\n",
    "param：当前参数\n",
    "$$\\theta_{t} = \\theta_{t-1} - (1 - decay) * (\\theta_{t-1} - \\tilde\\theta_{t})$$\n",
    "$$\\theta_{t} = \\theta_{t-1} - (\\theta_{t-1} - \\tilde\\theta_{t}) + decay(\\theta_{t-1} - \\tilde\\theta_{t})$$\n",
    "$$\\theta_{t} = \\tilde\\theta_{t} + decay(\\theta_{t-1} - \\tilde\\theta_{t})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```             #\\theta_{t-1} param \\theta_{t-1}\n",
    "                for s_param, param in zip(kd_model.parameters(), parameters):\n",
    "                    s_param.sub_(one_minus_decay * (s_param - param))\n",
    "```\n",
    "$$ \\theta_{t} = \\theta_{t-1} - (1-decay)( \\theta_{t-1} - \\tilde\\theta_{t})$$\n",
    "$$ \\theta_{t} = \\theta_{t-1} - ( \\theta_{t-1} - \\tilde\\theta_{t}) + decay( \\theta_{t-1} - \\tilde\\theta_{t})$$\n",
    "$$ \\theta_{t} = \\tilde\\theta_{t} + decay( \\theta_{t-1} - \\tilde\\theta_{t})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from torch.cuda import amp\n",
    "from extra_file.extra_optim import *\n",
    "from extra_file.extra_pgd import *\n",
    "from extra_file.extra_fgm import *\n",
    "from extra_file.extra_loss import *\n",
    "from tqdm import trange\n",
    "import copy, os\n",
    "def train(config, train_dataloader, dev_dataloader):\n",
    "    model = BertForSequenceClassification.from_pretrained(config['model_path'])\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    scaler = amp.GradScaler(enabled=config['use_amp'])\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": config['weight_decay']},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config['learning_rate'],\n",
    "                      correct_bias=False, eps=1e-8)\n",
    "    optimizer = Lookahead(optimizer, 5, 1)\n",
    "    total_steps = config['num_epochs'] * len(train_dataloader)\n",
    "    lr_scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                        warmup_steps=int(config['warmup_ratio'] * total_steps),\n",
    "                                        t_total=total_steps)\n",
    "    model.to(config['device'])\n",
    "    epoch_iterator = trange(config['num_epochs'])\n",
    "    global_steps = 0\n",
    "    train_loss = 0.\n",
    "    logging_loss = 0.\n",
    "    best_acc = 0.\n",
    "    best_model_path = ''\n",
    "\n",
    "    if config['n_gpus'] > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    # -----------    new----------------#\n",
    "    # 定义 MSE loss. (x-y)**2\n",
    "    kd_loss_fct = nn.MSELoss()\n",
    "    # 复制BERT模型，得到Average\n",
    "    kd_model = copy.deepcopy(model)\n",
    "    # Average不需要反向传播\n",
    "    kd_model.eval()\n",
    "\n",
    "    for _ in epoch_iterator:\n",
    "\n",
    "        train_iterator = tqdm(train_dataloader, desc='Training', total=len(train_dataloader))\n",
    "        model.train()\n",
    "        for batch in train_iterator:\n",
    "            batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
    "            with amp.autocast(enabled=config['use_amp']):\n",
    "                loss, logits = model(**batch_cuda)[:2]\n",
    "                if config['n_gpus'] > 1:\n",
    "                    loss = loss.mean()\n",
    "                \n",
    "                # --------------------     new -----------------#\n",
    "                with torch.no_grad():\n",
    "                    # SDA 拿到标签\n",
    "                    kd_logits = kd_model(**batch_cuda)[1]\n",
    "                # SDA 使用 MSE计算损失\n",
    "                kd_loss = kd_loss_fct(logits, kd_logits)\n",
    "                # SDA 加权损失\n",
    "                loss += config['kd_coeff'] * kd_loss\n",
    "                # --------------------     new -----------------#\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            if config['ema_start']:\n",
    "                ema.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            global_steps += 1\n",
    "            \n",
    "            # --------------------     new -----------------#\n",
    "            # 修正偏差\n",
    "            decay = min(config['decay'], (1 + global_steps) / (10 + global_steps))\n",
    "\n",
    "            one_minus_decay = 1.0 - decay\n",
    "            # SDA 更新Average参数\n",
    "            with torch.no_grad():\n",
    "                parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "                # 指数移动平均\n",
    "                for s_param, param in zip(kd_model.parameters(), parameters):\n",
    "                    s_param.sub_(one_minus_decay * (s_param - param))\n",
    "            # --------------------     new -----------------#\n",
    "\n",
    "            train_iterator.set_postfix_str(f'running training loss: {loss.item():.4f}')\n",
    "\n",
    "            if global_steps % config['logging_step'] == 0:\n",
    "                if global_steps >= config['ema_start_step'] and not config['ema_start']:\n",
    "                    print('\\n>>> EMA starting ...')\n",
    "                    config['ema_start'] = True\n",
    "                    # --------------------     new -----------------#\n",
    "                    # if 多张GPU卡， model 会放在 model.module 属性里， else 返回 model\n",
    "                    ema = EMA(model.module if hasattr(model, 'module') else model, decay=0.999)\n",
    "                    # --------------------     new -----------------#\n",
    "                print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
    "                logging_loss = train_loss\n",
    "\n",
    "                if config['ema_start']:\n",
    "                    ema.apply_shadow()\n",
    "                val_loss, f1, acc = evaluation(config, model, dev_dataloader)\n",
    "\n",
    "                print_log = f'\\n>>> training loss: {print_train_loss:.6f}, valid loss: {val_loss:.6f}, '\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    model_save_path = os.path.join(config['output_path'],\n",
    "                                                   f'checkpoint-{global_steps}-{acc:.6f}')\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                    model_to_save.save_pretrained(model_save_path)\n",
    "                    best_acc = acc\n",
    "                    best_model_path = model_save_path\n",
    "\n",
    "                print_log += f'valid f1: {f1:.6f}, valid acc: {acc:.6f}'\n",
    "\n",
    "                print(print_log)\n",
    "                model.train()\n",
    "                if config['ema_start']:\n",
    "                    ema.restore()\n",
    "\n",
    "    return model, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at data/data94445 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at data/data94445 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:111: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Training:   0%|          | 0/537 [00:00<?, ?it/s]\u001b[A/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py:114: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "\n",
      "Training:   0%|          | 0/537 [00:20<?, ?it/s, running training loss: 0.6342]\u001b[A\n",
      "Training:   0%|          | 1/537 [00:20<3:05:29, 20.76s/it, running training loss: 0.6342]\u001b[A\n",
      "Training:   0%|          | 1/537 [00:25<3:05:29, 20.76s/it, running training loss: 0.6915]\u001b[A\n",
      "Training:   0%|          | 2/537 [00:25<2:22:59, 16.04s/it, running training loss: 0.6915]\u001b[A\n",
      "Training:   0%|          | 2/537 [00:40<2:22:59, 16.04s/it, running training loss: 0.6579]\u001b[A\n",
      "Training:   1%|          | 3/537 [00:40<2:19:23, 15.66s/it, running training loss: 0.6579]\u001b[A\n",
      "Training:   1%|          | 3/537 [00:46<2:19:23, 15.66s/it, running training loss: 0.6746]\u001b[A\n",
      "Training:   1%|          | 4/537 [00:46<1:52:21, 12.65s/it, running training loss: 0.6746]\u001b[A\n",
      "Training:   1%|          | 4/537 [00:52<1:52:21, 12.65s/it, running training loss: 0.7156]\u001b[A\n",
      "Training:   1%|          | 5/537 [00:52<1:36:15, 10.86s/it, running training loss: 0.7156]\u001b[A\n",
      "Training:   1%|          | 5/537 [00:59<1:36:15, 10.86s/it, running training loss: 0.6032]\u001b[A\n",
      "Training:   1%|          | 6/537 [00:59<1:26:06,  9.73s/it, running training loss: 0.6032]\u001b[A\n",
      "Training:   1%|          | 6/537 [01:08<1:26:06,  9.73s/it, running training loss: 0.5403]\u001b[A\n",
      "Training:   1%|▏         | 7/537 [01:08<1:22:09,  9.30s/it, running training loss: 0.5403]\u001b[A\n",
      "Training:   1%|▏         | 7/537 [01:15<1:22:09,  9.30s/it, running training loss: 0.6302]\u001b[A\n",
      "Training:   1%|▏         | 8/537 [01:15<1:17:16,  8.76s/it, running training loss: 0.6302]\u001b[A\n",
      "Training:   1%|▏         | 8/537 [01:22<1:17:16,  8.76s/it, running training loss: 0.4963]\u001b[A\n",
      "Training:   2%|▏         | 9/537 [01:22<1:10:51,  8.05s/it, running training loss: 0.4963]\u001b[A\n",
      "Training:   2%|▏         | 9/537 [01:28<1:10:51,  8.05s/it, running training loss: 0.8652]\u001b[A\n",
      "Training:   2%|▏         | 10/537 [01:28<1:05:23,  7.44s/it, running training loss: 0.8652]\u001b[A\n",
      "Training:   2%|▏         | 10/537 [01:35<1:05:23,  7.44s/it, running training loss: 0.6803]\u001b[A\n",
      "Training:   2%|▏         | 11/537 [01:35<1:05:05,  7.43s/it, running training loss: 0.6803]\u001b[A\n",
      "Training:   2%|▏         | 11/537 [01:41<1:05:05,  7.43s/it, running training loss: 0.6671]\u001b[A\n",
      "Training:   2%|▏         | 12/537 [01:41<1:01:28,  7.03s/it, running training loss: 0.6671]\u001b[A\n",
      "Training:   2%|▏         | 12/537 [01:49<1:01:28,  7.03s/it, running training loss: 0.6237]\u001b[A\n",
      "Training:   2%|▏         | 13/537 [01:49<1:02:36,  7.17s/it, running training loss: 0.6237]\u001b[A\n",
      "Training:   2%|▏         | 13/537 [02:04<1:02:36,  7.17s/it, running training loss: 0.7067]\u001b[A\n",
      "Training:   3%|▎         | 14/537 [02:04<1:22:57,  9.52s/it, running training loss: 0.7067]\u001b[A\n",
      "Training:   3%|▎         | 14/537 [02:10<1:22:57,  9.52s/it, running training loss: 0.6691]\u001b[A\n",
      "Training:   3%|▎         | 15/537 [02:10<1:14:41,  8.59s/it, running training loss: 0.6691]\u001b[A\n",
      "Training:   3%|▎         | 15/537 [02:15<1:14:41,  8.59s/it, running training loss: 0.7622]\u001b[A\n",
      "Training:   3%|▎         | 16/537 [02:15<1:05:43,  7.57s/it, running training loss: 0.7622]\u001b[A\n",
      "Training:   3%|▎         | 16/537 [02:25<1:05:43,  7.57s/it, running training loss: 0.6342]\u001b[A\n",
      "Training:   3%|▎         | 17/537 [02:25<1:11:11,  8.21s/it, running training loss: 0.6342]\u001b[A\n",
      "Training:   3%|▎         | 17/537 [02:31<1:11:11,  8.21s/it, running training loss: 0.6633]\u001b[A\n",
      "Training:   3%|▎         | 18/537 [02:31<1:05:14,  7.54s/it, running training loss: 0.6633]\u001b[A\n",
      "Training:   3%|▎         | 18/537 [02:38<1:05:14,  7.54s/it, running training loss: 0.6030]\u001b[A\n",
      "Training:   4%|▎         | 19/537 [02:38<1:03:14,  7.33s/it, running training loss: 0.6030]\u001b[A\n",
      "Training:   4%|▎         | 19/537 [02:43<1:03:14,  7.33s/it, running training loss: 0.7197]\u001b[A\n",
      "Training:   4%|▎         | 20/537 [02:43<57:53,  6.72s/it, running training loss: 0.7197]  \u001b[A\n",
      "Training:   4%|▎         | 20/537 [02:50<57:53,  6.72s/it, running training loss: 0.6541]\u001b[A\n",
      "Training:   4%|▍         | 21/537 [02:50<57:23,  6.67s/it, running training loss: 0.6541]\u001b[A\n",
      "Training:   4%|▍         | 21/537 [02:56<57:23,  6.67s/it, running training loss: 0.6956]\u001b[A\n",
      "Training:   4%|▍         | 22/537 [02:56<55:48,  6.50s/it, running training loss: 0.6956]\u001b[A\n",
      "Training:   4%|▍         | 22/537 [03:01<55:48,  6.50s/it, running training loss: 0.5545]\u001b[A\n",
      "Training:   4%|▍         | 23/537 [03:01<53:23,  6.23s/it, running training loss: 0.5545]\u001b[A\n",
      "Training:   4%|▍         | 23/537 [03:15<53:23,  6.23s/it, running training loss: 0.7986]\u001b[A\n",
      "Training:   4%|▍         | 24/537 [03:15<1:11:11,  8.33s/it, running training loss: 0.7986]\u001b[A\n",
      "Training:   4%|▍         | 24/537 [03:20<1:11:11,  8.33s/it, running training loss: 0.6332]\u001b[A\n",
      "Training:   5%|▍         | 25/537 [03:20<1:04:17,  7.53s/it, running training loss: 0.6332]\u001b[A\n",
      "Training:   5%|▍         | 25/537 [03:28<1:04:17,  7.53s/it, running training loss: 0.6642]\u001b[A\n",
      "Training:   5%|▍         | 26/537 [03:28<1:03:49,  7.49s/it, running training loss: 0.6642]\u001b[A\n",
      "Training:   5%|▍         | 26/537 [03:35<1:03:49,  7.49s/it, running training loss: 0.6025]\u001b[A\n",
      "Training:   5%|▌         | 27/537 [03:35<1:03:46,  7.50s/it, running training loss: 0.6025]\u001b[A\n",
      "Training:   5%|▌         | 27/537 [03:44<1:03:46,  7.50s/it, running training loss: 0.8102]\u001b[A\n",
      "Training:   5%|▌         | 28/537 [03:44<1:06:42,  7.86s/it, running training loss: 0.8102]\u001b[A\n",
      "Training:   5%|▌         | 28/537 [03:49<1:06:42,  7.86s/it, running training loss: 0.6703]\u001b[A\n",
      "Training:   5%|▌         | 29/537 [03:49<1:00:00,  7.09s/it, running training loss: 0.6703]\u001b[A\n",
      "Training:   5%|▌         | 29/537 [03:58<1:00:00,  7.09s/it, running training loss: 0.6414]\u001b[A\n",
      "Training:   6%|▌         | 30/537 [03:58<1:03:11,  7.48s/it, running training loss: 0.6414]\u001b[A\n",
      "Training:   6%|▌         | 30/537 [04:05<1:03:11,  7.48s/it, running training loss: 0.6293]\u001b[A\n",
      "Training:   6%|▌         | 31/537 [04:05<1:03:41,  7.55s/it, running training loss: 0.6293]\u001b[A\n",
      "Training:   6%|▌         | 31/537 [04:13<1:03:41,  7.55s/it, running training loss: 0.6918]\u001b[A\n",
      "Training:   6%|▌         | 32/537 [04:13<1:03:38,  7.56s/it, running training loss: 0.6918]\u001b[A\n",
      "Training:   6%|▌         | 32/537 [04:20<1:03:38,  7.56s/it, running training loss: 0.6598]\u001b[A\n",
      "Training:   6%|▌         | 33/537 [04:20<1:01:37,  7.34s/it, running training loss: 0.6598]\u001b[A\n",
      "Training:   6%|▌         | 33/537 [04:25<1:01:37,  7.34s/it, running training loss: 0.5472]\u001b[A\n",
      "Training:   6%|▋         | 34/537 [04:25<57:08,  6.82s/it, running training loss: 0.5472]  \u001b[A\n",
      "Training:   6%|▋         | 34/537 [04:32<57:08,  6.82s/it, running training loss: 0.6297]\u001b[A\n",
      "Training:   7%|▋         | 35/537 [04:32<56:59,  6.81s/it, running training loss: 0.6297]\u001b[A\n",
      "Training:   7%|▋         | 35/537 [04:40<56:59,  6.81s/it, running training loss: 0.5534]\u001b[A\n",
      "Training:   7%|▋         | 36/537 [04:40<58:33,  7.01s/it, running training loss: 0.5534]\u001b[A\n",
      "Training:   7%|▋         | 36/537 [04:46<58:33,  7.01s/it, running training loss: 0.5207]\u001b[A\n",
      "Training:   7%|▋         | 37/537 [04:46<56:43,  6.81s/it, running training loss: 0.5207]\u001b[A\n",
      "Training:   7%|▋         | 37/537 [04:50<56:43,  6.81s/it, running training loss: 0.7130]\u001b[A\n",
      "Training:   7%|▋         | 38/537 [04:50<50:46,  6.11s/it, running training loss: 0.7130]\u001b[A\n",
      "Training:   7%|▋         | 38/537 [04:57<50:46,  6.11s/it, running training loss: 0.6376]\u001b[A\n",
      "Training:   7%|▋         | 39/537 [04:57<51:40,  6.23s/it, running training loss: 0.6376]\u001b[A\n",
      "Training:   7%|▋         | 39/537 [05:02<51:40,  6.23s/it, running training loss: 0.6346]\u001b[A\n",
      "Training:   7%|▋         | 40/537 [05:02<48:04,  5.80s/it, running training loss: 0.6346]\u001b[A\n",
      "Training:   7%|▋         | 40/537 [05:10<48:04,  5.80s/it, running training loss: 0.7166]\u001b[A\n",
      "Training:   8%|▊         | 41/537 [05:10<54:51,  6.64s/it, running training loss: 0.7166]\u001b[A\n",
      "Training:   8%|▊         | 41/537 [05:16<54:51,  6.64s/it, running training loss: 0.6228]\u001b[A\n",
      "Training:   8%|▊         | 42/537 [05:16<53:42,  6.51s/it, running training loss: 0.6228]\u001b[A\n",
      "Training:   8%|▊         | 42/537 [05:22<53:42,  6.51s/it, running training loss: 0.6395]\u001b[A\n",
      "Training:   8%|▊         | 43/537 [05:22<50:51,  6.18s/it, running training loss: 0.6395]\u001b[A\n",
      "Training:   8%|▊         | 43/537 [05:31<50:51,  6.18s/it, running training loss: 0.5389]\u001b[A\n",
      "Training:   8%|▊         | 44/537 [05:31<58:13,  7.09s/it, running training loss: 0.5389]\u001b[A\n",
      "Training:   8%|▊         | 44/537 [05:38<58:13,  7.09s/it, running training loss: 0.6789]\u001b[A\n",
      "Training:   8%|▊         | 45/537 [05:38<57:08,  6.97s/it, running training loss: 0.6789]\u001b[A\n",
      "Training:   8%|▊         | 45/537 [05:44<57:08,  6.97s/it, running training loss: 0.5973]\u001b[A\n",
      "Training:   9%|▊         | 46/537 [05:44<55:07,  6.74s/it, running training loss: 0.5973]\u001b[A\n",
      "Training:   9%|▊         | 46/537 [05:50<55:07,  6.74s/it, running training loss: 0.5714]\u001b[A\n",
      "Training:   9%|▉         | 47/537 [05:50<53:59,  6.61s/it, running training loss: 0.5714]\u001b[A\n",
      "Training:   9%|▉         | 47/537 [05:59<53:59,  6.61s/it, running training loss: 0.7049]\u001b[A\n",
      "Training:   9%|▉         | 48/537 [05:59<59:40,  7.32s/it, running training loss: 0.7049]\u001b[A\n",
      "Training:   9%|▉         | 48/537 [06:06<59:40,  7.32s/it, running training loss: 0.6753]\u001b[A\n",
      "Training:   9%|▉         | 49/537 [06:06<57:17,  7.04s/it, running training loss: 0.6753]\u001b[A\n",
      "Training:   9%|▉         | 49/537 [06:13<57:17,  7.04s/it, running training loss: 0.5785]\u001b[A\n",
      "Training:   9%|▉         | 50/537 [06:13<56:49,  7.00s/it, running training loss: 0.5785]\u001b[A\n",
      "Training:   9%|▉         | 50/537 [06:20<56:49,  7.00s/it, running training loss: 0.5258]\u001b[A\n",
      "Training:   9%|▉         | 51/537 [06:20<57:24,  7.09s/it, running training loss: 0.5258]\u001b[A\n",
      "Training:   9%|▉         | 51/537 [06:27<57:24,  7.09s/it, running training loss: 0.8066]\u001b[A\n",
      "Training:  10%|▉         | 52/537 [06:27<58:03,  7.18s/it, running training loss: 0.8066]\u001b[A\n",
      "Training:  10%|▉         | 52/537 [06:33<58:03,  7.18s/it, running training loss: 0.7537]\u001b[A\n",
      "Training:  10%|▉         | 53/537 [06:33<53:38,  6.65s/it, running training loss: 0.7537]\u001b[A\n",
      "Training:  10%|▉         | 53/537 [06:39<53:38,  6.65s/it, running training loss: 0.6775]\u001b[A\n",
      "Training:  10%|█         | 54/537 [06:39<51:55,  6.45s/it, running training loss: 0.6775]\u001b[A\n",
      "Training:  10%|█         | 54/537 [06:46<51:55,  6.45s/it, running training loss: 0.6094]\u001b[A\n",
      "Training:  10%|█         | 55/537 [06:46<55:07,  6.86s/it, running training loss: 0.6094]\u001b[A\n",
      "Training:  10%|█         | 55/537 [06:56<55:07,  6.86s/it, running training loss: 0.6774]\u001b[A\n",
      "Training:  10%|█         | 56/537 [06:56<1:00:23,  7.53s/it, running training loss: 0.6774]\u001b[A\n",
      "Training:  10%|█         | 56/537 [07:04<1:00:23,  7.53s/it, running training loss: 0.6403]\u001b[A\n",
      "Training:  11%|█         | 57/537 [07:04<1:03:18,  7.91s/it, running training loss: 0.6403]\u001b[A\n",
      "Training:  11%|█         | 57/537 [07:10<1:03:18,  7.91s/it, running training loss: 0.5397]\u001b[A\n",
      "Training:  11%|█         | 58/537 [07:10<58:04,  7.27s/it, running training loss: 0.5397]  \u001b[A\n",
      "Training:  11%|█         | 58/537 [07:18<58:04,  7.27s/it, running training loss: 0.6617]\u001b[A\n",
      "Training:  11%|█         | 59/537 [07:18<59:02,  7.41s/it, running training loss: 0.6617]\u001b[A\n",
      "Training:  11%|█         | 59/537 [07:26<59:02,  7.41s/it, running training loss: 0.7162]\u001b[A\n",
      "Training:  11%|█         | 60/537 [07:26<1:00:33,  7.62s/it, running training loss: 0.7162]\u001b[A\n",
      "Training:  11%|█         | 60/537 [07:33<1:00:33,  7.62s/it, running training loss: 0.5572]\u001b[A\n",
      "Training:  11%|█▏        | 61/537 [07:33<58:28,  7.37s/it, running training loss: 0.5572]  \u001b[A\n",
      "Training:  11%|█▏        | 61/537 [07:40<58:28,  7.37s/it, running training loss: 0.6288]\u001b[A\n",
      "Training:  12%|█▏        | 62/537 [07:40<59:06,  7.47s/it, running training loss: 0.6288]\u001b[A\n",
      "Training:  12%|█▏        | 62/537 [07:45<59:06,  7.47s/it, running training loss: 0.6466]\u001b[A\n",
      "Training:  12%|█▏        | 63/537 [07:45<51:56,  6.58s/it, running training loss: 0.6466]\u001b[A\n",
      "Training:  12%|█▏        | 63/537 [07:53<51:56,  6.58s/it, running training loss: 0.6916]\u001b[A\n",
      "Training:  12%|█▏        | 64/537 [07:53<55:13,  7.01s/it, running training loss: 0.6916]\u001b[A\n",
      "Training:  12%|█▏        | 64/537 [07:58<55:13,  7.01s/it, running training loss: 0.6338]\u001b[A\n",
      "Training:  12%|█▏        | 65/537 [07:58<50:49,  6.46s/it, running training loss: 0.6338]\u001b[A\n",
      "Training:  12%|█▏        | 65/537 [08:03<50:49,  6.46s/it, running training loss: 0.6234]\u001b[A\n",
      "Training:  12%|█▏        | 66/537 [08:03<47:59,  6.11s/it, running training loss: 0.6234]\u001b[A\n",
      "Training:  12%|█▏        | 66/537 [08:09<47:59,  6.11s/it, running training loss: 0.6642]\u001b[A\n",
      "Training:  12%|█▏        | 67/537 [08:09<46:12,  5.90s/it, running training loss: 0.6642]\u001b[A\n",
      "Training:  12%|█▏        | 67/537 [08:18<46:12,  5.90s/it, running training loss: 0.5611]\u001b[A\n",
      "Training:  13%|█▎        | 68/537 [08:18<52:38,  6.74s/it, running training loss: 0.5611]\u001b[A\n",
      "Training:  13%|█▎        | 68/537 [08:28<52:38,  6.74s/it, running training loss: 0.7382]\u001b[A\n",
      "Training:  13%|█▎        | 69/537 [08:28<1:02:05,  7.96s/it, running training loss: 0.7382]\u001b[A\n",
      "Training:  13%|█▎        | 69/537 [08:33<1:02:05,  7.96s/it, running training loss: 0.6590]\u001b[A\n",
      "Training:  13%|█▎        | 70/537 [08:33<54:05,  6.95s/it, running training loss: 0.6590]  \u001b[A\n",
      "Training:  13%|█▎        | 70/537 [08:40<54:05,  6.95s/it, running training loss: 0.6419]\u001b[A\n",
      "Training:  13%|█▎        | 71/537 [08:40<54:49,  7.06s/it, running training loss: 0.6419]\u001b[A\n",
      "Training:  13%|█▎        | 71/537 [08:47<54:49,  7.06s/it, running training loss: 0.7157]\u001b[A\n",
      "Training:  13%|█▎        | 72/537 [08:47<54:46,  7.07s/it, running training loss: 0.7157]\u001b[A\n",
      "Training:  13%|█▎        | 72/537 [08:54<54:46,  7.07s/it, running training loss: 0.6345]\u001b[A\n",
      "Training:  14%|█▎        | 73/537 [08:54<54:01,  6.99s/it, running training loss: 0.6345]\u001b[A\n",
      "Training:  14%|█▎        | 73/537 [09:01<54:01,  6.99s/it, running training loss: 0.6391]\u001b[A\n",
      "Training:  14%|█▍        | 74/537 [09:01<54:40,  7.09s/it, running training loss: 0.6391]\u001b[A\n",
      "Training:  14%|█▍        | 74/537 [09:07<54:40,  7.09s/it, running training loss: 0.6201]\u001b[A\n",
      "Training:  14%|█▍        | 75/537 [09:07<50:27,  6.55s/it, running training loss: 0.6201]\u001b[A\n",
      "Training:  14%|█▍        | 75/537 [09:13<50:27,  6.55s/it, running training loss: 0.6138]\u001b[A\n",
      "Training:  14%|█▍        | 76/537 [09:13<48:59,  6.38s/it, running training loss: 0.6138]\u001b[A\n",
      "Training:  14%|█▍        | 76/537 [09:18<48:59,  6.38s/it, running training loss: 0.5321]\u001b[A\n",
      "Training:  14%|█▍        | 77/537 [09:18<47:06,  6.14s/it, running training loss: 0.5321]\u001b[A\n",
      "Training:  14%|█▍        | 77/537 [09:25<47:06,  6.14s/it, running training loss: 0.6696]\u001b[A\n",
      "Training:  15%|█▍        | 78/537 [09:25<47:36,  6.22s/it, running training loss: 0.6696]\u001b[A\n",
      "Training:  15%|█▍        | 78/537 [09:33<47:36,  6.22s/it, running training loss: 0.6878]\u001b[A\n",
      "Training:  15%|█▍        | 79/537 [09:33<52:03,  6.82s/it, running training loss: 0.6878]\u001b[A\n",
      "Training:  15%|█▍        | 79/537 [09:40<52:03,  6.82s/it, running training loss: 0.6855]\u001b[A\n",
      "Training:  15%|█▍        | 80/537 [09:40<52:32,  6.90s/it, running training loss: 0.6855]\u001b[A\n",
      "Training:  15%|█▍        | 80/537 [09:52<52:32,  6.90s/it, running training loss: 0.6744]\u001b[A\n",
      "Training:  15%|█▌        | 81/537 [09:52<1:04:33,  8.50s/it, running training loss: 0.6744]\u001b[A\n",
      "Training:  15%|█▌        | 81/537 [09:59<1:04:33,  8.50s/it, running training loss: 0.6845]\u001b[A\n",
      "Training:  15%|█▌        | 82/537 [09:59<1:00:18,  7.95s/it, running training loss: 0.6845]\u001b[A\n",
      "Training:  15%|█▌        | 82/537 [10:06<1:00:18,  7.95s/it, running training loss: 0.6630]\u001b[A\n",
      "Training:  15%|█▌        | 83/537 [10:06<57:18,  7.57s/it, running training loss: 0.6630]  \u001b[A\n",
      "Training:  15%|█▌        | 83/537 [10:10<57:18,  7.57s/it, running training loss: 0.6254]\u001b[A\n",
      "Training:  16%|█▌        | 84/537 [10:10<50:40,  6.71s/it, running training loss: 0.6254]\u001b[A\n",
      "Training:  16%|█▌        | 84/537 [10:15<50:40,  6.71s/it, running training loss: 0.6177]\u001b[A\n",
      "Training:  16%|█▌        | 85/537 [10:15<46:55,  6.23s/it, running training loss: 0.6177]\u001b[A\n",
      "Training:  16%|█▌        | 85/537 [10:22<46:55,  6.23s/it, running training loss: 0.5624]\u001b[A\n",
      "Training:  16%|█▌        | 86/537 [10:22<46:59,  6.25s/it, running training loss: 0.5624]\u001b[A\n",
      "Training:  16%|█▌        | 86/537 [10:29<46:59,  6.25s/it, running training loss: 0.5786]\u001b[A\n",
      "Training:  16%|█▌        | 87/537 [10:29<49:28,  6.60s/it, running training loss: 0.5786]\u001b[A\n",
      "Training:  16%|█▌        | 87/537 [10:44<49:28,  6.60s/it, running training loss: 0.6219]\u001b[A\n",
      "Training:  16%|█▋        | 88/537 [10:44<1:07:50,  9.07s/it, running training loss: 0.6219]\u001b[A\n",
      "Training:  16%|█▋        | 88/537 [10:52<1:07:50,  9.07s/it, running training loss: 0.6901]\u001b[A\n",
      "Training:  17%|█▋        | 89/537 [10:52<1:06:07,  8.86s/it, running training loss: 0.6901]\u001b[A\n",
      "Training:  17%|█▋        | 89/537 [10:58<1:06:07,  8.86s/it, running training loss: 0.6777]\u001b[A\n",
      "Training:  17%|█▋        | 90/537 [10:58<59:50,  8.03s/it, running training loss: 0.6777]  \u001b[A\n",
      "Training:  17%|█▋        | 90/537 [11:07<59:50,  8.03s/it, running training loss: 0.7263]\u001b[A\n",
      "Training:  17%|█▋        | 91/537 [11:07<1:00:05,  8.08s/it, running training loss: 0.7263]\u001b[A\n",
      "Training:  17%|█▋        | 91/537 [11:15<1:00:05,  8.08s/it, running training loss: 0.6267]\u001b[A\n",
      "Training:  17%|█▋        | 92/537 [11:15<1:01:05,  8.24s/it, running training loss: 0.6267]\u001b[A\n",
      "Training:  17%|█▋        | 92/537 [11:21<1:01:05,  8.24s/it, running training loss: 0.5349]\u001b[A\n",
      "Training:  17%|█▋        | 93/537 [11:21<54:39,  7.39s/it, running training loss: 0.5349]  \u001b[A\n",
      "Training:  17%|█▋        | 93/537 [11:27<54:39,  7.39s/it, running training loss: 0.6337]\u001b[A\n",
      "Training:  18%|█▊        | 94/537 [11:27<51:15,  6.94s/it, running training loss: 0.6337]\u001b[A\n",
      "Training:  18%|█▊        | 94/537 [11:34<51:15,  6.94s/it, running training loss: 0.6395]\u001b[A\n",
      "Training:  18%|█▊        | 95/537 [11:34<53:13,  7.23s/it, running training loss: 0.6395]\u001b[A\n",
      "Training:  18%|█▊        | 95/537 [11:41<53:13,  7.23s/it, running training loss: 0.6386]\u001b[A\n",
      "Training:  18%|█▊        | 96/537 [11:41<51:29,  7.01s/it, running training loss: 0.6386]\u001b[A\n",
      "Training:  18%|█▊        | 96/537 [11:50<51:29,  7.01s/it, running training loss: 0.6181]\u001b[A\n",
      "Training:  18%|█▊        | 97/537 [11:50<55:47,  7.61s/it, running training loss: 0.6181]\u001b[A\n",
      "Training:  18%|█▊        | 97/537 [11:58<55:47,  7.61s/it, running training loss: 0.7089]\u001b[A\n",
      "Training:  18%|█▊        | 98/537 [11:58<56:29,  7.72s/it, running training loss: 0.7089]\u001b[A\n",
      "Training:  18%|█▊        | 98/537 [12:07<56:29,  7.72s/it, running training loss: 0.5864]\u001b[A\n",
      "Training:  18%|█▊        | 99/537 [12:07<1:00:04,  8.23s/it, running training loss: 0.5864]\u001b[A\n",
      "Training:  18%|█▊        | 99/537 [12:14<1:00:04,  8.23s/it, running training loss: 0.6487]\u001b[A\n",
      "Training:  19%|█▊        | 100/537 [12:14<55:55,  7.68s/it, running training loss: 0.6487] \u001b[A\n",
      "Training:  19%|█▊        | 100/537 [12:24<55:55,  7.68s/it, running training loss: 0.6296]\u001b[A\n",
      "Training:  19%|█▉        | 101/537 [12:24<1:00:24,  8.31s/it, running training loss: 0.6296]\u001b[A\n",
      "Training:  19%|█▉        | 101/537 [12:30<1:00:24,  8.31s/it, running training loss: 0.6391]\u001b[A\n",
      "Training:  19%|█▉        | 102/537 [12:30<57:00,  7.86s/it, running training loss: 0.6391]  \u001b[A\n",
      "Training:  19%|█▉        | 102/537 [12:41<57:00,  7.86s/it, running training loss: 0.5991]\u001b[A\n",
      "Training:  19%|█▉        | 103/537 [12:41<1:02:22,  8.62s/it, running training loss: 0.5991]\u001b[A\n",
      "Training:  19%|█▉        | 103/537 [12:47<1:02:22,  8.62s/it, running training loss: 0.5367]\u001b[A\n",
      "Training:  19%|█▉        | 104/537 [12:47<56:21,  7.81s/it, running training loss: 0.5367]  \u001b[A\n",
      "Training:  19%|█▉        | 104/537 [12:53<56:21,  7.81s/it, running training loss: 0.6874]\u001b[A\n",
      "Training:  20%|█▉        | 105/537 [12:53<53:07,  7.38s/it, running training loss: 0.6874]\u001b[A\n",
      "Training:  20%|█▉        | 105/537 [12:59<53:07,  7.38s/it, running training loss: 0.5071]\u001b[A\n",
      "Training:  20%|█▉        | 106/537 [12:59<50:42,  7.06s/it, running training loss: 0.5071]\u001b[A\n",
      "Training:  20%|█▉        | 106/537 [13:08<50:42,  7.06s/it, running training loss: 0.6225]\u001b[A\n",
      "Training:  20%|█▉        | 107/537 [13:08<53:53,  7.52s/it, running training loss: 0.6225]\u001b[A\n",
      "Training:  20%|█▉        | 107/537 [13:14<53:53,  7.52s/it, running training loss: 0.5606]\u001b[A\n",
      "Training:  20%|██        | 108/537 [13:14<51:08,  7.15s/it, running training loss: 0.5606]\u001b[A\n",
      "Training:  20%|██        | 108/537 [13:24<51:08,  7.15s/it, running training loss: 0.5633]\u001b[A\n",
      "Training:  20%|██        | 109/537 [13:24<56:53,  7.97s/it, running training loss: 0.5633]\u001b[A\n",
      "Training:  20%|██        | 109/537 [13:32<56:53,  7.97s/it, running training loss: 0.6899]\u001b[A\n",
      "Training:  20%|██        | 110/537 [13:32<56:27,  7.93s/it, running training loss: 0.6899]\u001b[A\n",
      "Training:  20%|██        | 110/537 [13:40<56:27,  7.93s/it, running training loss: 0.5962]\u001b[A\n",
      "Training:  21%|██        | 111/537 [13:40<57:29,  8.10s/it, running training loss: 0.5962]\u001b[A\n",
      "Training:  21%|██        | 111/537 [13:47<57:29,  8.10s/it, running training loss: 0.6416]\u001b[A\n",
      "Training:  21%|██        | 112/537 [13:47<54:36,  7.71s/it, running training loss: 0.6416]\u001b[A\n",
      "Training:  21%|██        | 112/537 [13:53<54:36,  7.71s/it, running training loss: 0.5903]\u001b[A\n",
      "Training:  21%|██        | 113/537 [13:53<50:00,  7.08s/it, running training loss: 0.5903]\u001b[A\n",
      "Training:  21%|██        | 113/537 [13:59<50:00,  7.08s/it, running training loss: 0.6718]\u001b[A\n",
      "Training:  21%|██        | 114/537 [13:59<47:49,  6.78s/it, running training loss: 0.6718]\u001b[A\n",
      "Training:  21%|██        | 114/537 [14:05<47:49,  6.78s/it, running training loss: 0.5832]\u001b[A\n",
      "Training:  21%|██▏       | 115/537 [14:05<45:26,  6.46s/it, running training loss: 0.5832]\u001b[A\n",
      "Training:  21%|██▏       | 115/537 [14:14<45:26,  6.46s/it, running training loss: 0.6120]\u001b[A\n",
      "Training:  22%|██▏       | 116/537 [14:14<51:18,  7.31s/it, running training loss: 0.6120]\u001b[A\n",
      "Training:  22%|██▏       | 116/537 [14:23<51:18,  7.31s/it, running training loss: 0.5813]\u001b[A\n",
      "Training:  22%|██▏       | 117/537 [14:23<54:41,  7.81s/it, running training loss: 0.5813]\u001b[A\n",
      "Training:  22%|██▏       | 117/537 [14:29<54:41,  7.81s/it, running training loss: 0.6437]\u001b[A\n",
      "Training:  22%|██▏       | 118/537 [14:29<50:59,  7.30s/it, running training loss: 0.6437]\u001b[A\n",
      "Training:  22%|██▏       | 118/537 [14:35<50:59,  7.30s/it, running training loss: 0.7199]\u001b[A\n",
      "Training:  22%|██▏       | 119/537 [14:35<47:32,  6.82s/it, running training loss: 0.7199]\u001b[A\n",
      "Training:  22%|██▏       | 119/537 [14:41<47:32,  6.82s/it, running training loss: 0.6016]\u001b[A\n",
      "Training:  22%|██▏       | 120/537 [14:41<46:57,  6.76s/it, running training loss: 0.6016]\u001b[A\n",
      "Training:  22%|██▏       | 120/537 [14:49<46:57,  6.76s/it, running training loss: 0.5852]\u001b[A\n",
      "Training:  23%|██▎       | 121/537 [14:49<47:56,  6.91s/it, running training loss: 0.5852]\u001b[A\n",
      "Training:  23%|██▎       | 121/537 [14:57<47:56,  6.91s/it, running training loss: 0.7052]\u001b[A\n",
      "Training:  23%|██▎       | 122/537 [14:57<51:22,  7.43s/it, running training loss: 0.7052]\u001b[A\n",
      "Training:  23%|██▎       | 122/537 [15:03<51:22,  7.43s/it, running training loss: 0.5447]\u001b[A\n",
      "Training:  23%|██▎       | 123/537 [15:03<47:13,  6.84s/it, running training loss: 0.5447]\u001b[A\n",
      "Training:  23%|██▎       | 123/537 [15:10<47:13,  6.84s/it, running training loss: 0.6360]\u001b[A\n",
      "Training:  23%|██▎       | 124/537 [15:10<47:24,  6.89s/it, running training loss: 0.6360]\u001b[A\n",
      "Training:  23%|██▎       | 124/537 [15:16<47:24,  6.89s/it, running training loss: 0.6527]\u001b[A\n",
      "Training:  23%|██▎       | 125/537 [15:16<46:29,  6.77s/it, running training loss: 0.6527]\u001b[A\n",
      "Training:  23%|██▎       | 125/537 [15:23<46:29,  6.77s/it, running training loss: 0.6991]\u001b[A\n",
      "Training:  23%|██▎       | 126/537 [15:23<46:51,  6.84s/it, running training loss: 0.6991]\u001b[A\n",
      "Training:  23%|██▎       | 126/537 [15:32<46:51,  6.84s/it, running training loss: 0.6900]\u001b[A\n",
      "Training:  24%|██▎       | 127/537 [15:32<50:32,  7.40s/it, running training loss: 0.6900]\u001b[A\n",
      "Training:  24%|██▎       | 127/537 [15:40<50:32,  7.40s/it, running training loss: 0.6014]\u001b[A\n",
      "Training:  24%|██▍       | 128/537 [15:40<51:30,  7.56s/it, running training loss: 0.6014]\u001b[A\n",
      "Training:  24%|██▍       | 128/537 [15:47<51:30,  7.56s/it, running training loss: 0.6462]\u001b[A\n",
      "Training:  24%|██▍       | 129/537 [15:47<51:13,  7.53s/it, running training loss: 0.6462]\u001b[A\n",
      "Training:  24%|██▍       | 129/537 [15:54<51:13,  7.53s/it, running training loss: 0.6423]\u001b[A\n",
      "Training:  24%|██▍       | 130/537 [15:54<49:50,  7.35s/it, running training loss: 0.6423]\u001b[A\n",
      "Training:  24%|██▍       | 130/537 [16:04<49:50,  7.35s/it, running training loss: 0.5658]\u001b[A\n",
      "Training:  24%|██▍       | 131/537 [16:04<54:29,  8.05s/it, running training loss: 0.5658]\u001b[A\n",
      "Training:  24%|██▍       | 131/537 [16:15<54:29,  8.05s/it, running training loss: 0.6277]\u001b[A\n",
      "Training:  25%|██▍       | 132/537 [16:15<1:01:07,  9.05s/it, running training loss: 0.6277]\u001b[A\n",
      "Training:  25%|██▍       | 132/537 [16:22<1:01:07,  9.05s/it, running training loss: 0.6009]\u001b[A\n",
      "Training:  25%|██▍       | 133/537 [16:22<57:02,  8.47s/it, running training loss: 0.6009]  \u001b[A\n",
      "Training:  25%|██▍       | 133/537 [16:32<57:02,  8.47s/it, running training loss: 0.6444]\u001b[A\n",
      "Training:  25%|██▍       | 134/537 [16:32<58:08,  8.66s/it, running training loss: 0.6444]\u001b[A\n",
      "Training:  25%|██▍       | 134/537 [16:41<58:08,  8.66s/it, running training loss: 0.6712]\u001b[A\n",
      "Training:  25%|██▌       | 135/537 [16:41<58:54,  8.79s/it, running training loss: 0.6712]\u001b[A\n",
      "Training:  25%|██▌       | 135/537 [16:47<58:54,  8.79s/it, running training loss: 0.6642]\u001b[A\n",
      "Training:  25%|██▌       | 136/537 [16:47<52:55,  7.92s/it, running training loss: 0.6642]\u001b[A\n",
      "Training:  25%|██▌       | 136/537 [16:55<52:55,  7.92s/it, running training loss: 0.7045]\u001b[A\n",
      "Training:  26%|██▌       | 137/537 [16:55<54:11,  8.13s/it, running training loss: 0.7045]\u001b[A\n",
      "Training:  26%|██▌       | 137/537 [17:02<54:11,  8.13s/it, running training loss: 0.6086]\u001b[A\n",
      "Training:  26%|██▌       | 138/537 [17:02<51:01,  7.67s/it, running training loss: 0.6086]\u001b[A\n",
      "Training:  26%|██▌       | 138/537 [17:07<51:01,  7.67s/it, running training loss: 0.6018]\u001b[A\n",
      "Training:  26%|██▌       | 139/537 [17:07<45:11,  6.81s/it, running training loss: 0.6018]\u001b[A\n",
      "Training:  26%|██▌       | 139/537 [17:19<45:11,  6.81s/it, running training loss: 0.5796]\u001b[A\n",
      "Training:  26%|██▌       | 140/537 [17:19<56:54,  8.60s/it, running training loss: 0.5796]\u001b[A\n",
      "Training:  26%|██▌       | 140/537 [17:27<56:54,  8.60s/it, running training loss: 0.6233]\u001b[A\n",
      "Training:  26%|██▋       | 141/537 [17:27<54:25,  8.25s/it, running training loss: 0.6233]\u001b[A\n",
      "Training:  26%|██▋       | 141/537 [17:32<54:25,  8.25s/it, running training loss: 0.5799]\u001b[A\n",
      "Training:  26%|██▋       | 142/537 [17:32<48:51,  7.42s/it, running training loss: 0.5799]\u001b[A\n",
      "Training:  26%|██▋       | 142/537 [17:42<48:51,  7.42s/it, running training loss: 0.5987]\u001b[A\n",
      "Training:  27%|██▋       | 143/537 [17:42<52:24,  7.98s/it, running training loss: 0.5987]\u001b[A\n",
      "Training:  27%|██▋       | 143/537 [17:47<52:24,  7.98s/it, running training loss: 0.6305]\u001b[A\n",
      "Training:  27%|██▋       | 144/537 [17:47<46:27,  7.09s/it, running training loss: 0.6305]\u001b[A\n",
      "Training:  27%|██▋       | 144/537 [17:52<46:27,  7.09s/it, running training loss: 0.6737]\u001b[A\n",
      "Training:  27%|██▋       | 145/537 [17:52<42:37,  6.52s/it, running training loss: 0.6737]\u001b[A\n",
      "Training:  27%|██▋       | 145/537 [18:00<42:37,  6.52s/it, running training loss: 0.5906]\u001b[A\n",
      "Training:  27%|██▋       | 146/537 [18:00<46:22,  7.12s/it, running training loss: 0.5906]\u001b[A\n",
      "Training:  27%|██▋       | 146/537 [18:07<46:22,  7.12s/it, running training loss: 0.6608]\u001b[A\n",
      "Training:  27%|██▋       | 147/537 [18:07<45:23,  6.98s/it, running training loss: 0.6608]\u001b[A\n",
      "Training:  27%|██▋       | 147/537 [18:14<45:23,  6.98s/it, running training loss: 0.6331]\u001b[A\n",
      "Training:  28%|██▊       | 148/537 [18:14<45:31,  7.02s/it, running training loss: 0.6331]\u001b[A\n",
      "Training:  28%|██▊       | 148/537 [18:28<45:31,  7.02s/it, running training loss: 0.5740]\u001b[A\n",
      "Training:  28%|██▊       | 149/537 [18:28<59:44,  9.24s/it, running training loss: 0.5740]\u001b[A\n",
      "Training:  28%|██▊       | 149/537 [18:37<59:44,  9.24s/it, running training loss: 0.6275]\u001b[A\n",
      "Training:  28%|██▊       | 150/537 [18:37<58:45,  9.11s/it, running training loss: 0.6275]\u001b[A\n",
      "Training:  28%|██▊       | 150/537 [18:45<58:45,  9.11s/it, running training loss: 0.6550]\u001b[A\n",
      "Training:  28%|██▊       | 151/537 [18:45<55:26,  8.62s/it, running training loss: 0.6550]\u001b[A\n",
      "Training:  28%|██▊       | 151/537 [18:52<55:26,  8.62s/it, running training loss: 0.7378]\u001b[A\n",
      "Training:  28%|██▊       | 152/537 [18:52<53:11,  8.29s/it, running training loss: 0.7378]\u001b[A\n",
      "Training:  28%|██▊       | 152/537 [18:58<53:11,  8.29s/it, running training loss: 0.5188]\u001b[A\n",
      "Training:  28%|██▊       | 153/537 [18:58<47:31,  7.43s/it, running training loss: 0.5188]\u001b[A\n",
      "Training:  28%|██▊       | 153/537 [19:04<47:31,  7.43s/it, running training loss: 0.5484]\u001b[A\n",
      "Training:  29%|██▊       | 154/537 [19:04<46:08,  7.23s/it, running training loss: 0.5484]\u001b[A\n",
      "Training:  29%|██▉       | 157/537 [19:37<48:35,  7.67s/it, running training loss: 0.5452]\u001b[A\n",
      "Training:  29%|██▉       | 158/537 [19:37<49:48,  7.89s/it, running training loss: 0.5452]\u001b[A\n",
      "Training:  29%|██▉       | 158/537 [19:45<49:48,  7.89s/it, running training loss: 0.6004]\u001b[A\n",
      "Training:  30%|██▉       | 159/537 [19:45<50:50,  8.07s/it, running training loss: 0.6004]\u001b[A\n",
      "Training:  30%|██▉       | 159/537 [19:52<50:50,  8.07s/it, running training loss: 0.6012]\u001b[A\n",
      "Training:  30%|██▉       | 160/537 [19:52<47:58,  7.64s/it, running training loss: 0.6012]\u001b[A\n",
      "Training:  30%|██▉       | 160/537 [20:00<47:58,  7.64s/it, running training loss: 0.5746]\u001b[A\n",
      "Training:  30%|██▉       | 161/537 [20:00<47:33,  7.59s/it, running training loss: 0.5746]\u001b[A\n",
      "Training:  30%|██▉       | 161/537 [20:07<47:33,  7.59s/it, running training loss: 0.5817]\u001b[A\n",
      "Training:  30%|███       | 162/537 [20:07<46:53,  7.50s/it, running training loss: 0.5817]\u001b[A\n",
      "Training:  30%|███       | 162/537 [20:14<46:53,  7.50s/it, running training loss: 0.6597]\u001b[A\n",
      "Training:  30%|███       | 163/537 [20:14<46:01,  7.38s/it, running training loss: 0.6597]\u001b[A\n",
      "Training:  30%|███       | 163/537 [20:20<46:01,  7.38s/it, running training loss: 0.5490]\u001b[A\n",
      "Training:  31%|███       | 164/537 [20:20<42:35,  6.85s/it, running training loss: 0.5490]\u001b[A\n",
      "Training:  31%|███       | 164/537 [20:25<42:35,  6.85s/it, running training loss: 0.6180]\u001b[A\n",
      "Training:  31%|███       | 165/537 [20:25<40:19,  6.51s/it, running training loss: 0.6180]\u001b[A\n",
      "Training:  31%|███       | 165/537 [20:31<40:19,  6.51s/it, running training loss: 0.5290]\u001b[A\n",
      "Training:  31%|███       | 166/537 [20:31<37:57,  6.14s/it, running training loss: 0.5290]\u001b[A\n",
      "Training:  31%|███       | 166/537 [20:41<37:57,  6.14s/it, running training loss: 0.6266]\u001b[A\n",
      "Training:  31%|███       | 167/537 [20:41<45:58,  7.45s/it, running training loss: 0.6266]\u001b[A\n",
      "Training:  31%|███       | 167/537 [20:48<45:58,  7.45s/it, running training loss: 0.6145]\u001b[A\n",
      "Training:  31%|███▏      | 168/537 [20:48<44:46,  7.28s/it, running training loss: 0.6145]\u001b[A\n",
      "Training:  31%|███▏      | 168/537 [20:58<44:46,  7.28s/it, running training loss: 0.6517]\u001b[A\n",
      "Training:  31%|███▏      | 169/537 [20:58<49:19,  8.04s/it, running training loss: 0.6517]\u001b[A\n",
      "Training:  31%|███▏      | 169/537 [21:04<49:19,  8.04s/it, running training loss: 0.6519]\u001b[A\n",
      "Training:  32%|███▏      | 170/537 [21:04<45:57,  7.51s/it, running training loss: 0.6519]\u001b[A\n",
      "Training:  32%|███▏      | 170/537 [21:12<45:57,  7.51s/it, running training loss: 0.6106]\u001b[A\n",
      "Training:  32%|███▏      | 171/537 [21:12<47:16,  7.75s/it, running training loss: 0.6106]\u001b[A\n",
      "Training:  32%|███▏      | 171/537 [21:20<47:16,  7.75s/it, running training loss: 0.7076]\u001b[A\n",
      "Training:  32%|███▏      | 172/537 [21:20<46:31,  7.65s/it, running training loss: 0.7076]\u001b[A\n",
      "Training:  32%|███▏      | 172/537 [21:26<46:31,  7.65s/it, running training loss: 0.5835]\u001b[A\n",
      "Training:  32%|███▏      | 173/537 [21:26<44:29,  7.33s/it, running training loss: 0.5835]\u001b[A\n",
      "Training:  32%|███▏      | 173/537 [21:31<44:29,  7.33s/it, running training loss: 0.6670]\u001b[A\n",
      "Training:  32%|███▏      | 174/537 [21:31<40:07,  6.63s/it, running training loss: 0.6670]\u001b[A\n",
      "Training:  32%|███▏      | 174/537 [21:40<40:07,  6.63s/it, running training loss: 0.7086]\u001b[A\n",
      "Training:  33%|███▎      | 175/537 [21:40<43:23,  7.19s/it, running training loss: 0.7086]\u001b[A\n",
      "Training:  33%|███▎      | 175/537 [21:49<43:23,  7.19s/it, running training loss: 0.6118]\u001b[A\n",
      "Training:  33%|███▎      | 176/537 [21:49<45:58,  7.64s/it, running training loss: 0.6118]\u001b[A\n",
      "Training:  33%|███▎      | 176/537 [22:00<45:58,  7.64s/it, running training loss: 0.6352]\u001b[A\n",
      "Training:  33%|███▎      | 177/537 [22:00<53:09,  8.86s/it, running training loss: 0.6352]\u001b[A\n",
      "Training:  33%|███▎      | 177/537 [22:10<53:09,  8.86s/it, running training loss: 0.5539]\u001b[A\n",
      "Training:  33%|███▎      | 178/537 [22:10<54:54,  9.18s/it, running training loss: 0.5539]\u001b[A\n",
      "Training:  33%|███▎      | 178/537 [22:16<54:54,  9.18s/it, running training loss: 0.6131]\u001b[A\n",
      "Training:  33%|███▎      | 179/537 [22:16<48:20,  8.10s/it, running training loss: 0.6131]\u001b[A\n",
      "Training:  33%|███▎      | 179/537 [22:21<48:20,  8.10s/it, running training loss: 0.5341]\u001b[A\n",
      "Training:  34%|███▎      | 180/537 [22:21<43:45,  7.35s/it, running training loss: 0.5341]\u001b[A\n",
      "Training:  34%|███▎      | 180/537 [22:31<43:45,  7.35s/it, running training loss: 0.6058]\u001b[A\n",
      "Training:  34%|███▎      | 181/537 [22:31<46:54,  7.91s/it, running training loss: 0.6058]\u001b[A\n",
      "Training:  34%|███▎      | 181/537 [22:36<46:54,  7.91s/it, running training loss: 0.6285]\u001b[A\n",
      "Training:  34%|███▍      | 182/537 [22:36<42:51,  7.24s/it, running training loss: 0.6285]\u001b[A\n",
      "Training:  34%|███▍      | 182/537 [22:43<42:51,  7.24s/it, running training loss: 0.5584]\u001b[A\n",
      "Training:  34%|███▍      | 183/537 [22:43<41:48,  7.09s/it, running training loss: 0.5584]\u001b[A\n",
      "Training:  34%|███▍      | 183/537 [22:50<41:48,  7.09s/it, running training loss: 0.5983]\u001b[A\n",
      "Training:  34%|███▍      | 184/537 [22:50<40:57,  6.96s/it, running training loss: 0.5983]\u001b[A\n",
      "Training:  34%|███▍      | 184/537 [22:56<40:57,  6.96s/it, running training loss: 0.5857]\u001b[A\n",
      "Training:  34%|███▍      | 185/537 [22:56<39:39,  6.76s/it, running training loss: 0.5857]\u001b[A\n",
      "Training:  34%|███▍      | 185/537 [23:03<39:39,  6.76s/it, running training loss: 0.6248]\u001b[A\n",
      "Training:  35%|███▍      | 186/537 [23:03<40:20,  6.90s/it, running training loss: 0.6248]\u001b[A\n",
      "Training:  35%|███▍      | 186/537 [23:13<40:20,  6.90s/it, running training loss: 0.5181]\u001b[A\n",
      "Training:  35%|███▍      | 187/537 [23:13<44:47,  7.68s/it, running training loss: 0.5181]\u001b[A\n",
      "Training:  35%|███▍      | 187/537 [23:19<44:47,  7.68s/it, running training loss: 0.6783]\u001b[A\n",
      "Training:  35%|███▌      | 188/537 [23:19<42:46,  7.36s/it, running training loss: 0.6783]\u001b[A\n",
      "Training:  35%|███▌      | 188/537 [23:25<42:46,  7.36s/it, running training loss: 0.5332]\u001b[A\n",
      "Training:  35%|███▌      | 189/537 [23:25<40:28,  6.98s/it, running training loss: 0.5332]\u001b[A\n",
      "Training:  35%|███▌      | 189/537 [23:32<40:28,  6.98s/it, running training loss: 0.6881]\u001b[A\n",
      "Training:  35%|███▌      | 190/537 [23:32<40:34,  7.01s/it, running training loss: 0.6881]\u001b[A\n",
      "Training:  35%|███▌      | 190/537 [23:39<40:34,  7.01s/it, running training loss: 0.6440]\u001b[A\n",
      "Training:  36%|███▌      | 191/537 [23:39<40:25,  7.01s/it, running training loss: 0.6440]\u001b[A\n",
      "Training:  36%|███▌      | 191/537 [23:46<40:25,  7.01s/it, running training loss: 0.6080]\u001b[A\n",
      "Training:  36%|███▌      | 192/537 [23:46<40:18,  7.01s/it, running training loss: 0.6080]\u001b[A\n",
      "Training:  36%|███▌      | 192/537 [23:53<40:18,  7.01s/it, running training loss: 0.6554]\u001b[A\n",
      "Training:  36%|███▌      | 193/537 [23:53<40:10,  7.01s/it, running training loss: 0.6554]\u001b[A\n",
      "Training:  36%|███▌      | 193/537 [24:00<40:10,  7.01s/it, running training loss: 0.6194]\u001b[A\n",
      "Training:  36%|███▌      | 194/537 [24:00<38:29,  6.73s/it, running training loss: 0.6194]\u001b[A\n",
      "Training:  36%|███▌      | 194/537 [24:07<38:29,  6.73s/it, running training loss: 0.5975]\u001b[A\n",
      "Training:  36%|███▋      | 195/537 [24:07<38:50,  6.81s/it, running training loss: 0.5975]\u001b[A\n",
      "Training:  36%|███▋      | 195/537 [24:17<38:50,  6.81s/it, running training loss: 0.5906]\u001b[A\n",
      "Training:  36%|███▋      | 196/537 [24:17<45:10,  7.95s/it, running training loss: 0.5906]\u001b[A\n",
      "Training:  36%|███▋      | 196/537 [24:26<45:10,  7.95s/it, running training loss: 0.6429]\u001b[A\n",
      "Training:  37%|███▋      | 197/537 [24:26<47:08,  8.32s/it, running training loss: 0.6429]\u001b[A\n",
      "Training:  37%|███▋      | 197/537 [24:38<47:08,  8.32s/it, running training loss: 0.6064]\u001b[A\n",
      "Training:  37%|███▋      | 198/537 [24:38<53:25,  9.46s/it, running training loss: 0.6064]\u001b[A\n",
      "Training:  37%|███▋      | 198/537 [24:45<53:25,  9.46s/it, running training loss: 0.5792]\u001b[A\n",
      "Training:  37%|███▋      | 199/537 [24:45<47:35,  8.45s/it, running training loss: 0.5792]\u001b[A\n",
      "Training:  37%|███▋      | 199/537 [24:51<47:35,  8.45s/it, running training loss: 0.6353]\u001b[A\n",
      "Training:  37%|███▋      | 200/537 [24:51<44:21,  7.90s/it, running training loss: 0.6353]\u001b[A\n",
      "Training:  37%|███▋      | 200/537 [24:58<44:21,  7.90s/it, running training loss: 0.6934]\u001b[A\n",
      "Training:  37%|███▋      | 201/537 [24:58<43:11,  7.71s/it, running training loss: 0.6934]\u001b[A\n",
      "Training:  37%|███▋      | 201/537 [25:03<43:11,  7.71s/it, running training loss: 0.5574]\u001b[A\n",
      "Training:  38%|███▊      | 202/537 [25:03<37:44,  6.76s/it, running training loss: 0.5574]\u001b[A\n",
      "Training:  38%|███▊      | 202/537 [25:08<37:44,  6.76s/it, running training loss: 0.5319]\u001b[A\n",
      "Training:  38%|███▊      | 203/537 [25:08<35:09,  6.32s/it, running training loss: 0.5319]\u001b[A\n",
      "Training:  38%|███▊      | 203/537 [25:12<35:09,  6.32s/it, running training loss: 0.6061]\u001b[A\n",
      "Training:  38%|███▊      | 204/537 [25:12<30:52,  5.56s/it, running training loss: 0.6061]\u001b[A\n",
      "Training:  38%|███▊      | 204/537 [25:19<30:52,  5.56s/it, running training loss: 0.5064]\u001b[A\n",
      "Training:  38%|███▊      | 205/537 [25:19<32:47,  5.93s/it, running training loss: 0.5064]\u001b[A\n",
      "Training:  38%|███▊      | 205/537 [25:25<32:47,  5.93s/it, running training loss: 0.5550]\u001b[A\n",
      "Training:  38%|███▊      | 206/537 [25:25<33:49,  6.13s/it, running training loss: 0.5550]\u001b[A\n",
      "Training:  38%|███▊      | 206/537 [25:32<33:49,  6.13s/it, running training loss: 0.6202]\u001b[A\n",
      "Training:  39%|███▊      | 207/537 [25:32<34:20,  6.24s/it, running training loss: 0.6202]\u001b[A\n",
      "Training:  39%|███▊      | 207/537 [25:38<34:20,  6.24s/it, running training loss: 0.5740]\u001b[A\n",
      "Training:  39%|███▊      | 208/537 [25:38<33:59,  6.20s/it, running training loss: 0.5740]\u001b[A\n",
      "Training:  39%|███▊      | 208/537 [25:44<33:59,  6.20s/it, running training loss: 0.7271]\u001b[A\n",
      "Training:  39%|███▉      | 209/537 [25:44<33:14,  6.08s/it, running training loss: 0.7271]\u001b[A\n",
      "Training:  39%|███▉      | 209/537 [25:53<33:14,  6.08s/it, running training loss: 0.5748]\u001b[A\n",
      "Training:  39%|███▉      | 210/537 [25:53<38:52,  7.13s/it, running training loss: 0.5748]\u001b[A\n",
      "Training:  39%|███▉      | 210/537 [26:00<38:52,  7.13s/it, running training loss: 0.6045]\u001b[A\n",
      "Training:  39%|███▉      | 211/537 [26:00<37:34,  6.92s/it, running training loss: 0.6045]\u001b[A\n",
      "Training:  39%|███▉      | 211/537 [26:07<37:34,  6.92s/it, running training loss: 0.5745]\u001b[A\n",
      "Training:  39%|███▉      | 212/537 [26:07<38:34,  7.12s/it, running training loss: 0.5745]\u001b[A\n",
      "Training:  39%|███▉      | 212/537 [26:15<38:34,  7.12s/it, running training loss: 0.6590]\u001b[A\n",
      "Training:  40%|███▉      | 213/537 [26:15<38:44,  7.17s/it, running training loss: 0.6590]\u001b[A\n",
      "Training:  40%|███▉      | 213/537 [26:20<38:44,  7.17s/it, running training loss: 0.5602]\u001b[A\n",
      "Training:  40%|███▉      | 214/537 [26:20<36:03,  6.70s/it, running training loss: 0.5602]\u001b[A\n",
      "Training:  40%|███▉      | 214/537 [26:28<36:03,  6.70s/it, running training loss: 0.5857]\u001b[A\n",
      "Training:  40%|████      | 215/537 [26:28<38:03,  7.09s/it, running training loss: 0.5857]\u001b[A\n",
      "Training:  40%|████      | 215/537 [26:34<38:03,  7.09s/it, running training loss: 0.6809]\u001b[A\n",
      "Training:  40%|████      | 216/537 [26:34<35:04,  6.55s/it, running training loss: 0.6809]\u001b[A\n",
      "Training:  40%|████      | 216/537 [26:41<35:04,  6.55s/it, running training loss: 0.6019]\u001b[A\n",
      "Training:  40%|████      | 217/537 [26:41<35:58,  6.75s/it, running training loss: 0.6019]\u001b[A\n",
      "Training:  40%|████      | 217/537 [26:47<35:58,  6.75s/it, running training loss: 0.6909]\u001b[A\n",
      "Training:  41%|████      | 218/537 [26:47<35:39,  6.71s/it, running training loss: 0.6909]\u001b[A\n",
      "Training:  41%|████      | 218/537 [26:55<35:39,  6.71s/it, running training loss: 0.5825]\u001b[A\n",
      "Training:  41%|████      | 219/537 [26:55<36:47,  6.94s/it, running training loss: 0.5825]\u001b[A\n",
      "Training:  41%|████      | 219/537 [27:05<36:47,  6.94s/it, running training loss: 0.6252]\u001b[A\n",
      "Training:  41%|████      | 220/537 [27:05<40:52,  7.74s/it, running training loss: 0.6252]\u001b[A\n",
      "Training:  41%|████      | 220/537 [27:10<40:52,  7.74s/it, running training loss: 0.5572]\u001b[A\n",
      "Training:  41%|████      | 221/537 [27:10<37:32,  7.13s/it, running training loss: 0.5572]\u001b[A\n",
      "Training:  41%|████      | 221/537 [27:17<37:32,  7.13s/it, running training loss: 0.6799]\u001b[A\n",
      "Training:  41%|████▏     | 222/537 [27:17<37:31,  7.15s/it, running training loss: 0.6799]\u001b[A\n",
      "Training:  41%|████▏     | 222/537 [27:30<37:31,  7.15s/it, running training loss: 0.5413]\u001b[A\n",
      "Training:  42%|████▏     | 223/537 [27:30<45:29,  8.69s/it, running training loss: 0.5413]\u001b[A\n",
      "Training:  42%|████▏     | 223/537 [27:40<45:29,  8.69s/it, running training loss: 0.6156]\u001b[A\n",
      "Training:  42%|████▏     | 224/537 [27:40<47:25,  9.09s/it, running training loss: 0.6156]\u001b[A\n",
      "Training:  42%|████▏     | 224/537 [27:45<47:25,  9.09s/it, running training loss: 0.5586]\u001b[A\n",
      "Training:  42%|████▏     | 225/537 [27:45<41:30,  7.98s/it, running training loss: 0.5586]\u001b[A\n",
      "Training:  42%|████▏     | 225/537 [27:54<41:30,  7.98s/it, running training loss: 0.6861]\u001b[A\n",
      "Training:  42%|████▏     | 226/537 [27:54<43:23,  8.37s/it, running training loss: 0.6861]\u001b[A\n",
      "Training:  42%|████▏     | 226/537 [28:01<43:23,  8.37s/it, running training loss: 0.5985]\u001b[A\n",
      "Training:  42%|████▏     | 227/537 [28:01<40:21,  7.81s/it, running training loss: 0.5985]\u001b[A\n",
      "Training:  42%|████▏     | 227/537 [28:06<40:21,  7.81s/it, running training loss: 0.5523]\u001b[A\n",
      "Training:  42%|████▏     | 228/537 [28:06<35:53,  6.97s/it, running training loss: 0.5523]\u001b[A\n",
      "Training:  42%|████▏     | 228/537 [28:14<35:53,  6.97s/it, running training loss: 0.5621]\u001b[A\n",
      "Training:  43%|████▎     | 229/537 [28:14<37:03,  7.22s/it, running training loss: 0.5621]\u001b[A\n",
      "Training:  43%|████▎     | 229/537 [28:21<37:03,  7.22s/it, running training loss: 0.5670]\u001b[A\n",
      "Training:  43%|████▎     | 230/537 [28:21<36:18,  7.10s/it, running training loss: 0.5670]\u001b[A\n",
      "Training:  43%|████▎     | 230/537 [28:26<36:18,  7.10s/it, running training loss: 0.5877]\u001b[A\n",
      "Training:  43%|████▎     | 231/537 [28:26<33:24,  6.55s/it, running training loss: 0.5877]\u001b[A\n",
      "Training:  43%|████▎     | 231/537 [28:37<33:24,  6.55s/it, running training loss: 0.5767]\u001b[A\n",
      "Training:  43%|████▎     | 232/537 [28:37<41:01,  8.07s/it, running training loss: 0.5767]\u001b[A\n",
      "Training:  43%|████▎     | 232/537 [28:48<41:01,  8.07s/it, running training loss: 0.6096]\u001b[A\n",
      "Training:  43%|████▎     | 233/537 [28:48<44:15,  8.74s/it, running training loss: 0.6096]\u001b[A\n",
      "Training:  43%|████▎     | 233/537 [29:03<44:15,  8.74s/it, running training loss: 0.6661]\u001b[A\n",
      "Training:  44%|████▎     | 234/537 [29:03<53:37, 10.62s/it, running training loss: 0.6661]\u001b[A\n",
      "Training:  44%|████▎     | 234/537 [29:09<53:37, 10.62s/it, running training loss: 0.5804]\u001b[A\n",
      "Training:  44%|████▍     | 235/537 [29:09<46:29,  9.24s/it, running training loss: 0.5804]\u001b[A\n",
      "Training:  44%|████▍     | 235/537 [29:15<46:29,  9.24s/it, running training loss: 0.5979]\u001b[A\n",
      "Training:  44%|████▍     | 236/537 [29:15<42:03,  8.38s/it, running training loss: 0.5979]\u001b[A\n",
      "Training:  44%|████▍     | 236/537 [29:23<42:03,  8.38s/it, running training loss: 0.6296]\u001b[A\n",
      "Training:  44%|████▍     | 237/537 [29:23<40:45,  8.15s/it, running training loss: 0.6296]\u001b[A\n",
      "Training:  44%|████▍     | 237/537 [29:34<40:45,  8.15s/it, running training loss: 0.6115]\u001b[A\n",
      "Training:  44%|████▍     | 238/537 [29:34<44:32,  8.94s/it, running training loss: 0.6115]\u001b[A\n",
      "Training:  44%|████▍     | 238/537 [29:42<44:32,  8.94s/it, running training loss: 0.5853]\u001b[A\n",
      "Training:  45%|████▍     | 239/537 [29:42<43:18,  8.72s/it, running training loss: 0.5853]\u001b[A\n",
      "Training:  45%|████▍     | 239/537 [29:49<43:18,  8.72s/it, running training loss: 0.5775]\u001b[A\n",
      "Training:  45%|████▍     | 240/537 [29:49<41:04,  8.30s/it, running training loss: 0.5775]\u001b[A\n",
      "Training:  45%|████▍     | 240/537 [29:58<41:04,  8.30s/it, running training loss: 0.6271]\u001b[A\n",
      "Training:  45%|████▍     | 241/537 [29:58<41:57,  8.50s/it, running training loss: 0.6271]\u001b[A\n",
      "Training:  45%|████▍     | 241/537 [30:04<41:57,  8.50s/it, running training loss: 0.6473]\u001b[A\n",
      "Training:  45%|████▌     | 242/537 [30:04<37:51,  7.70s/it, running training loss: 0.6473]\u001b[A\n",
      "Training:  45%|████▌     | 242/537 [30:11<37:51,  7.70s/it, running training loss: 0.6178]\u001b[A\n",
      "Training:  45%|████▌     | 243/537 [30:11<36:40,  7.49s/it, running training loss: 0.6178]\u001b[A\n",
      "Training:  45%|████▌     | 243/537 [30:16<36:40,  7.49s/it, running training loss: 0.5781]\u001b[A\n",
      "Training:  45%|████▌     | 244/537 [30:16<32:37,  6.68s/it, running training loss: 0.5781]\u001b[A\n",
      "Training:  45%|████▌     | 244/537 [30:22<32:37,  6.68s/it, running training loss: 0.5732]\u001b[A\n",
      "Training:  46%|████▌     | 245/537 [30:22<31:20,  6.44s/it, running training loss: 0.5732]\u001b[A\n",
      "Training:  46%|████▌     | 245/537 [30:28<31:20,  6.44s/it, running training loss: 0.5842]\u001b[A\n",
      "Training:  46%|████▌     | 246/537 [30:28<30:36,  6.31s/it, running training loss: 0.5842]\u001b[A\n",
      "Training:  46%|████▌     | 246/537 [30:35<30:36,  6.31s/it, running training loss: 0.6603]\u001b[A\n",
      "Training:  46%|████▌     | 247/537 [30:35<31:48,  6.58s/it, running training loss: 0.6603]\u001b[A\n",
      "Training:  46%|████▌     | 247/537 [30:42<31:48,  6.58s/it, running training loss: 0.5165]\u001b[A\n",
      "Training:  46%|████▌     | 248/537 [30:42<32:34,  6.76s/it, running training loss: 0.5165]\u001b[A\n",
      "Training:  46%|████▌     | 248/537 [30:49<32:34,  6.76s/it, running training loss: 0.6032]\u001b[A\n",
      "Training:  46%|████▋     | 249/537 [30:49<32:38,  6.80s/it, running training loss: 0.6032]\u001b[A\n",
      "Training:  46%|████▋     | 249/537 [30:55<32:38,  6.80s/it, running training loss: 0.6019]\u001b[A\n",
      "Training:  47%|████▋     | 250/537 [30:55<31:23,  6.56s/it, running training loss: 0.6019]\u001b[A\n",
      "Training:  47%|████▋     | 250/537 [31:02<31:23,  6.56s/it, running training loss: 0.5658]\u001b[A\n",
      "Training:  47%|████▋     | 251/537 [31:02<31:38,  6.64s/it, running training loss: 0.5658]\u001b[A\n",
      "Training:  47%|████▋     | 251/537 [31:11<31:38,  6.64s/it, running training loss: 0.5985]\u001b[A\n",
      "Training:  47%|████▋     | 252/537 [31:11<34:45,  7.32s/it, running training loss: 0.5985]\u001b[A\n",
      "Training:  47%|████▋     | 252/537 [31:17<34:45,  7.32s/it, running training loss: 0.5335]\u001b[A\n",
      "Training:  47%|████▋     | 253/537 [31:17<33:43,  7.13s/it, running training loss: 0.5335]\u001b[A\n",
      "Training:  47%|████▋     | 253/537 [31:29<33:43,  7.13s/it, running training loss: 0.5765]\u001b[A\n",
      "Training:  47%|████▋     | 254/537 [31:29<39:57,  8.47s/it, running training loss: 0.5765]\u001b[A\n",
      "Training:  47%|████▋     | 254/537 [31:35<39:57,  8.47s/it, running training loss: 0.5390]\u001b[A\n",
      "Training:  47%|████▋     | 255/537 [31:35<37:09,  7.91s/it, running training loss: 0.5390]\u001b[A\n",
      "Training:  47%|████▋     | 255/537 [31:40<37:09,  7.91s/it, running training loss: 0.7161]\u001b[A\n",
      "Training:  48%|████▊     | 256/537 [31:40<32:56,  7.04s/it, running training loss: 0.7161]\u001b[A\n",
      "Training:  48%|████▊     | 256/537 [31:48<32:56,  7.04s/it, running training loss: 0.5556]\u001b[A\n",
      "Training:  48%|████▊     | 257/537 [31:48<33:45,  7.23s/it, running training loss: 0.5556]\u001b[A\n",
      "Training:  48%|████▊     | 257/537 [31:54<33:45,  7.23s/it, running training loss: 0.5711]\u001b[A\n",
      "Training:  48%|████▊     | 258/537 [31:54<31:31,  6.78s/it, running training loss: 0.5711]\u001b[A\n",
      "Training:  48%|████▊     | 258/537 [32:09<31:31,  6.78s/it, running training loss: 0.5611]\u001b[A\n",
      "Training:  48%|████▊     | 259/537 [32:09<43:22,  9.36s/it, running training loss: 0.5611]\u001b[A\n",
      "Training:  48%|████▊     | 259/537 [32:16<43:22,  9.36s/it, running training loss: 0.5978]\u001b[A\n",
      "Training:  48%|████▊     | 260/537 [32:16<40:15,  8.72s/it, running training loss: 0.5978]\u001b[A\n",
      "Training:  48%|████▊     | 260/537 [32:21<40:15,  8.72s/it, running training loss: 0.5637]\u001b[A\n",
      "Training:  49%|████▊     | 261/537 [32:21<34:48,  7.57s/it, running training loss: 0.5637]\u001b[A\n",
      "Training:  49%|████▊     | 261/537 [32:31<34:48,  7.57s/it, running training loss: 0.6538]\u001b[A\n",
      "Training:  49%|████▉     | 262/537 [32:31<37:36,  8.21s/it, running training loss: 0.6538]\u001b[A\n",
      "Training:  49%|████▉     | 262/537 [32:40<37:36,  8.21s/it, running training loss: 0.6230]\u001b[A\n",
      "Training:  49%|████▉     | 263/537 [32:40<38:26,  8.42s/it, running training loss: 0.6230]\u001b[A\n",
      "Training:  49%|████▉     | 263/537 [32:50<38:26,  8.42s/it, running training loss: 0.6104]\u001b[A\n",
      "Training:  49%|████▉     | 264/537 [32:50<40:36,  8.92s/it, running training loss: 0.6104]\u001b[A\n",
      "Training:  49%|████▉     | 264/537 [32:56<40:36,  8.92s/it, running training loss: 0.4918]\u001b[A\n",
      "Training:  49%|████▉     | 265/537 [32:56<36:12,  7.99s/it, running training loss: 0.4918]\u001b[A\n",
      "Training:  49%|████▉     | 265/537 [33:06<36:12,  7.99s/it, running training loss: 0.5304]\u001b[A\n",
      "Training:  50%|████▉     | 266/537 [33:06<38:46,  8.58s/it, running training loss: 0.5304]\u001b[A\n",
      "Training:  50%|████▉     | 266/537 [33:11<38:46,  8.58s/it, running training loss: 0.6200]\u001b[A\n",
      "Training:  50%|████▉     | 267/537 [33:11<34:21,  7.63s/it, running training loss: 0.6200]\u001b[A\n",
      "Training:  50%|████▉     | 267/537 [33:20<34:21,  7.63s/it, running training loss: 0.5694]\u001b[A\n",
      "Training:  50%|████▉     | 268/537 [33:20<36:02,  8.04s/it, running training loss: 0.5694]\u001b[A\n",
      "Training:  50%|████▉     | 268/537 [33:27<36:02,  8.04s/it, running training loss: 0.5494]\u001b[A\n",
      "Training:  50%|█████     | 269/537 [33:27<33:51,  7.58s/it, running training loss: 0.5494]\u001b[A\n",
      "Training:  50%|█████     | 269/537 [33:36<33:51,  7.58s/it, running training loss: 0.6387]\u001b[A\n",
      "Training:  50%|█████     | 270/537 [33:36<36:02,  8.10s/it, running training loss: 0.6387]\u001b[A\n",
      "Training:  50%|█████     | 270/537 [33:43<36:02,  8.10s/it, running training loss: 0.6896]\u001b[A\n",
      "Training:  50%|█████     | 271/537 [33:43<34:50,  7.86s/it, running training loss: 0.6896]\u001b[A\n",
      "Training:  50%|█████     | 271/537 [33:52<34:50,  7.86s/it, running training loss: 0.5193]\u001b[A\n",
      "Training:  51%|█████     | 272/537 [33:52<35:40,  8.08s/it, running training loss: 0.5193]\u001b[A\n",
      "Training:  51%|█████     | 272/537 [34:00<35:40,  8.08s/it, running training loss: 0.4807]\u001b[A\n",
      "Training:  51%|█████     | 273/537 [34:00<35:10,  7.99s/it, running training loss: 0.4807]\u001b[A\n",
      "Training:  51%|█████     | 273/537 [34:06<35:10,  7.99s/it, running training loss: 0.5938]\u001b[A\n",
      "Training:  51%|█████     | 274/537 [34:06<33:04,  7.55s/it, running training loss: 0.5938]\u001b[A\n",
      "Training:  51%|█████     | 274/537 [34:12<33:04,  7.55s/it, running training loss: 0.5877]\u001b[A\n",
      "Training:  51%|█████     | 275/537 [34:12<30:49,  7.06s/it, running training loss: 0.5877]\u001b[A\n",
      "Training:  51%|█████     | 275/537 [34:19<30:49,  7.06s/it, running training loss: 0.5629]\u001b[A\n",
      "Training:  51%|█████▏    | 276/537 [34:19<30:59,  7.12s/it, running training loss: 0.5629]\u001b[A\n",
      "Training:  51%|█████▏    | 276/537 [34:25<30:59,  7.12s/it, running training loss: 0.5785]\u001b[A\n",
      "Training:  52%|█████▏    | 277/537 [34:25<29:10,  6.73s/it, running training loss: 0.5785]\u001b[A\n",
      "Training:  52%|█████▏    | 277/537 [34:32<29:10,  6.73s/it, running training loss: 0.5675]\u001b[A\n",
      "Training:  52%|█████▏    | 278/537 [34:32<29:08,  6.75s/it, running training loss: 0.5675]\u001b[A\n",
      "Training:  52%|█████▏    | 278/537 [34:39<29:08,  6.75s/it, running training loss: 0.5529]\u001b[A\n",
      "Training:  52%|█████▏    | 279/537 [34:39<29:06,  6.77s/it, running training loss: 0.5529]\u001b[A\n",
      "Training:  52%|█████▏    | 279/537 [34:45<29:06,  6.77s/it, running training loss: 0.5970]\u001b[A\n",
      "Training:  52%|█████▏    | 280/537 [34:45<27:35,  6.44s/it, running training loss: 0.5970]\u001b[A\n",
      "Training:  52%|█████▏    | 280/537 [34:51<27:35,  6.44s/it, running training loss: 0.5988]\u001b[A\n",
      "Training:  52%|█████▏    | 281/537 [34:51<28:04,  6.58s/it, running training loss: 0.5988]\u001b[A\n",
      "Training:  52%|█████▏    | 281/537 [34:57<28:04,  6.58s/it, running training loss: 0.5756]\u001b[A\n",
      "Training:  53%|█████▎    | 282/537 [34:57<26:42,  6.28s/it, running training loss: 0.5756]\u001b[A\n",
      "Training:  53%|█████▎    | 282/537 [35:04<26:42,  6.28s/it, running training loss: 0.6132]\u001b[A\n",
      "Training:  53%|█████▎    | 283/537 [35:04<28:01,  6.62s/it, running training loss: 0.6132]\u001b[A\n",
      "Training:  53%|█████▎    | 283/537 [35:11<28:01,  6.62s/it, running training loss: 0.6069]\u001b[A\n",
      "Training:  53%|█████▎    | 284/537 [35:11<27:54,  6.62s/it, running training loss: 0.6069]\u001b[A\n",
      "Training:  53%|█████▎    | 284/537 [35:17<27:54,  6.62s/it, running training loss: 0.5157]\u001b[A\n",
      "Training:  53%|█████▎    | 285/537 [35:17<26:38,  6.34s/it, running training loss: 0.5157]\u001b[A\n",
      "Training:  53%|█████▎    | 285/537 [35:27<26:38,  6.34s/it, running training loss: 0.5807]\u001b[A\n",
      "Training:  53%|█████▎    | 286/537 [35:27<31:23,  7.50s/it, running training loss: 0.5807]\u001b[A\n",
      "Training:  53%|█████▎    | 286/537 [35:34<31:23,  7.50s/it, running training loss: 0.5893]\u001b[A\n",
      "Training:  53%|█████▎    | 287/537 [35:34<30:21,  7.29s/it, running training loss: 0.5893]\u001b[A\n",
      "Training:  53%|█████▎    | 287/537 [35:42<30:21,  7.29s/it, running training loss: 0.5896]\u001b[A\n",
      "Training:  54%|█████▎    | 288/537 [35:42<31:51,  7.68s/it, running training loss: 0.5896]\u001b[A\n",
      "Training:  54%|█████▎    | 288/537 [35:47<31:51,  7.68s/it, running training loss: 0.5622]\u001b[A\n",
      "Training:  54%|█████▍    | 289/537 [35:47<28:02,  6.78s/it, running training loss: 0.5622]\u001b[A\n",
      "Training:  54%|█████▍    | 289/537 [35:54<28:02,  6.78s/it, running training loss: 0.5797]\u001b[A\n",
      "Training:  54%|█████▍    | 290/537 [35:54<28:04,  6.82s/it, running training loss: 0.5797]\u001b[A\n",
      "Training:  54%|█████▍    | 290/537 [36:05<28:04,  6.82s/it, running training loss: 0.5596]\u001b[A\n",
      "Training:  54%|█████▍    | 291/537 [36:05<32:52,  8.02s/it, running training loss: 0.5596]\u001b[A\n",
      "Training:  54%|█████▍    | 291/537 [36:11<32:52,  8.02s/it, running training loss: 0.5561]\u001b[A\n",
      "Training:  54%|█████▍    | 292/537 [36:11<30:00,  7.35s/it, running training loss: 0.5561]\u001b[A\n",
      "Training:  54%|█████▍    | 292/537 [36:22<30:00,  7.35s/it, running training loss: 0.5607]\u001b[A\n",
      "Training:  55%|█████▍    | 293/537 [36:22<34:51,  8.57s/it, running training loss: 0.5607]\u001b[A\n",
      "Training:  55%|█████▍    | 293/537 [36:30<34:51,  8.57s/it, running training loss: 0.5733]\u001b[A\n",
      "Training:  55%|█████▍    | 294/537 [36:30<33:38,  8.30s/it, running training loss: 0.5733]\u001b[A\n",
      "Training:  55%|█████▍    | 294/537 [36:37<33:38,  8.30s/it, running training loss: 0.4430]\u001b[A\n",
      "Training:  55%|█████▍    | 295/537 [36:37<32:30,  8.06s/it, running training loss: 0.4430]\u001b[A\n",
      "Training:  55%|█████▍    | 295/537 [36:43<32:30,  8.06s/it, running training loss: 0.6955]\u001b[A\n",
      "Training:  55%|█████▌    | 296/537 [36:43<29:46,  7.41s/it, running training loss: 0.6955]\u001b[A\n",
      "Training:  55%|█████▌    | 296/537 [36:53<29:46,  7.41s/it, running training loss: 0.6363]\u001b[A\n",
      "Training:  55%|█████▌    | 297/537 [36:53<32:24,  8.10s/it, running training loss: 0.6363]\u001b[A\n",
      "Training:  55%|█████▌    | 297/537 [37:01<32:24,  8.10s/it, running training loss: 0.6109]\u001b[A\n",
      "Training:  55%|█████▌    | 298/537 [37:01<32:07,  8.07s/it, running training loss: 0.6109]\u001b[A\n",
      "Training:  55%|█████▌    | 298/537 [37:08<32:07,  8.07s/it, running training loss: 0.5559]\u001b[A\n",
      "Training:  56%|█████▌    | 299/537 [37:08<30:30,  7.69s/it, running training loss: 0.5559]\u001b[A\n",
      "Training:  56%|█████▌    | 299/537 [37:13<30:30,  7.69s/it, running training loss: 0.5766]\u001b[A\n",
      "Training:  56%|█████▌    | 300/537 [37:13<28:15,  7.15s/it, running training loss: 0.5766]\u001b[A\n",
      "Training:  56%|█████▌    | 300/537 [37:20<28:15,  7.15s/it, running training loss: 0.6129]\u001b[A\n",
      "Training:  56%|█████▌    | 301/537 [37:20<27:35,  7.01s/it, running training loss: 0.6129]\u001b[A\n",
      "Training:  56%|█████▌    | 301/537 [37:33<27:35,  7.01s/it, running training loss: 0.6057]\u001b[A\n",
      "Training:  56%|█████▌    | 302/537 [37:33<34:45,  8.87s/it, running training loss: 0.6057]\u001b[A\n",
      "Training:  56%|█████▌    | 302/537 [37:41<34:45,  8.87s/it, running training loss: 0.5286]\u001b[A\n",
      "Training:  56%|█████▋    | 303/537 [37:41<33:22,  8.56s/it, running training loss: 0.5286]\u001b[A\n",
      "Training:  56%|█████▋    | 303/537 [37:48<33:22,  8.56s/it, running training loss: 0.5257]\u001b[A\n",
      "Training:  57%|█████▋    | 304/537 [37:48<31:30,  8.11s/it, running training loss: 0.5257]\u001b[A\n",
      "Training:  57%|█████▋    | 304/537 [37:55<31:30,  8.11s/it, running training loss: 0.5599]\u001b[A\n",
      "Training:  57%|█████▋    | 305/537 [37:55<29:59,  7.75s/it, running training loss: 0.5599]\u001b[A\n",
      "Training:  57%|█████▋    | 305/537 [38:02<29:59,  7.75s/it, running training loss: 0.5579]\u001b[A\n",
      "Training:  57%|█████▋    | 306/537 [38:02<28:22,  7.37s/it, running training loss: 0.5579]\u001b[A\n",
      "Training:  57%|█████▋    | 306/537 [38:10<28:22,  7.37s/it, running training loss: 0.5832]\u001b[A\n",
      "Training:  57%|█████▋    | 307/537 [38:10<29:34,  7.71s/it, running training loss: 0.5832]\u001b[A\n",
      "Training:  57%|█████▋    | 307/537 [38:16<29:34,  7.71s/it, running training loss: 0.5912]\u001b[A\n",
      "Training:  57%|█████▋    | 308/537 [38:16<27:08,  7.11s/it, running training loss: 0.5912]\u001b[A\n",
      "Training:  57%|█████▋    | 308/537 [38:20<27:08,  7.11s/it, running training loss: 0.5720]\u001b[A\n",
      "Training:  58%|█████▊    | 309/537 [38:20<23:41,  6.24s/it, running training loss: 0.5720]\u001b[A\n",
      "Training:  58%|█████▊    | 309/537 [38:27<23:41,  6.24s/it, running training loss: 0.5606]\u001b[A\n",
      "Training:  58%|█████▊    | 310/537 [38:27<24:21,  6.44s/it, running training loss: 0.5606]\u001b[A\n",
      "Training:  58%|█████▊    | 310/537 [38:35<24:21,  6.44s/it, running training loss: 0.6274]\u001b[A\n",
      "Training:  58%|█████▊    | 311/537 [38:35<26:27,  7.03s/it, running training loss: 0.6274]\u001b[A\n",
      "Training:  58%|█████▊    | 311/537 [38:51<26:27,  7.03s/it, running training loss: 0.7283]\u001b[A\n",
      "Training:  58%|█████▊    | 312/537 [38:51<35:32,  9.48s/it, running training loss: 0.7283]\u001b[A\n",
      "Training:  58%|█████▊    | 312/537 [38:56<35:32,  9.48s/it, running training loss: 0.5336]\u001b[A\n",
      "Training:  58%|█████▊    | 313/537 [38:56<30:34,  8.19s/it, running training loss: 0.5336]\u001b[A\n",
      "Training:  58%|█████▊    | 313/537 [39:02<30:34,  8.19s/it, running training loss: 0.6202]\u001b[A\n",
      "Training:  58%|█████▊    | 314/537 [39:02<28:07,  7.57s/it, running training loss: 0.6202]\u001b[A\n",
      "Training:  58%|█████▊    | 314/537 [39:08<28:07,  7.57s/it, running training loss: 0.5992]\u001b[A\n",
      "Training:  59%|█████▊    | 315/537 [39:08<26:08,  7.07s/it, running training loss: 0.5992]\u001b[A\n",
      "Training:  59%|█████▊    | 315/537 [39:16<26:08,  7.07s/it, running training loss: 0.5708]\u001b[A\n",
      "Training:  59%|█████▉    | 316/537 [39:16<27:41,  7.52s/it, running training loss: 0.5708]\u001b[A\n",
      "Training:  59%|█████▉    | 316/537 [39:23<27:41,  7.52s/it, running training loss: 0.5841]\u001b[A\n",
      "Training:  59%|█████▉    | 317/537 [39:23<26:07,  7.13s/it, running training loss: 0.5841]\u001b[A\n",
      "Training:  59%|█████▉    | 317/537 [39:28<26:07,  7.13s/it, running training loss: 0.5427]\u001b[A\n",
      "Training:  59%|█████▉    | 318/537 [39:28<24:34,  6.73s/it, running training loss: 0.5427]\u001b[A\n",
      "Training:  59%|█████▉    | 318/537 [39:37<24:34,  6.73s/it, running training loss: 0.6527]\u001b[A\n",
      "Training:  59%|█████▉    | 319/537 [39:37<26:49,  7.38s/it, running training loss: 0.6527]\u001b[A\n",
      "Training:  59%|█████▉    | 319/537 [39:44<26:49,  7.38s/it, running training loss: 0.6146]\u001b[A\n",
      "Training:  60%|█████▉    | 320/537 [39:44<25:44,  7.12s/it, running training loss: 0.6146]\u001b[A\n",
      "Training:  60%|█████▉    | 320/537 [39:51<25:44,  7.12s/it, running training loss: 0.5856]\u001b[A\n",
      "Training:  60%|█████▉    | 321/537 [39:51<25:43,  7.15s/it, running training loss: 0.5856]\u001b[A\n",
      "Training:  60%|█████▉    | 321/537 [39:58<25:43,  7.15s/it, running training loss: 0.5405]\u001b[A\n",
      "Training:  60%|█████▉    | 322/537 [39:58<25:19,  7.07s/it, running training loss: 0.5405]\u001b[A\n",
      "Training:  60%|█████▉    | 322/537 [40:05<25:19,  7.07s/it, running training loss: 0.5964]\u001b[A\n",
      "Training:  60%|██████    | 323/537 [40:05<24:54,  6.98s/it, running training loss: 0.5964]\u001b[A\n",
      "Training:  60%|██████    | 323/537 [40:11<24:54,  6.98s/it, running training loss: 0.6106]\u001b[A\n",
      "Training:  60%|██████    | 324/537 [40:11<24:35,  6.93s/it, running training loss: 0.6106]\u001b[A\n",
      "Training:  60%|██████    | 324/537 [40:22<24:35,  6.93s/it, running training loss: 0.5735]\u001b[A\n",
      "Training:  61%|██████    | 325/537 [40:22<28:42,  8.13s/it, running training loss: 0.5735]\u001b[A\n",
      "Training:  61%|██████    | 325/537 [40:29<28:42,  8.13s/it, running training loss: 0.5779]\u001b[A\n",
      "Training:  61%|██████    | 326/537 [40:29<27:11,  7.73s/it, running training loss: 0.5779]\u001b[A\n",
      "Training:  61%|██████    | 326/537 [40:35<27:11,  7.73s/it, running training loss: 0.5603]\u001b[A\n",
      "Training:  61%|██████    | 327/537 [40:35<24:55,  7.12s/it, running training loss: 0.5603]\u001b[A\n",
      "Training:  61%|██████    | 327/537 [40:42<24:55,  7.12s/it, running training loss: 0.6038]\u001b[A\n",
      "Training:  61%|██████    | 328/537 [40:42<25:16,  7.26s/it, running training loss: 0.6038]\u001b[A\n",
      "Training:  61%|██████    | 328/537 [40:53<25:16,  7.26s/it, running training loss: 0.5537]\u001b[A\n",
      "Training:  61%|██████▏   | 329/537 [40:53<28:39,  8.27s/it, running training loss: 0.5537]\u001b[A\n",
      "Training:  61%|██████▏   | 329/537 [41:01<28:39,  8.27s/it, running training loss: 0.5216]\u001b[A\n",
      "Training:  61%|██████▏   | 330/537 [41:01<27:43,  8.04s/it, running training loss: 0.5216]\u001b[A\n",
      "Training:  61%|██████▏   | 330/537 [41:08<27:43,  8.04s/it, running training loss: 0.6770]\u001b[A\n",
      "Training:  62%|██████▏   | 331/537 [41:08<27:09,  7.91s/it, running training loss: 0.6770]\u001b[A\n",
      "Training:  62%|██████▏   | 331/537 [41:17<27:09,  7.91s/it, running training loss: 0.5433]\u001b[A\n",
      "Training:  62%|██████▏   | 332/537 [41:17<28:00,  8.20s/it, running training loss: 0.5433]\u001b[A\n",
      "Training:  62%|██████▏   | 332/537 [41:25<28:00,  8.20s/it, running training loss: 0.6834]\u001b[A\n",
      "Training:  62%|██████▏   | 333/537 [41:25<27:47,  8.17s/it, running training loss: 0.6834]\u001b[A\n",
      "Training:  62%|██████▏   | 333/537 [41:33<27:47,  8.17s/it, running training loss: 0.5628]\u001b[A\n",
      "Training:  62%|██████▏   | 334/537 [41:33<27:40,  8.18s/it, running training loss: 0.5628]\u001b[A\n",
      "Training:  62%|██████▏   | 334/537 [41:39<27:40,  8.18s/it, running training loss: 0.5817]\u001b[A\n",
      "Training:  62%|██████▏   | 335/537 [41:39<24:38,  7.32s/it, running training loss: 0.5817]\u001b[A\n",
      "Training:  62%|██████▏   | 335/537 [41:44<24:38,  7.32s/it, running training loss: 0.6625]\u001b[A\n",
      "Training:  63%|██████▎   | 336/537 [41:44<22:45,  6.79s/it, running training loss: 0.6625]\u001b[A\n",
      "Training:  63%|██████▎   | 336/537 [41:52<22:45,  6.79s/it, running training loss: 0.5740]\u001b[A\n",
      "Training:  63%|██████▎   | 337/537 [41:52<23:16,  6.98s/it, running training loss: 0.5740]\u001b[A\n",
      "Training:  63%|██████▎   | 337/537 [42:00<23:16,  6.98s/it, running training loss: 0.5388]\u001b[A\n",
      "Training:  63%|██████▎   | 338/537 [42:00<24:08,  7.28s/it, running training loss: 0.5388]\u001b[A\n",
      "Training:  63%|██████▎   | 338/537 [42:05<24:08,  7.28s/it, running training loss: 0.5002]\u001b[A\n",
      "Training:  63%|██████▎   | 339/537 [42:05<21:58,  6.66s/it, running training loss: 0.5002]\u001b[A\n",
      "Training:  63%|██████▎   | 339/537 [42:10<21:58,  6.66s/it, running training loss: 0.5228]\u001b[A\n",
      "Training:  63%|██████▎   | 340/537 [42:10<20:49,  6.34s/it, running training loss: 0.5228]\u001b[A\n",
      "Training:  63%|██████▎   | 340/537 [42:18<20:49,  6.34s/it, running training loss: 0.5396]\u001b[A\n",
      "Training:  64%|██████▎   | 341/537 [42:18<21:51,  6.69s/it, running training loss: 0.5396]\u001b[A\n",
      "Training:  64%|██████▎   | 341/537 [42:24<21:51,  6.69s/it, running training loss: 0.5565]\u001b[A\n",
      "Training:  64%|██████▎   | 342/537 [42:24<20:41,  6.37s/it, running training loss: 0.5565]\u001b[A\n",
      "Training:  64%|██████▎   | 342/537 [42:29<20:41,  6.37s/it, running training loss: 0.5729]\u001b[A\n",
      "Training:  64%|██████▍   | 343/537 [42:29<19:43,  6.10s/it, running training loss: 0.5729]\u001b[A\n",
      "Training:  64%|██████▍   | 343/537 [42:39<19:43,  6.10s/it, running training loss: 0.5828]\u001b[A\n",
      "Training:  64%|██████▍   | 344/537 [42:39<23:11,  7.21s/it, running training loss: 0.5828]\u001b[A\n",
      "Training:  64%|██████▍   | 344/537 [42:48<23:11,  7.21s/it, running training loss: 0.5684]\u001b[A\n",
      "Training:  64%|██████▍   | 345/537 [42:48<24:48,  7.75s/it, running training loss: 0.5684]\u001b[A\n",
      "Training:  64%|██████▍   | 345/537 [42:56<24:48,  7.75s/it, running training loss: 0.5995]\u001b[A\n",
      "Training:  64%|██████▍   | 346/537 [42:56<25:23,  7.98s/it, running training loss: 0.5995]\u001b[A\n",
      "Training:  64%|██████▍   | 346/537 [43:02<25:23,  7.98s/it, running training loss: 0.5178]\u001b[A\n",
      "Training:  65%|██████▍   | 347/537 [43:02<23:22,  7.38s/it, running training loss: 0.5178]\u001b[A\n",
      "Training:  65%|██████▍   | 347/537 [43:10<23:22,  7.38s/it, running training loss: 0.5331]\u001b[A\n",
      "Training:  65%|██████▍   | 348/537 [43:10<23:44,  7.54s/it, running training loss: 0.5331]\u001b[A\n",
      "Training:  65%|██████▍   | 348/537 [43:17<23:44,  7.54s/it, running training loss: 0.6325]\u001b[A\n",
      "Training:  65%|██████▍   | 349/537 [43:17<23:06,  7.38s/it, running training loss: 0.6325]\u001b[A\n",
      "Training:  65%|██████▍   | 349/537 [43:23<23:06,  7.38s/it, running training loss: 0.6229]\u001b[A\n",
      "Training:  65%|██████▌   | 350/537 [43:23<21:36,  6.93s/it, running training loss: 0.6229]\u001b[A\n",
      "Training:  65%|██████▌   | 350/537 [43:32<21:36,  6.93s/it, running training loss: 0.6215]\u001b[A\n",
      "Training:  65%|██████▌   | 351/537 [43:32<23:40,  7.64s/it, running training loss: 0.6215]\u001b[A\n",
      "Training:  65%|██████▌   | 351/537 [43:40<23:40,  7.64s/it, running training loss: 0.6199]\u001b[A\n",
      "Training:  66%|██████▌   | 352/537 [43:40<23:37,  7.66s/it, running training loss: 0.6199]\u001b[A\n",
      "Training:  66%|██████▌   | 352/537 [43:45<23:37,  7.66s/it, running training loss: 0.5710]\u001b[A\n",
      "Training:  66%|██████▌   | 353/537 [43:45<21:14,  6.93s/it, running training loss: 0.5710]\u001b[A\n",
      "Training:  66%|██████▌   | 353/537 [43:58<21:14,  6.93s/it, running training loss: 0.5488]\u001b[A\n",
      "Training:  66%|██████▌   | 354/537 [43:58<26:29,  8.68s/it, running training loss: 0.5488]\u001b[A\n",
      "Training:  66%|██████▌   | 354/537 [44:06<26:29,  8.68s/it, running training loss: 0.5691]\u001b[A\n",
      "Training:  66%|██████▌   | 355/537 [44:06<25:10,  8.30s/it, running training loss: 0.5691]\u001b[A\n",
      "Training:  66%|██████▌   | 355/537 [44:15<25:10,  8.30s/it, running training loss: 0.4985]\u001b[A\n",
      "Training:  66%|██████▋   | 356/537 [44:15<25:39,  8.51s/it, running training loss: 0.4985]\u001b[A\n",
      "Training:  66%|██████▋   | 356/537 [44:22<25:39,  8.51s/it, running training loss: 0.5550]\u001b[A\n",
      "Training:  66%|██████▋   | 357/537 [44:22<24:42,  8.24s/it, running training loss: 0.5550]\u001b[A\n",
      "Training:  66%|██████▋   | 357/537 [44:29<24:42,  8.24s/it, running training loss: 0.5840]\u001b[A\n",
      "Training:  67%|██████▋   | 358/537 [44:29<23:33,  7.90s/it, running training loss: 0.5840]\u001b[A\n",
      "Training:  67%|██████▋   | 358/537 [44:35<23:33,  7.90s/it, running training loss: 0.5601]\u001b[A\n",
      "Training:  67%|██████▋   | 359/537 [44:35<21:23,  7.21s/it, running training loss: 0.5601]\u001b[A\n",
      "Training:  67%|██████▋   | 359/537 [44:43<21:23,  7.21s/it, running training loss: 0.5467]\u001b[A\n",
      "Training:  67%|██████▋   | 360/537 [44:43<21:47,  7.39s/it, running training loss: 0.5467]\u001b[A\n",
      "Training:  67%|██████▋   | 360/537 [44:50<21:47,  7.39s/it, running training loss: 0.5334]\u001b[A\n",
      "Training:  67%|██████▋   | 361/537 [44:50<21:13,  7.24s/it, running training loss: 0.5334]\u001b[A\n",
      "Training:  67%|██████▋   | 361/537 [45:03<21:13,  7.24s/it, running training loss: 0.6794]\u001b[A\n",
      "Training:  67%|██████▋   | 362/537 [45:03<26:30,  9.09s/it, running training loss: 0.6794]\u001b[A\n",
      "Training:  67%|██████▋   | 362/537 [45:10<26:30,  9.09s/it, running training loss: 0.5580]\u001b[A\n",
      "Training:  68%|██████▊   | 363/537 [45:10<24:21,  8.40s/it, running training loss: 0.5580]\u001b[A\n",
      "Training:  68%|██████▊   | 363/537 [45:14<24:21,  8.40s/it, running training loss: 0.5375]\u001b[A\n",
      "Training:  68%|██████▊   | 364/537 [45:14<20:56,  7.26s/it, running training loss: 0.5375]\u001b[A\n",
      "Training:  68%|██████▊   | 364/537 [45:22<20:56,  7.26s/it, running training loss: 0.5476]\u001b[A\n",
      "Training:  68%|██████▊   | 365/537 [45:22<20:45,  7.24s/it, running training loss: 0.5476]\u001b[A\n",
      "Training:  68%|██████▊   | 365/537 [45:29<20:45,  7.24s/it, running training loss: 0.5425]\u001b[A\n",
      "Training:  68%|██████▊   | 366/537 [45:29<20:56,  7.35s/it, running training loss: 0.5425]\u001b[A\n",
      "Training:  68%|██████▊   | 366/537 [45:35<20:56,  7.35s/it, running training loss: 0.5764]\u001b[A\n",
      "Training:  68%|██████▊   | 367/537 [45:35<19:15,  6.79s/it, running training loss: 0.5764]\u001b[A\n",
      "Training:  68%|██████▊   | 367/537 [45:42<19:15,  6.79s/it, running training loss: 0.5513]\u001b[A\n",
      "Training:  69%|██████▊   | 368/537 [45:42<19:54,  7.07s/it, running training loss: 0.5513]\u001b[A\n",
      "Training:  69%|██████▊   | 368/537 [45:47<19:54,  7.07s/it, running training loss: 0.6141]\u001b[A\n",
      "Training:  69%|██████▊   | 369/537 [45:47<17:28,  6.24s/it, running training loss: 0.6141]\u001b[A\n",
      "Training:  69%|██████▊   | 369/537 [45:54<17:28,  6.24s/it, running training loss: 0.6135]\u001b[A\n",
      "Training:  69%|██████▉   | 370/537 [45:54<18:39,  6.70s/it, running training loss: 0.6135]\u001b[A\n",
      "Training:  69%|██████▉   | 370/537 [46:04<18:39,  6.70s/it, running training loss: 0.5325]\u001b[A\n",
      "Training:  69%|██████▉   | 371/537 [46:04<21:06,  7.63s/it, running training loss: 0.5325]\u001b[A\n",
      "Training:  69%|██████▉   | 371/537 [46:10<21:06,  7.63s/it, running training loss: 0.5585]\u001b[A\n",
      "Training:  69%|██████▉   | 372/537 [46:10<19:08,  6.96s/it, running training loss: 0.5585]\u001b[A\n",
      "Training:  69%|██████▉   | 372/537 [46:17<19:08,  6.96s/it, running training loss: 0.5484]\u001b[A\n",
      "Training:  69%|██████▉   | 373/537 [46:17<19:43,  7.22s/it, running training loss: 0.5484]\u001b[A\n",
      "Training:  69%|██████▉   | 373/537 [46:22<19:43,  7.22s/it, running training loss: 0.6299]\u001b[A\n",
      "Training:  70%|██████▉   | 374/537 [46:22<17:38,  6.50s/it, running training loss: 0.6299]\u001b[A\n",
      "Training:  70%|██████▉   | 374/537 [46:28<17:38,  6.50s/it, running training loss: 0.5909]\u001b[A\n",
      "Training:  70%|██████▉   | 375/537 [46:28<16:38,  6.16s/it, running training loss: 0.5909]\u001b[A\n",
      "Training:  70%|██████▉   | 375/537 [46:33<16:38,  6.16s/it, running training loss: 0.5804]\u001b[A\n",
      "Training:  70%|███████   | 376/537 [46:33<16:09,  6.02s/it, running training loss: 0.5804]\u001b[A\n",
      "Training:  70%|███████   | 376/537 [46:42<16:09,  6.02s/it, running training loss: 0.5601]\u001b[A\n",
      "Training:  70%|███████   | 377/537 [46:42<18:07,  6.79s/it, running training loss: 0.5601]\u001b[A\n",
      "Training:  70%|███████   | 377/537 [46:49<18:07,  6.79s/it, running training loss: 0.5720]\u001b[A\n",
      "Training:  70%|███████   | 378/537 [46:49<17:55,  6.77s/it, running training loss: 0.5720]\u001b[A\n",
      "Training:  70%|███████   | 378/537 [46:56<17:55,  6.77s/it, running training loss: 0.5692]\u001b[A\n",
      "Training:  71%|███████   | 379/537 [46:56<18:10,  6.90s/it, running training loss: 0.5692]\u001b[A\n",
      "Training:  71%|███████   | 379/537 [47:02<18:10,  6.90s/it, running training loss: 0.7129]\u001b[A\n",
      "Training:  71%|███████   | 380/537 [47:02<17:06,  6.54s/it, running training loss: 0.7129]\u001b[A\n",
      "Training:  71%|███████   | 380/537 [47:10<17:06,  6.54s/it, running training loss: 0.5531]\u001b[A\n",
      "Training:  71%|███████   | 381/537 [47:10<18:08,  6.98s/it, running training loss: 0.5531]\u001b[A\n",
      "Training:  71%|███████   | 381/537 [47:19<18:08,  6.98s/it, running training loss: 0.5792]\u001b[A\n",
      "Training:  71%|███████   | 382/537 [47:19<20:02,  7.76s/it, running training loss: 0.5792]\u001b[A\n",
      "Training:  71%|███████   | 382/537 [47:25<20:02,  7.76s/it, running training loss: 0.5782]\u001b[A\n",
      "Training:  71%|███████▏  | 383/537 [47:25<18:16,  7.12s/it, running training loss: 0.5782]\u001b[A\n",
      "Training:  71%|███████▏  | 383/537 [47:32<18:16,  7.12s/it, running training loss: 0.5293]\u001b[A\n",
      "Training:  72%|███████▏  | 384/537 [47:32<18:07,  7.11s/it, running training loss: 0.5293]\u001b[A\n",
      "Training:  72%|███████▏  | 384/537 [47:38<18:07,  7.11s/it, running training loss: 0.5487]\u001b[A\n",
      "Training:  72%|███████▏  | 385/537 [47:38<17:33,  6.93s/it, running training loss: 0.5487]\u001b[A\n",
      "Training:  72%|███████▏  | 385/537 [47:45<17:33,  6.93s/it, running training loss: 0.5783]\u001b[A\n",
      "Training:  72%|███████▏  | 386/537 [47:45<17:29,  6.95s/it, running training loss: 0.5783]\u001b[A\n",
      "Training:  72%|███████▏  | 386/537 [47:53<17:29,  6.95s/it, running training loss: 0.6498]\u001b[A\n",
      "Training:  72%|███████▏  | 387/537 [47:53<18:09,  7.27s/it, running training loss: 0.6498]\u001b[A\n",
      "Training:  72%|███████▏  | 387/537 [47:59<18:09,  7.27s/it, running training loss: 0.5668]\u001b[A\n",
      "Training:  72%|███████▏  | 388/537 [47:59<16:30,  6.65s/it, running training loss: 0.5668]\u001b[A\n",
      "Training:  72%|███████▏  | 388/537 [48:05<16:30,  6.65s/it, running training loss: 0.5136]\u001b[A\n",
      "Training:  72%|███████▏  | 389/537 [48:05<16:08,  6.54s/it, running training loss: 0.5136]\u001b[A\n",
      "Training:  72%|███████▏  | 389/537 [48:12<16:08,  6.54s/it, running training loss: 0.6145]\u001b[A\n",
      "Training:  73%|███████▎  | 390/537 [48:12<16:13,  6.62s/it, running training loss: 0.6145]\u001b[A\n",
      "Training:  73%|███████▎  | 390/537 [48:21<16:13,  6.62s/it, running training loss: 0.5439]\u001b[A\n",
      "Training:  73%|███████▎  | 391/537 [48:21<18:07,  7.45s/it, running training loss: 0.5439]\u001b[A\n",
      "Training:  73%|███████▎  | 391/537 [48:29<18:07,  7.45s/it, running training loss: 0.5496]\u001b[A\n",
      "Training:  73%|███████▎  | 392/537 [48:29<18:28,  7.65s/it, running training loss: 0.5496]\u001b[A\n",
      "Training:  73%|███████▎  | 392/537 [48:34<18:28,  7.65s/it, running training loss: 0.6112]\u001b[A\n",
      "Training:  73%|███████▎  | 393/537 [48:34<16:01,  6.68s/it, running training loss: 0.6112]\u001b[A\n",
      "Training:  73%|███████▎  | 393/537 [48:39<16:01,  6.68s/it, running training loss: 0.5842]\u001b[A\n",
      "Training:  73%|███████▎  | 394/537 [48:39<14:59,  6.29s/it, running training loss: 0.5842]\u001b[A\n",
      "Training:  73%|███████▎  | 394/537 [48:45<14:59,  6.29s/it, running training loss: 0.6158]\u001b[A\n",
      "Training:  74%|███████▎  | 395/537 [48:45<14:40,  6.20s/it, running training loss: 0.6158]\u001b[A\n",
      "Training:  74%|███████▎  | 395/537 [48:50<14:40,  6.20s/it, running training loss: 0.5750]\u001b[A\n",
      "Training:  74%|███████▎  | 396/537 [48:50<13:30,  5.75s/it, running training loss: 0.5750]\u001b[A\n",
      "Training:  74%|███████▎  | 396/537 [48:56<13:30,  5.75s/it, running training loss: 0.5555]\u001b[A\n",
      "Training:  74%|███████▍  | 397/537 [48:56<13:43,  5.88s/it, running training loss: 0.5555]\u001b[A\n",
      "Training:  74%|███████▍  | 397/537 [49:01<13:43,  5.88s/it, running training loss: 0.5631]\u001b[A\n",
      "Training:  74%|███████▍  | 398/537 [49:01<13:22,  5.77s/it, running training loss: 0.5631]\u001b[A\n",
      "Training:  74%|███████▍  | 398/537 [49:07<13:22,  5.77s/it, running training loss: 0.4729]\u001b[A\n",
      "Training:  74%|███████▍  | 399/537 [49:07<13:29,  5.87s/it, running training loss: 0.4729]\u001b[A\n",
      "Training:  74%|███████▍  | 399/537 [49:16<13:29,  5.87s/it, running training loss: 0.6157]\u001b[A\n",
      "\n",
      "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:   1%|▏         | 1/68 [00:04<05:02,  4.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:   3%|▎         | 2/68 [00:08<04:38,  4.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:   4%|▍         | 3/68 [00:11<04:12,  3.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:   6%|▌         | 4/68 [00:14<04:02,  3.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:   7%|▋         | 5/68 [00:17<03:45,  3.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:   9%|▉         | 6/68 [00:20<03:25,  3.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  10%|█         | 7/68 [00:23<03:18,  3.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  12%|█▏        | 8/68 [00:26<03:15,  3.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  13%|█▎        | 9/68 [00:30<03:09,  3.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  15%|█▍        | 10/68 [00:33<03:07,  3.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  16%|█▌        | 11/68 [00:36<03:03,  3.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  18%|█▊        | 12/68 [00:39<03:00,  3.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  19%|█▉        | 13/68 [00:42<02:56,  3.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  21%|██        | 14/68 [00:45<02:48,  3.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  22%|██▏       | 15/68 [00:48<02:45,  3.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  24%|██▎       | 16/68 [00:52<02:41,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  25%|██▌       | 17/68 [00:54<02:30,  2.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  26%|██▋       | 18/68 [00:58<02:42,  3.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  28%|██▊       | 19/68 [01:00<02:27,  3.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  29%|██▉       | 20/68 [01:03<02:21,  2.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  31%|███       | 21/68 [01:07<02:23,  3.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  32%|███▏      | 22/68 [01:09<02:07,  2.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  34%|███▍      | 23/68 [01:12<02:11,  2.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  35%|███▌      | 24/68 [01:15<02:03,  2.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  37%|███▋      | 25/68 [01:18<02:06,  2.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  38%|███▊      | 26/68 [01:21<02:04,  2.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  40%|███▉      | 27/68 [01:24<02:04,  3.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  41%|████      | 28/68 [01:27<01:57,  2.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  43%|████▎     | 29/68 [01:30<01:55,  2.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  44%|████▍     | 30/68 [01:33<01:57,  3.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  46%|████▌     | 31/68 [01:36<01:48,  2.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  47%|████▋     | 32/68 [01:39<01:48,  3.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  49%|████▊     | 33/68 [01:42<01:46,  3.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  50%|█████     | 34/68 [01:45<01:39,  2.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  51%|█████▏    | 35/68 [01:47<01:33,  2.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  53%|█████▎    | 36/68 [01:51<01:36,  3.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  54%|█████▍    | 37/68 [01:54<01:35,  3.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  56%|█████▌    | 38/68 [01:57<01:33,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  57%|█████▋    | 39/68 [02:00<01:29,  3.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  59%|█████▉    | 40/68 [02:04<01:28,  3.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  60%|██████    | 41/68 [02:06<01:21,  3.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  62%|██████▏   | 42/68 [02:10<01:21,  3.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  63%|██████▎   | 43/68 [02:13<01:19,  3.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  65%|██████▍   | 44/68 [02:16<01:15,  3.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  66%|██████▌   | 45/68 [02:20<01:15,  3.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  68%|██████▊   | 46/68 [02:23<01:12,  3.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  69%|██████▉   | 47/68 [02:26<01:08,  3.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  71%|███████   | 48/68 [02:30<01:09,  3.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  72%|███████▏  | 49/68 [02:34<01:09,  3.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  74%|███████▎  | 50/68 [02:37<01:02,  3.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  75%|███████▌  | 51/68 [02:41<00:58,  3.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  76%|███████▋  | 52/68 [02:44<00:54,  3.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  78%|███████▊  | 53/68 [02:47<00:50,  3.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  79%|███████▉  | 54/68 [02:50<00:46,  3.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  81%|████████  | 55/68 [02:53<00:41,  3.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  82%|████████▏ | 56/68 [02:56<00:35,  2.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  84%|████████▍ | 57/68 [02:59<00:32,  2.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  85%|████████▌ | 58/68 [03:02<00:30,  3.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  87%|████████▋ | 59/68 [03:04<00:26,  2.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  88%|████████▊ | 60/68 [03:08<00:23,  2.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  90%|████████▉ | 61/68 [03:11<00:20,  2.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  91%|█████████ | 62/68 [03:14<00:18,  3.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  93%|█████████▎| 63/68 [03:17<00:15,  3.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  94%|█████████▍| 64/68 [03:20<00:12,  3.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  96%|█████████▌| 65/68 [03:23<00:09,  3.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  97%|█████████▋| 66/68 [03:27<00:06,  3.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation:  99%|█████████▊| 67/68 [03:30<00:03,  3.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluation: 100%|██████████| 68/68 [03:32<00:00,  3.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Training:  74%|███████▍  | 400/537 [52:49<2:40:59, 70.51s/it, running training loss: 0.6157]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> training loss: 0.607915, valid loss: 0.535067, valid f1: 0.458794, valid acc: 0.704819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:  74%|███████▍  | 400/537 [52:56<2:40:59, 70.51s/it, running training loss: 0.5476]\u001b[A\n",
      "Training:  75%|███████▍  | 401/537 [52:56<1:57:01, 51.63s/it, running training loss: 0.5476]\u001b[A\n",
      "Training:  75%|███████▍  | 401/537 [53:03<1:57:01, 51.63s/it, running training loss: 0.5423]\u001b[A\n",
      "Training:  75%|███████▍  | 402/537 [53:03<1:26:02, 38.24s/it, running training loss: 0.5423]\u001b[A\n",
      "Training:  75%|███████▍  | 402/537 [53:11<1:26:02, 38.24s/it, running training loss: 0.5135]\u001b[A\n",
      "Training:  75%|███████▌  | 403/537 [53:11<1:05:12, 29.20s/it, running training loss: 0.5135]\u001b[A\n",
      "Training:  75%|███████▌  | 403/537 [53:18<1:05:12, 29.20s/it, running training loss: 0.6159]\u001b[A\n",
      "Training:  75%|███████▌  | 404/537 [53:18<49:45, 22.45s/it, running training loss: 0.6159]  \u001b[A\n",
      "Training:  75%|███████▌  | 404/537 [53:24<49:45, 22.45s/it, running training loss: 0.5871]\u001b[A\n",
      "Training:  75%|███████▌  | 405/537 [53:24<38:39, 17.57s/it, running training loss: 0.5871]\u001b[A\n",
      "Training:  75%|███████▌  | 405/537 [53:32<38:39, 17.57s/it, running training loss: 0.5399]\u001b[A\n",
      "Training:  76%|███████▌  | 406/537 [53:32<31:38, 14.49s/it, running training loss: 0.5399]\u001b[A\n",
      "Training:  76%|███████▌  | 406/537 [53:37<31:38, 14.49s/it, running training loss: 0.5596]\u001b[A\n",
      "Training:  76%|███████▌  | 407/537 [53:37<25:17, 11.67s/it, running training loss: 0.5596]\u001b[A\n",
      "Training:  76%|███████▌  | 407/537 [53:43<25:17, 11.67s/it, running training loss: 0.5289]\u001b[A\n",
      "Training:  76%|███████▌  | 408/537 [53:43<21:45, 10.12s/it, running training loss: 0.5289]\u001b[A\n",
      "Training:  76%|███████▌  | 408/537 [53:53<21:45, 10.12s/it, running training loss: 0.4956]\u001b[A\n",
      "Training:  76%|███████▌  | 409/537 [53:53<21:11,  9.93s/it, running training loss: 0.4956]\u001b[A\n",
      "Training:  76%|███████▌  | 409/537 [53:58<21:11,  9.93s/it, running training loss: 0.6199]\u001b[A\n",
      "Training:  76%|███████▋  | 410/537 [53:58<18:17,  8.64s/it, running training loss: 0.6199]\u001b[A\n",
      "Training:  76%|███████▋  | 410/537 [54:05<18:17,  8.64s/it, running training loss: 0.5270]\u001b[A\n",
      "Training:  77%|███████▋  | 411/537 [54:05<17:02,  8.11s/it, running training loss: 0.5270]\u001b[A\n",
      "Training:  77%|███████▋  | 411/537 [54:13<17:02,  8.11s/it, running training loss: 0.5510]\u001b[A\n",
      "Training:  77%|███████▋  | 412/537 [54:13<16:49,  8.08s/it, running training loss: 0.5510]\u001b[A\n",
      "Training:  77%|███████▋  | 412/537 [54:22<16:49,  8.08s/it, running training loss: 0.6340]\u001b[A\n",
      "Training:  77%|███████▋  | 413/537 [54:22<17:19,  8.38s/it, running training loss: 0.6340]\u001b[A\n",
      "Training:  77%|███████▋  | 413/537 [54:32<17:19,  8.38s/it, running training loss: 0.6259]\u001b[A\n",
      "Training:  77%|███████▋  | 414/537 [54:32<18:07,  8.84s/it, running training loss: 0.6259]\u001b[A\n",
      "Training:  77%|███████▋  | 414/537 [54:39<18:07,  8.84s/it, running training loss: 0.4845]\u001b[A\n",
      "Training:  77%|███████▋  | 415/537 [54:39<16:25,  8.08s/it, running training loss: 0.4845]\u001b[A\n",
      "Training:  77%|███████▋  | 415/537 [54:48<16:25,  8.08s/it, running training loss: 0.6140]\u001b[A\n",
      "Training:  77%|███████▋  | 416/537 [54:48<16:54,  8.39s/it, running training loss: 0.6140]\u001b[A\n",
      "Training:  77%|███████▋  | 416/537 [54:55<16:54,  8.39s/it, running training loss: 0.5016]\u001b[A\n",
      "Training:  78%|███████▊  | 417/537 [54:55<16:14,  8.12s/it, running training loss: 0.5016]\u001b[A\n",
      "Training:  78%|███████▊  | 417/537 [55:03<16:14,  8.12s/it, running training loss: 0.4965]\u001b[A\n",
      "Training:  78%|███████▊  | 418/537 [55:03<15:54,  8.02s/it, running training loss: 0.4965]\u001b[A\n",
      "Training:  78%|███████▊  | 418/537 [55:08<15:54,  8.02s/it, running training loss: 0.5102]\u001b[A\n",
      "Training:  78%|███████▊  | 419/537 [55:08<13:52,  7.05s/it, running training loss: 0.5102]\u001b[A\n",
      "Training:  78%|███████▊  | 419/537 [55:14<13:52,  7.05s/it, running training loss: 0.5075]\u001b[A\n",
      "Training:  78%|███████▊  | 420/537 [55:14<13:07,  6.73s/it, running training loss: 0.5075]\u001b[A\n",
      "Training:  78%|███████▊  | 420/537 [55:22<13:07,  6.73s/it, running training loss: 0.4681]\u001b[A\n",
      "Training:  78%|███████▊  | 421/537 [55:22<13:53,  7.18s/it, running training loss: 0.4681]\u001b[A\n",
      "Training:  78%|███████▊  | 421/537 [55:29<13:53,  7.18s/it, running training loss: 0.5438]\u001b[A\n",
      "Training:  79%|███████▊  | 422/537 [55:29<13:45,  7.18s/it, running training loss: 0.5438]\u001b[A\n",
      "Training:  79%|███████▊  | 422/537 [55:35<13:45,  7.18s/it, running training loss: 0.6163]\u001b[A\n",
      "Training:  79%|███████▉  | 423/537 [55:35<12:41,  6.68s/it, running training loss: 0.6163]\u001b[A\n",
      "Training:  79%|███████▉  | 423/537 [55:42<12:41,  6.68s/it, running training loss: 0.7442]\u001b[A\n",
      "Training:  79%|███████▉  | 424/537 [55:42<13:12,  7.01s/it, running training loss: 0.7442]\u001b[A\n",
      "Training:  79%|███████▉  | 424/537 [55:51<13:12,  7.01s/it, running training loss: 0.5441]\u001b[A\n",
      "Training:  79%|███████▉  | 425/537 [55:51<13:58,  7.49s/it, running training loss: 0.5441]\u001b[A\n",
      "Training:  79%|███████▉  | 425/537 [55:57<13:58,  7.49s/it, running training loss: 0.5091]\u001b[A\n",
      "Training:  79%|███████▉  | 426/537 [55:57<13:08,  7.10s/it, running training loss: 0.5091]\u001b[A\n",
      "Training:  79%|███████▉  | 426/537 [56:06<13:08,  7.10s/it, running training loss: 0.5550]\u001b[A\n",
      "Training:  80%|███████▉  | 427/537 [56:06<14:07,  7.71s/it, running training loss: 0.5550]\u001b[A\n",
      "Training:  80%|███████▉  | 427/537 [56:11<14:07,  7.71s/it, running training loss: 0.5191]\u001b[A\n",
      "Training:  80%|███████▉  | 428/537 [56:11<12:31,  6.89s/it, running training loss: 0.5191]\u001b[A\n",
      "Training:  80%|███████▉  | 428/537 [56:21<12:31,  6.89s/it, running training loss: 0.6276]\u001b[A\n",
      "Training:  80%|███████▉  | 429/537 [56:21<14:01,  7.79s/it, running training loss: 0.6276]\u001b[A\n",
      "Training:  80%|███████▉  | 429/537 [56:28<14:01,  7.79s/it, running training loss: 0.5306]\u001b[A\n",
      "Training:  80%|████████  | 430/537 [56:28<13:22,  7.50s/it, running training loss: 0.5306]\u001b[A\n",
      "Training:  80%|████████  | 430/537 [56:35<13:22,  7.50s/it, running training loss: 0.5861]\u001b[A\n",
      "Training:  80%|████████  | 431/537 [56:35<12:58,  7.34s/it, running training loss: 0.5861]\u001b[A\n",
      "Training:  80%|████████  | 431/537 [56:48<12:58,  7.34s/it, running training loss: 0.5888]\u001b[A\n",
      "Training:  80%|████████  | 432/537 [56:48<15:34,  8.90s/it, running training loss: 0.5888]\u001b[A\n",
      "Training:  80%|████████  | 432/537 [56:55<15:34,  8.90s/it, running training loss: 0.6277]\u001b[A\n",
      "Training:  81%|████████  | 433/537 [56:55<14:44,  8.50s/it, running training loss: 0.6277]\u001b[A\n",
      "Training:  81%|████████  | 433/537 [57:03<14:44,  8.50s/it, running training loss: 0.5312]\u001b[A\n",
      "Training:  81%|████████  | 434/537 [57:03<14:08,  8.23s/it, running training loss: 0.5312]\u001b[A\n",
      "Training:  81%|████████  | 434/537 [57:09<14:08,  8.23s/it, running training loss: 0.5389]\u001b[A\n",
      "Training:  81%|████████  | 435/537 [57:09<12:57,  7.62s/it, running training loss: 0.5389]\u001b[A\n",
      "Training:  81%|████████  | 435/537 [57:16<12:57,  7.62s/it, running training loss: 0.6044]\u001b[A\n",
      "Training:  81%|████████  | 436/537 [57:16<12:46,  7.59s/it, running training loss: 0.6044]\u001b[A\n",
      "Training:  81%|████████  | 436/537 [57:27<12:46,  7.59s/it, running training loss: 0.4906]\u001b[A\n",
      "Training:  81%|████████▏ | 437/537 [57:27<13:57,  8.37s/it, running training loss: 0.4906]\u001b[A\n",
      "Training:  81%|████████▏ | 437/537 [57:31<13:57,  8.37s/it, running training loss: 0.5531]\u001b[A\n",
      "Training:  82%|████████▏ | 438/537 [57:31<11:59,  7.27s/it, running training loss: 0.5531]\u001b[A\n",
      "Training:  82%|████████▏ | 438/537 [57:38<11:59,  7.27s/it, running training loss: 0.5758]\u001b[A\n",
      "Training:  82%|████████▏ | 439/537 [57:38<11:24,  6.98s/it, running training loss: 0.5758]\u001b[A\n",
      "Training:  82%|████████▏ | 439/537 [57:43<11:24,  6.98s/it, running training loss: 0.5405]\u001b[A\n",
      "Training:  82%|████████▏ | 440/537 [57:43<10:36,  6.57s/it, running training loss: 0.5405]\u001b[A\n",
      "Training:  82%|████████▏ | 440/537 [57:51<10:36,  6.57s/it, running training loss: 0.5065]\u001b[A\n",
      "Training:  82%|████████▏ | 441/537 [57:51<10:59,  6.87s/it, running training loss: 0.5065]\u001b[A\n",
      "Training:  82%|████████▏ | 441/537 [57:58<10:59,  6.87s/it, running training loss: 0.5006]\u001b[A\n",
      "Training:  82%|████████▏ | 442/537 [57:58<11:05,  7.00s/it, running training loss: 0.5006]\u001b[A\n",
      "Training:  82%|████████▏ | 442/537 [58:06<11:05,  7.00s/it, running training loss: 0.5200]\u001b[A\n",
      "Training:  82%|████████▏ | 443/537 [58:06<11:33,  7.38s/it, running training loss: 0.5200]\u001b[A\n",
      "Training:  82%|████████▏ | 443/537 [58:14<11:33,  7.38s/it, running training loss: 0.4999]\u001b[A\n",
      "Training:  83%|████████▎ | 444/537 [58:14<11:41,  7.55s/it, running training loss: 0.4999]\u001b[A\n",
      "Training:  83%|████████▎ | 444/537 [58:20<11:41,  7.55s/it, running training loss: 0.6347]\u001b[A\n",
      "Training:  83%|████████▎ | 445/537 [58:20<10:53,  7.11s/it, running training loss: 0.6347]\u001b[A\n",
      "Training:  83%|████████▎ | 445/537 [58:27<10:53,  7.11s/it, running training loss: 0.5515]\u001b[A\n",
      "Training:  83%|████████▎ | 446/537 [58:27<10:44,  7.08s/it, running training loss: 0.5515]\u001b[A\n",
      "Training:  83%|████████▎ | 446/537 [58:32<10:44,  7.08s/it, running training loss: 0.5289]\u001b[A\n",
      "Training:  83%|████████▎ | 447/537 [58:32<09:40,  6.45s/it, running training loss: 0.5289]\u001b[A\n",
      "Training:  83%|████████▎ | 447/537 [58:43<09:40,  6.45s/it, running training loss: 0.5166]\u001b[A\n",
      "Training:  83%|████████▎ | 448/537 [58:43<11:19,  7.64s/it, running training loss: 0.5166]\u001b[A\n",
      "Training:  83%|████████▎ | 448/537 [58:59<11:19,  7.64s/it, running training loss: 0.6133]\u001b[A\n",
      "Training:  84%|████████▎ | 449/537 [58:59<14:47, 10.09s/it, running training loss: 0.6133]\u001b[A\n",
      "Training:  84%|████████▎ | 449/537 [59:05<14:47, 10.09s/it, running training loss: 0.5789]\u001b[A\n",
      "Training:  84%|████████▍ | 450/537 [59:05<12:48,  8.83s/it, running training loss: 0.5789]\u001b[A\n",
      "Training:  84%|████████▍ | 450/537 [59:10<12:48,  8.83s/it, running training loss: 0.5391]\u001b[A\n",
      "Training:  84%|████████▍ | 451/537 [59:10<11:24,  7.95s/it, running training loss: 0.5391]\u001b[A\n",
      "Training:  84%|████████▍ | 451/537 [59:22<11:24,  7.95s/it, running training loss: 0.6953]\u001b[A\n",
      "Training:  84%|████████▍ | 452/537 [59:22<12:48,  9.05s/it, running training loss: 0.6953]\u001b[A\n",
      "Training:  84%|████████▍ | 452/537 [59:29<12:48,  9.05s/it, running training loss: 0.5940]\u001b[A\n",
      "Training:  84%|████████▍ | 453/537 [59:29<11:50,  8.46s/it, running training loss: 0.5940]\u001b[A\n",
      "Training:  84%|████████▍ | 453/537 [59:35<11:50,  8.46s/it, running training loss: 0.7243]\u001b[A\n",
      "Training:  85%|████████▍ | 454/537 [59:35<10:35,  7.66s/it, running training loss: 0.7243]\u001b[A\n",
      "Training:  85%|████████▍ | 454/537 [59:43<10:35,  7.66s/it, running training loss: 0.5808]\u001b[A\n",
      "Training:  85%|████████▍ | 455/537 [59:43<10:32,  7.71s/it, running training loss: 0.5808]\u001b[A\n",
      "Training:  85%|████████▍ | 455/537 [59:50<10:32,  7.71s/it, running training loss: 0.5899]\u001b[A\n",
      "Training:  85%|████████▍ | 456/537 [59:50<10:09,  7.52s/it, running training loss: 0.5899]\u001b[A\n",
      "Training:  85%|████████▍ | 456/537 [59:55<10:09,  7.52s/it, running training loss: 0.5697]\u001b[A\n",
      "Training:  85%|████████▌ | 457/537 [59:55<09:13,  6.92s/it, running training loss: 0.5697]\u001b[A\n",
      "Training:  85%|████████▌ | 457/537 [1:00:02<09:13,  6.92s/it, running training loss: 0.5673]\u001b[A\n",
      "Training:  85%|████████▌ | 458/537 [1:00:02<09:10,  6.97s/it, running training loss: 0.5673]\u001b[A\n",
      "Training:  85%|████████▌ | 458/537 [1:00:15<09:10,  6.97s/it, running training loss: 0.6196]\u001b[A\n",
      "Training:  85%|████████▌ | 459/537 [1:00:15<11:06,  8.54s/it, running training loss: 0.6196]\u001b[A\n",
      "Training:  85%|████████▌ | 459/537 [1:00:21<11:06,  8.54s/it, running training loss: 0.6025]\u001b[A\n",
      "Training:  86%|████████▌ | 460/537 [1:00:21<10:12,  7.96s/it, running training loss: 0.6025]\u001b[A\n",
      "Training:  86%|████████▌ | 460/537 [1:00:29<10:12,  7.96s/it, running training loss: 0.5941]\u001b[A\n",
      "Training:  86%|████████▌ | 461/537 [1:00:29<10:07,  8.00s/it, running training loss: 0.5941]\u001b[A\n",
      "Training:  86%|████████▌ | 461/537 [1:00:35<10:07,  8.00s/it, running training loss: 0.6296]\u001b[A\n",
      "Training:  86%|████████▌ | 462/537 [1:00:35<09:13,  7.37s/it, running training loss: 0.6296]\u001b[A\n",
      "Training:  86%|████████▌ | 462/537 [1:00:41<09:13,  7.37s/it, running training loss: 0.5049]\u001b[A\n",
      "Training:  86%|████████▌ | 463/537 [1:00:41<08:21,  6.78s/it, running training loss: 0.5049]\u001b[A\n",
      "Training:  86%|████████▌ | 463/537 [1:00:46<08:21,  6.78s/it, running training loss: 0.5816]\u001b[A\n",
      "Training:  86%|████████▋ | 464/537 [1:00:46<07:46,  6.39s/it, running training loss: 0.5816]\u001b[A\n",
      "Training:  86%|████████▋ | 464/537 [1:00:54<07:46,  6.39s/it, running training loss: 0.6276]\u001b[A\n",
      "Training:  87%|████████▋ | 465/537 [1:00:54<08:13,  6.85s/it, running training loss: 0.6276]\u001b[A\n",
      "Training:  87%|████████▋ | 465/537 [1:01:04<08:13,  6.85s/it, running training loss: 0.5196]\u001b[A\n",
      "Training:  87%|████████▋ | 466/537 [1:01:04<09:11,  7.76s/it, running training loss: 0.5196]\u001b[A\n",
      "Training:  87%|████████▋ | 466/537 [1:01:09<09:11,  7.76s/it, running training loss: 0.5596]\u001b[A\n",
      "Training:  87%|████████▋ | 467/537 [1:01:09<08:03,  6.90s/it, running training loss: 0.5596]\u001b[A\n",
      "Training:  87%|████████▋ | 467/537 [1:01:15<08:03,  6.90s/it, running training loss: 0.6061]\u001b[A\n",
      "Training:  87%|████████▋ | 468/537 [1:01:15<07:47,  6.78s/it, running training loss: 0.6061]\u001b[A\n",
      "Training:  87%|████████▋ | 468/537 [1:01:25<07:47,  6.78s/it, running training loss: 0.6147]\u001b[A\n",
      "Training:  87%|████████▋ | 469/537 [1:01:25<08:43,  7.69s/it, running training loss: 0.6147]\u001b[A\n",
      "Training:  87%|████████▋ | 469/537 [1:01:31<08:43,  7.69s/it, running training loss: 0.5854]\u001b[A\n",
      "Training:  88%|████████▊ | 470/537 [1:01:31<08:01,  7.18s/it, running training loss: 0.5854]\u001b[A\n",
      "Training:  88%|████████▊ | 470/537 [1:01:37<08:01,  7.18s/it, running training loss: 0.5332]\u001b[A\n",
      "Training:  88%|████████▊ | 471/537 [1:01:37<07:36,  6.92s/it, running training loss: 0.5332]\u001b[A\n",
      "Training:  88%|████████▊ | 471/537 [1:01:47<07:36,  6.92s/it, running training loss: 0.5633]\u001b[A\n",
      "Training:  88%|████████▊ | 472/537 [1:01:47<08:27,  7.81s/it, running training loss: 0.5633]\u001b[A\n",
      "Training:  88%|████████▊ | 472/537 [1:01:55<08:27,  7.81s/it, running training loss: 0.4588]\u001b[A\n",
      "Training:  88%|████████▊ | 473/537 [1:01:55<08:12,  7.69s/it, running training loss: 0.4588]\u001b[A\n",
      "Training:  88%|████████▊ | 473/537 [1:02:00<08:12,  7.69s/it, running training loss: 0.4929]\u001b[A\n",
      "Training:  88%|████████▊ | 474/537 [1:02:00<07:19,  6.97s/it, running training loss: 0.4929]\u001b[A\n",
      "Training:  88%|████████▊ | 474/537 [1:02:06<07:19,  6.97s/it, running training loss: 0.6361]\u001b[A\n",
      "Training:  88%|████████▊ | 475/537 [1:02:06<06:48,  6.60s/it, running training loss: 0.6361]\u001b[A\n",
      "Training:  88%|████████▊ | 475/537 [1:02:13<06:48,  6.60s/it, running training loss: 0.5282]\u001b[A\n",
      "Training:  89%|████████▊ | 476/537 [1:02:13<07:02,  6.92s/it, running training loss: 0.5282]\u001b[A\n",
      "Training:  89%|████████▊ | 476/537 [1:02:19<07:02,  6.92s/it, running training loss: 0.5348]\u001b[A\n",
      "Training:  89%|████████▉ | 477/537 [1:02:19<06:31,  6.53s/it, running training loss: 0.5348]\u001b[A\n",
      "Training:  89%|████████▉ | 477/537 [1:02:29<06:31,  6.53s/it, running training loss: 0.5223]\u001b[A\n",
      "Training:  89%|████████▉ | 478/537 [1:02:29<07:17,  7.42s/it, running training loss: 0.5223]\u001b[A\n",
      "Training:  89%|████████▉ | 478/537 [1:02:34<07:17,  7.42s/it, running training loss: 0.5744]\u001b[A\n",
      "Training:  89%|████████▉ | 479/537 [1:02:34<06:41,  6.93s/it, running training loss: 0.5744]\u001b[A\n",
      "Training:  89%|████████▉ | 479/537 [1:02:40<06:41,  6.93s/it, running training loss: 0.5625]\u001b[A\n",
      "Training:  89%|████████▉ | 480/537 [1:02:40<06:12,  6.53s/it, running training loss: 0.5625]\u001b[A\n",
      "Training:  89%|████████▉ | 480/537 [1:02:48<06:12,  6.53s/it, running training loss: 0.5670]\u001b[A\n",
      "Training:  90%|████████▉ | 481/537 [1:02:48<06:39,  7.13s/it, running training loss: 0.5670]\u001b[A\n",
      "Training:  90%|████████▉ | 481/537 [1:02:56<06:39,  7.13s/it, running training loss: 0.4954]\u001b[A\n",
      "Training:  90%|████████▉ | 482/537 [1:02:56<06:38,  7.24s/it, running training loss: 0.4954]\u001b[A\n",
      "Training:  90%|████████▉ | 482/537 [1:03:03<06:38,  7.24s/it, running training loss: 0.6416]\u001b[A\n",
      "Training:  90%|████████▉ | 483/537 [1:03:03<06:22,  7.08s/it, running training loss: 0.6416]\u001b[A\n",
      "Training:  90%|████████▉ | 483/537 [1:03:10<06:22,  7.08s/it, running training loss: 0.5763]\u001b[A\n",
      "Training:  90%|█████████ | 484/537 [1:03:10<06:20,  7.18s/it, running training loss: 0.5763]\u001b[A\n",
      "Training:  90%|█████████ | 484/537 [1:03:15<06:20,  7.18s/it, running training loss: 0.5023]\u001b[A\n",
      "Training:  90%|█████████ | 485/537 [1:03:15<05:45,  6.64s/it, running training loss: 0.5023]\u001b[A\n",
      "Training:  90%|█████████ | 485/537 [1:03:24<05:45,  6.64s/it, running training loss: 0.5218]\u001b[A\n",
      "Training:  91%|█████████ | 486/537 [1:03:24<06:09,  7.25s/it, running training loss: 0.5218]\u001b[A\n",
      "Training:  91%|█████████ | 486/537 [1:03:34<06:09,  7.25s/it, running training loss: 0.5643]\u001b[A\n",
      "Training:  91%|█████████ | 487/537 [1:03:34<06:36,  7.93s/it, running training loss: 0.5643]\u001b[A\n",
      "Training:  91%|█████████ | 487/537 [1:03:39<06:36,  7.93s/it, running training loss: 0.5692]\u001b[A\n",
      "Training:  91%|█████████ | 488/537 [1:03:39<05:57,  7.29s/it, running training loss: 0.5692]\u001b[A\n",
      "Training:  91%|█████████ | 488/537 [1:03:47<05:57,  7.29s/it, running training loss: 0.4541]\u001b[A\n",
      "Training:  91%|█████████ | 489/537 [1:03:47<05:55,  7.41s/it, running training loss: 0.4541]\u001b[A\n",
      "Training:  91%|█████████ | 489/537 [1:03:54<05:55,  7.41s/it, running training loss: 0.5295]\u001b[A\n",
      "Training:  91%|█████████ | 490/537 [1:03:54<05:35,  7.14s/it, running training loss: 0.5295]\u001b[A\n",
      "Training:  91%|█████████ | 490/537 [1:04:00<05:35,  7.14s/it, running training loss: 0.5382]\u001b[A\n",
      "Training:  91%|█████████▏| 491/537 [1:04:00<05:19,  6.95s/it, running training loss: 0.5382]\u001b[A\n",
      "Training:  91%|█████████▏| 491/537 [1:04:07<05:19,  6.95s/it, running training loss: 0.5548]\u001b[A\n",
      "Training:  92%|█████████▏| 492/537 [1:04:07<05:09,  6.87s/it, running training loss: 0.5548]\u001b[A\n",
      "Training:  92%|█████████▏| 492/537 [1:04:15<05:09,  6.87s/it, running training loss: 0.4696]\u001b[A\n",
      "Training:  92%|█████████▏| 493/537 [1:04:15<05:13,  7.13s/it, running training loss: 0.4696]\u001b[A\n",
      "Training:  92%|█████████▏| 493/537 [1:04:26<05:13,  7.13s/it, running training loss: 0.5817]\u001b[A\n",
      "Training:  92%|█████████▏| 494/537 [1:04:26<06:00,  8.38s/it, running training loss: 0.5817]\u001b[A\n",
      "Training:  92%|█████████▏| 494/537 [1:04:32<06:00,  8.38s/it, running training loss: 0.5222]\u001b[A\n",
      "Training:  92%|█████████▏| 495/537 [1:04:32<05:18,  7.57s/it, running training loss: 0.5222]\u001b[A\n",
      "Training:  92%|█████████▏| 495/537 [1:04:38<05:18,  7.57s/it, running training loss: 0.5911]\u001b[A\n",
      "Training:  92%|█████████▏| 496/537 [1:04:38<04:59,  7.31s/it, running training loss: 0.5911]\u001b[A\n",
      "Training:  92%|█████████▏| 496/537 [1:04:45<04:59,  7.31s/it, running training loss: 0.5288]\u001b[A\n",
      "Training:  93%|█████████▎| 497/537 [1:04:45<04:51,  7.28s/it, running training loss: 0.5288]\u001b[A\n",
      "Training:  93%|█████████▎| 497/537 [1:04:58<04:51,  7.28s/it, running training loss: 0.6085]\u001b[A\n",
      "Training:  93%|█████████▎| 498/537 [1:04:58<05:49,  8.96s/it, running training loss: 0.6085]\u001b[A\n",
      "Training:  93%|█████████▎| 498/537 [1:05:03<05:49,  8.96s/it, running training loss: 0.5001]\u001b[A\n",
      "Training:  93%|█████████▎| 499/537 [1:05:03<04:55,  7.78s/it, running training loss: 0.5001]\u001b[A\n",
      "Training:  93%|█████████▎| 499/537 [1:05:10<04:55,  7.78s/it, running training loss: 0.6310]\u001b[A\n",
      "Training:  93%|█████████▎| 500/537 [1:05:10<04:37,  7.51s/it, running training loss: 0.6310]\u001b[A\n",
      "Training:  93%|█████████▎| 500/537 [1:05:16<04:37,  7.51s/it, running training loss: 0.5488]\u001b[A\n",
      "Training:  93%|█████████▎| 501/537 [1:05:16<04:12,  7.00s/it, running training loss: 0.5488]\u001b[A\n",
      "Training:  93%|█████████▎| 501/537 [1:05:24<04:12,  7.00s/it, running training loss: 0.5351]\u001b[A\n",
      "Training:  93%|█████████▎| 502/537 [1:05:24<04:17,  7.36s/it, running training loss: 0.5351]\u001b[A\n",
      "Training:  93%|█████████▎| 502/537 [1:05:31<04:17,  7.36s/it, running training loss: 0.4775]\u001b[A\n",
      "Training:  94%|█████████▎| 503/537 [1:05:31<04:05,  7.22s/it, running training loss: 0.4775]\u001b[A\n",
      "Training:  94%|█████████▎| 503/537 [1:05:38<04:05,  7.22s/it, running training loss: 0.5566]\u001b[A\n",
      "Training:  94%|█████████▍| 504/537 [1:05:38<03:52,  7.04s/it, running training loss: 0.5566]\u001b[A\n",
      "Training:  94%|█████████▍| 504/537 [1:05:45<03:52,  7.04s/it, running training loss: 0.4934]\u001b[A\n",
      "Training:  94%|█████████▍| 505/537 [1:05:45<03:42,  6.97s/it, running training loss: 0.4934]\u001b[A\n",
      "Training:  94%|█████████▍| 505/537 [1:05:50<03:42,  6.97s/it, running training loss: 0.5425]\u001b[A\n",
      "Training:  94%|█████████▍| 506/537 [1:05:50<03:23,  6.56s/it, running training loss: 0.5425]\u001b[A\n",
      "Training:  94%|█████████▍| 506/537 [1:05:55<03:23,  6.56s/it, running training loss: 0.5474]\u001b[A\n",
      "Training:  94%|█████████▍| 507/537 [1:05:55<03:00,  6.02s/it, running training loss: 0.5474]\u001b[A\n",
      "Training:  94%|█████████▍| 507/537 [1:06:01<03:00,  6.02s/it, running training loss: 0.5704]\u001b[A\n",
      "Training:  95%|█████████▍| 508/537 [1:06:01<02:51,  5.93s/it, running training loss: 0.5704]\u001b[A\n",
      "Training:  95%|█████████▍| 508/537 [1:06:07<02:51,  5.93s/it, running training loss: 0.5871]\u001b[A\n",
      "Training:  95%|█████████▍| 509/537 [1:06:07<02:49,  6.07s/it, running training loss: 0.5871]\u001b[A\n",
      "Training:  95%|█████████▍| 509/537 [1:06:12<02:49,  6.07s/it, running training loss: 0.5313]\u001b[A\n",
      "Training:  95%|█████████▍| 510/537 [1:06:12<02:37,  5.84s/it, running training loss: 0.5313]\u001b[A\n",
      "Training:  95%|█████████▍| 510/537 [1:06:19<02:37,  5.84s/it, running training loss: 0.5770]\u001b[A\n",
      "Training:  95%|█████████▌| 511/537 [1:06:19<02:41,  6.22s/it, running training loss: 0.5770]\u001b[A\n",
      "Training:  95%|█████████▌| 511/537 [1:06:26<02:41,  6.22s/it, running training loss: 0.6167]\u001b[A\n",
      "Training:  95%|█████████▌| 512/537 [1:06:26<02:34,  6.19s/it, running training loss: 0.6167]\u001b[A\n",
      "Training:  95%|█████████▌| 512/537 [1:06:36<02:34,  6.19s/it, running training loss: 0.5596]\u001b[A\n",
      "Training:  96%|█████████▌| 513/537 [1:06:36<02:55,  7.32s/it, running training loss: 0.5596]\u001b[A\n",
      "Training:  96%|█████████▌| 513/537 [1:06:41<02:55,  7.32s/it, running training loss: 0.5573]\u001b[A\n",
      "Training:  96%|█████████▌| 514/537 [1:06:41<02:37,  6.87s/it, running training loss: 0.5573]\u001b[A\n",
      "Training:  96%|█████████▌| 514/537 [1:06:48<02:37,  6.87s/it, running training loss: 0.5102]\u001b[A\n",
      "Training:  96%|█████████▌| 515/537 [1:06:48<02:29,  6.79s/it, running training loss: 0.5102]\u001b[A\n",
      "Training:  96%|█████████▌| 515/537 [1:06:53<02:29,  6.79s/it, running training loss: 0.6048]\u001b[A\n",
      "Training:  96%|█████████▌| 516/537 [1:06:53<02:12,  6.31s/it, running training loss: 0.6048]\u001b[A\n",
      "Training:  96%|█████████▌| 516/537 [1:07:00<02:12,  6.31s/it, running training loss: 0.5064]\u001b[A\n",
      "Training:  96%|█████████▋| 517/537 [1:07:00<02:08,  6.43s/it, running training loss: 0.5064]\u001b[A\n",
      "Training:  96%|█████████▋| 517/537 [1:07:06<02:08,  6.43s/it, running training loss: 0.5963]\u001b[A\n",
      "Training:  96%|█████████▋| 518/537 [1:07:06<01:59,  6.27s/it, running training loss: 0.5963]\u001b[A\n",
      "Training:  96%|█████████▋| 518/537 [1:07:12<01:59,  6.27s/it, running training loss: 0.5323]\u001b[A\n",
      "Training:  97%|█████████▋| 519/537 [1:07:12<01:53,  6.28s/it, running training loss: 0.5323]\u001b[A\n",
      "Training:  97%|█████████▋| 519/537 [1:07:22<01:53,  6.28s/it, running training loss: 0.5636]\u001b[A\n",
      "Training:  97%|█████████▋| 520/537 [1:07:22<02:06,  7.42s/it, running training loss: 0.5636]\u001b[A\n",
      "Training:  97%|█████████▋| 520/537 [1:07:30<02:06,  7.42s/it, running training loss: 0.5620]\u001b[A\n",
      "Training:  97%|█████████▋| 521/537 [1:07:30<01:58,  7.42s/it, running training loss: 0.5620]\u001b[A\n",
      "Training:  97%|█████████▋| 521/537 [1:07:35<01:58,  7.42s/it, running training loss: 0.5473]\u001b[A\n",
      "Training:  97%|█████████▋| 522/537 [1:07:35<01:44,  6.96s/it, running training loss: 0.5473]\u001b[A\n",
      "Training:  97%|█████████▋| 522/537 [1:07:50<01:44,  6.96s/it, running training loss: 0.6234]\u001b[A\n",
      "Training:  97%|█████████▋| 523/537 [1:07:50<02:07,  9.14s/it, running training loss: 0.6234]\u001b[A\n",
      "Training:  97%|█████████▋| 523/537 [1:07:57<02:07,  9.14s/it, running training loss: 0.5194]\u001b[A\n",
      "Training:  98%|█████████▊| 524/537 [1:07:57<01:53,  8.70s/it, running training loss: 0.5194]\u001b[A\n",
      "Training:  98%|█████████▊| 524/537 [1:08:02<01:53,  8.70s/it, running training loss: 0.5851]\u001b[A\n",
      "Training:  98%|█████████▊| 525/537 [1:08:02<01:31,  7.63s/it, running training loss: 0.5851]\u001b[A\n",
      "Training:  98%|█████████▊| 525/537 [1:08:10<01:31,  7.63s/it, running training loss: 0.5086]\u001b[A\n",
      "Training:  98%|█████████▊| 526/537 [1:08:10<01:25,  7.73s/it, running training loss: 0.5086]\u001b[A\n",
      "Training:  98%|█████████▊| 526/537 [1:08:18<01:25,  7.73s/it, running training loss: 0.5854]\u001b[A\n",
      "Training:  98%|█████████▊| 527/537 [1:08:18<01:15,  7.55s/it, running training loss: 0.5854]\u001b[A\n",
      "Training:  98%|█████████▊| 527/537 [1:08:24<01:15,  7.55s/it, running training loss: 0.5334]\u001b[A\n",
      "Training:  98%|█████████▊| 528/537 [1:08:24<01:05,  7.29s/it, running training loss: 0.5334]\u001b[A\n",
      "Training:  98%|█████████▊| 528/537 [1:08:37<01:05,  7.29s/it, running training loss: 0.5465]\u001b[A\n",
      "Training:  99%|█████████▊| 529/537 [1:08:37<01:11,  8.94s/it, running training loss: 0.5465]\u001b[A\n",
      "Training:  99%|█████████▊| 529/537 [1:08:45<01:11,  8.94s/it, running training loss: 0.5532]\u001b[A\n",
      "Training:  99%|█████████▊| 530/537 [1:08:45<01:01,  8.78s/it, running training loss: 0.5532]\u001b[A\n",
      "Training:  99%|█████████▊| 530/537 [1:08:55<01:01,  8.78s/it, running training loss: 0.6078]\u001b[A\n",
      "Training:  99%|█████████▉| 531/537 [1:08:55<00:53,  8.97s/it, running training loss: 0.6078]\u001b[A\n",
      "Training:  99%|█████████▉| 531/537 [1:09:01<00:53,  8.97s/it, running training loss: 0.5953]\u001b[A\n",
      "Training:  99%|█████████▉| 532/537 [1:09:01<00:40,  8.05s/it, running training loss: 0.5953]\u001b[A\n",
      "Training:  99%|█████████▉| 532/537 [1:09:07<00:40,  8.05s/it, running training loss: 0.4353]\u001b[A\n",
      "Training:  99%|█████████▉| 533/537 [1:09:07<00:29,  7.37s/it, running training loss: 0.4353]\u001b[A\n",
      "Training:  99%|█████████▉| 533/537 [1:09:11<00:29,  7.37s/it, running training loss: 0.4642]\u001b[A\n",
      "Training:  99%|█████████▉| 534/537 [1:09:11<00:19,  6.63s/it, running training loss: 0.4642]\u001b[A\n",
      "Training:  99%|█████████▉| 534/537 [1:09:20<00:19,  6.63s/it, running training loss: 0.5121]\u001b[A\n",
      "Training: 100%|█████████▉| 535/537 [1:09:20<00:14,  7.13s/it, running training loss: 0.5121]\u001b[A\n",
      "Training: 100%|█████████▉| 535/537 [1:09:26<00:14,  7.13s/it, running training loss: 0.6283]\u001b[A\n",
      "Training: 100%|█████████▉| 536/537 [1:09:26<00:07,  7.00s/it, running training loss: 0.6283]\u001b[A\n",
      "Training: 100%|█████████▉| 536/537 [1:09:33<00:07,  7.00s/it, running training loss: 0.4232]\u001b[A\n",
      "Training: 100%|██████████| 537/537 [1:09:34<00:00,  7.77s/it, running training loss: 0.4232]\u001b[A\n",
      "100%|██████████| 1/1 [1:09:34<00:00, 4174.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(21128, 768, padding_idx=1)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (1): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (2): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (3): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (4): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (5): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (6): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (7): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (8): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (9): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (10): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (11): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       " ), './checkpoint-400-0.704819')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(config, train_dataloader, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.8.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
