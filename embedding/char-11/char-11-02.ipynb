{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 使用Embedding提升神经网络性能\n",
    "\t接下来我们构建一个神经网络，根据输入数据的格式不同，一种只对分类特征进行one-hot编码转换，另一种对分类特征使用Embedding处理，然后比较两种方式的模型性能。\n",
    "### 11.2.1 基于one-hot编码的模型\n",
    "1）把训练数据集进行one-hot编码转换，并对原数据进行划分、采样等操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using one-hot encoding as input\n",
      "Number of samples used for training: 200000\n"
     ]
    }
   ],
   "source": [
    "#对特征转换为one-hot编码\n",
    "one_hot_as_input=True\n",
    "if one_hot_as_input:\n",
    "    print(\"Using one-hot encoding as input\")\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X)\n",
    "\n",
    "\n",
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]\n",
    "\n",
    "\n",
    "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def evaluate(self, X_val, y_val):\n",
    "        assert(min(y_val) > 0)\n",
    "        guessed_sales = self.guess(X_val)\n",
    "        relative_err = numpy.absolute((y_val - guessed_sales) / y_val)\n",
    "        result = numpy.sum(relative_err) / len(y_val)\n",
    "        return result\n",
    "\n",
    "\n",
    "class NN(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.epochs = 10\n",
    "        self.checkpointer = ModelCheckpoint(filepath=\"best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
    "        self.max_log_y = max(numpy.max(numpy.log(y_train)), numpy.max(numpy.log(y_val)))\n",
    "        self.__build_keras_model()\n",
    "        self.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    def __build_keras_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(1000, kernel_initializer=\"uniform\", input_dim=1183))\n",
    "        #self.model.add(Dense(1000, kernel_initializer=\"uniform\", input_dim=8))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(500, kernel_initializer=\"uniform\"))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "    def _val_for_fit(self, val):\n",
    "        val = numpy.log(val) / self.max_log_y\n",
    "        return val\n",
    "\n",
    "    def _val_for_pred(self, val):\n",
    "        return numpy.exp(val * self.max_log_y)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.model.fit(X_train, self._val_for_fit(y_train),\n",
    "                       validation_data=(X_val, self._val_for_fit(y_val)),\n",
    "                       epochs=self.epochs, batch_size=128,\n",
    "                       # callbacks=[self.checkpointer],\n",
    "                       )\n",
    "        # self.model.load_weights('best_model_weights.hdf5')\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        result = self.model.predict(features).flatten()\n",
    "        return self._val_for_pred(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3）训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NN...\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 100s 500us/sample - loss: 0.0123 - val_loss: 0.0108\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 100s 502us/sample - loss: 0.0081 - val_loss: 0.0105\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 98s 490us/sample - loss: 0.0071 - val_loss: 0.0100\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 100s 501us/sample - loss: 0.0064 - val_loss: 0.0103\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 102s 510us/sample - loss: 0.0060 - val_loss: 0.0097\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 97s 486us/sample - loss: 0.0055 - val_loss: 0.0100\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 102s 512us/sample - loss: 0.0051 - val_loss: 0.0102\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 106s 532us/sample - loss: 0.0048 - val_loss: 0.0099\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 215s 1ms/sample - loss: 0.0045 - val_loss: 0.0101\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 159s 793us/sample - loss: 0.0042 - val_loss: 0.0103\n",
      "Result on validation data:  0.1111847571553691\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 119s 593us/sample - loss: 0.0123 - val_loss: 0.0109\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 100s 500us/sample - loss: 0.0081 - val_loss: 0.0106\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 100s 499us/sample - loss: 0.0072 - val_loss: 0.0098\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 102s 508us/sample - loss: 0.0066 - val_loss: 0.0101\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 187s 937us/sample - loss: 0.0060 - val_loss: 0.0100\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 184s 919us/sample - loss: 0.0056 - val_loss: 0.0099\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 85s 426us/sample - loss: 0.0052 - val_loss: 0.0097\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 83s 417us/sample - loss: 0.0048 - val_loss: 0.0098\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 87s 437us/sample - loss: 0.0045 - val_loss: 0.0098\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 86s 430us/sample - loss: 0.0043 - val_loss: 0.0099\n",
      "Result on validation data:  0.10668033416072932\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "print(\"Fitting NN...\")\n",
    "for i in range(2):\n",
    "     models.append(NN(X_train, y_train, X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4）评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate combined models...\n",
      "Training error...\n",
      "0.033962882939043926\n",
      "Validation error...\n",
      "0.10440801234057484\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X_val, y_val)\n",
    "print(r_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.2 基于Embedding的模型\n",
    "1.生成用于含Embedding层神经网络数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重新获取训练数据\n",
    "f = open('feature_train_data.pickle', 'rb')\n",
    "(X, y) = pickle.load(f)\n",
    "\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.对数据进行划分和采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used for training: 200000\n"
     ]
    }
   ],
   "source": [
    "#划分数据\n",
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]\n",
    "\n",
    "\n",
    "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.构建含Embedding层的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_with_EntityEmbedding(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.epochs = 10\n",
    "        self.checkpointer = ModelCheckpoint(filepath=\"best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
    "        self.max_log_y = max(numpy.max(numpy.log(y_train)), numpy.max(numpy.log(y_val)))\n",
    "        self.__build_keras_model()\n",
    "        self.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    def preprocessing(self, X):\n",
    "        X_list = split_features(X)\n",
    "        return X_list\n",
    "\n",
    "    def __build_keras_model(self):\n",
    "        input_store = Input(shape=(1,))\n",
    "        output_store = Embedding(1115, 10, name='store_embedding')(input_store)\n",
    "        output_store = Reshape(target_shape=(10,))(output_store)\n",
    "\n",
    "        input_dow = Input(shape=(1,))\n",
    "        output_dow = Embedding(7, 6, name='dow_embedding')(input_dow)\n",
    "        output_dow = Reshape(target_shape=(6,))(output_dow)\n",
    "\n",
    "        input_promo = Input(shape=(1,))\n",
    "        output_promo = Dense(1)(input_promo)\n",
    "\n",
    "        input_year = Input(shape=(1,))\n",
    "        output_year = Embedding(3, 2, name='year_embedding')(input_year)\n",
    "        output_year = Reshape(target_shape=(2,))(output_year)\n",
    "\n",
    "        input_month = Input(shape=(1,))\n",
    "        output_month = Embedding(12, 6, name='month_embedding')(input_month)\n",
    "        output_month = Reshape(target_shape=(6,))(output_month)\n",
    "\n",
    "        input_day = Input(shape=(1,))\n",
    "        output_day = Embedding(31, 10, name='day_embedding')(input_day)\n",
    "        output_day = Reshape(target_shape=(10,))(output_day)\n",
    "\n",
    "        input_germanstate = Input(shape=(1,))\n",
    "        output_germanstate = Embedding(12, 6, name='state_embedding')(input_germanstate)\n",
    "        output_germanstate = Reshape(target_shape=(6,))(output_germanstate)\n",
    "\n",
    "        input_model = [input_store, input_dow, input_promo,\n",
    "                       input_year, input_month, input_day, input_germanstate]\n",
    "\n",
    "        output_embeddings = [output_store, output_dow, output_promo,\n",
    "                             output_year, output_month, output_day, output_germanstate]\n",
    "\n",
    "        output_model = Concatenate()(output_embeddings)\n",
    "        output_model = Dense(1000, kernel_initializer=\"uniform\")(output_model)\n",
    "        output_model = Activation('relu')(output_model)\n",
    "        output_model = Dense(500, kernel_initializer=\"uniform\")(output_model)\n",
    "        output_model = Activation('relu')(output_model)\n",
    "        output_model = Dense(1)(output_model)\n",
    "        output_model = Activation('sigmoid')(output_model)\n",
    "\n",
    "        self.model = KerasModel(inputs=input_model, outputs=output_model)\n",
    "\n",
    "        self.model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "    def _val_for_fit(self, val):\n",
    "        val = numpy.log(val) / self.max_log_y\n",
    "        return val\n",
    "\n",
    "    def _val_for_pred(self, val):\n",
    "        return numpy.exp(val * self.max_log_y)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.model.fit(self.preprocessing(X_train), self._val_for_fit(y_train),\n",
    "                       validation_data=(self.preprocessing(X_val), self._val_for_fit(y_val)),\n",
    "                       epochs=self.epochs, batch_size=128,\n",
    "                       # callbacks=[self.checkpointer],\n",
    "                       )\n",
    "        # self.model.load_weights('best_model_weights.hdf5')\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        features = self.preprocessing(features)\n",
    "        result = self.model.predict(features).flatten()\n",
    "        return self._val_for_pred(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NN_with_EntityEmbedding...\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 0.0140 - val_loss: 0.0126\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 0.0092 - val_loss: 0.0102\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 56s 280us/sample - loss: 0.0083 - val_loss: 0.0099\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 59s 297us/sample - loss: 0.0079 - val_loss: 0.0102\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 59s 297us/sample - loss: 0.0075 - val_loss: 0.0094\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 58s 289us/sample - loss: 0.0074 - val_loss: 0.0105\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 61s 304us/sample - loss: 0.0072 - val_loss: 0.0093\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 62s 310us/sample - loss: 0.0071 - val_loss: 0.0093\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 58s 291us/sample - loss: 0.0069 - val_loss: 0.0091\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 59s 297us/sample - loss: 0.0068 - val_loss: 0.0094\n",
      "Result on validation data:  0.10162505226807965\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "print(\"Fitting NN_with_EntityEmbedding...\")\n",
    "for i in range(1):\n",
    "    models.append(NN_with_EntityEmbedding(X_train, y_train, X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate combined models...\n",
      "Training error...\n",
      "0.07067204018839432\n",
      "Validation error...\n",
      "0.10162505226807965\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X_val, y_val)\n",
    "print(r_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.保存生成的Embedding数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings = True\n",
    "if save_embeddings:\n",
    "    model = models[0].model\n",
    "    store_embedding = model.get_layer('store_embedding').get_weights()[0]\n",
    "    dow_embedding = model.get_layer('dow_embedding').get_weights()[0]\n",
    "    year_embedding = model.get_layer('year_embedding').get_weights()[0]\n",
    "    month_embedding = model.get_layer('month_embedding').get_weights()[0]\n",
    "    day_embedding = model.get_layer('day_embedding').get_weights()[0]\n",
    "    german_states_embedding = model.get_layer('state_embedding').get_weights()[0]\n",
    "    with open(saved_embeddings_fname, 'wb') as f:\n",
    "        pickle.dump([store_embedding, dow_embedding, year_embedding,\n",
    "                     month_embedding, day_embedding, german_states_embedding], f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.定义获取Embedding数据的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从训练结果读取各特征的embedding向量，并用这些向量作为输入值\n",
    "def embed_features(X, saved_embeddings_fname):\n",
    "    # f_embeddings = open(\"embeddings_shuffled.pickle\", \"rb\")\n",
    "    f_embeddings = open(saved_embeddings_fname, \"rb\")\n",
    "    embeddings = pickle.load(f_embeddings) \n",
    "    \n",
    "    #因store_open,promo这两列，至多只有两个值，没有进行embedding，故需排除在外\n",
    "    index_embedding_mapping = {1: 0, 2: 1, 4: 2, 5: 3, 6: 4, 7: 5}\n",
    "    X_embedded = []\n",
    "\n",
    "    (num_records, num_features) = X.shape\n",
    "    for record in X:\n",
    "        embedded_features = []\n",
    "        for i, feat in enumerate(record):\n",
    "            feat = int(feat)\n",
    "            if i not in index_embedding_mapping.keys():\n",
    "                embedded_features += [feat]\n",
    "            else:\n",
    "                embedding_index = index_embedding_mapping[i]\n",
    "                embedded_features += embeddings[embedding_index][feat].tolist()\n",
    "\n",
    "        X_embedded.append(embedded_features)\n",
    "\n",
    "    return numpy.array(X_embedded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3构建XGBoost模型\n",
    "1）生成培训数据。这里对特征只进行数值化处理，不做独热编码转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重新获取训练数据\n",
    "f = open('feature_train_data.pickle', 'rb')\n",
    "(X, y) = pickle.load(f)\n",
    "\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）划分数据并进行数据采样  \n",
    "独热编码这种操作通常适用于利用向量空间度量的算法，无序型分类变量的独热编码可以避免向量距离计算导致的偏序性。而对于树模型，通常不用独热编码，对分类变量进行标签化就行,这里我们也不对分类特征进行独热编码转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used for training: 200000\n"
     ]
    }
   ],
   "source": [
    "#划分数据\n",
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]\n",
    "\n",
    "\n",
    "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3）构建XGBoost模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        dtrain = xgb.DMatrix(X_train, label=numpy.log(y_train))\n",
    "        evallist = [(dtrain, 'train')]\n",
    "        param = {'nthread': -1,\n",
    "                 'max_depth': 7,\n",
    "                 'eta': 0.02,\n",
    "                 'silent': 1,\n",
    "                 'objective': 'reg:linear',\n",
    "                 'colsample_bytree': 0.7,\n",
    "                 'subsample': 0.7}\n",
    "        num_round = 1000\n",
    "        self.bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, feature):\n",
    "        dtest = xgb.DMatrix(feature)\n",
    "        return numpy.exp(self.bst.predict(dtest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4）训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting XGBoost...\n",
      "[0]\ttrain-rmse:8.10002\n",
      "[1]\ttrain-rmse:7.93845\n",
      "[2]\ttrain-rmse:7.78014\n",
      "[3]\ttrain-rmse:7.625\n",
      "[4]\ttrain-rmse:7.47285\n",
      "[5]\ttrain-rmse:7.32385\n",
      "[6]\ttrain-rmse:7.17773\n",
      "[7]\ttrain-rmse:7.03462\n",
      "[8]\ttrain-rmse:6.89434\n",
      "[9]\ttrain-rmse:6.75693\n",
      "[10]\ttrain-rmse:6.62224\n",
      "[11]\ttrain-rmse:6.49033\n",
      "[12]\ttrain-rmse:6.36091\n",
      "[13]\ttrain-rmse:6.23419\n",
      "[14]\ttrain-rmse:6.11001\n",
      "[15]\ttrain-rmse:5.98827\n",
      "[16]\ttrain-rmse:5.86906\n",
      "[17]\ttrain-rmse:5.7522\n",
      "[18]\ttrain-rmse:5.63768\n",
      "[19]\ttrain-rmse:5.52545\n",
      "[20]\ttrain-rmse:5.4155\n",
      "[21]\ttrain-rmse:5.3077\n",
      "[22]\ttrain-rmse:5.20213\n",
      "[23]\ttrain-rmse:5.09864\n",
      "[24]\ttrain-rmse:4.99722\n",
      "[25]\ttrain-rmse:4.89787\n",
      "[26]\ttrain-rmse:4.80051\n",
      "[27]\ttrain-rmse:4.70508\n",
      "[28]\ttrain-rmse:4.61165\n",
      "[29]\ttrain-rmse:4.52001\n",
      "[30]\ttrain-rmse:4.43024\n",
      "[31]\ttrain-rmse:4.34227\n",
      "[32]\ttrain-rmse:4.25609\n",
      "[33]\ttrain-rmse:4.17168\n",
      "[34]\ttrain-rmse:4.08889\n",
      "[35]\ttrain-rmse:4.00782\n",
      "[36]\ttrain-rmse:3.92835\n",
      "[37]\ttrain-rmse:3.85054\n",
      "[38]\ttrain-rmse:3.77426\n",
      "[39]\ttrain-rmse:3.69948\n",
      "[40]\ttrain-rmse:3.62625\n",
      "[41]\ttrain-rmse:3.55446\n",
      "[42]\ttrain-rmse:3.48414\n",
      "[43]\ttrain-rmse:3.41526\n",
      "[44]\ttrain-rmse:3.34778\n",
      "[45]\ttrain-rmse:3.28161\n",
      "[46]\ttrain-rmse:3.21683\n",
      "[47]\ttrain-rmse:3.15335\n",
      "[48]\ttrain-rmse:3.09115\n",
      "[49]\ttrain-rmse:3.0302\n",
      "[50]\ttrain-rmse:2.9705\n",
      "[51]\ttrain-rmse:2.91199\n",
      "[52]\ttrain-rmse:2.85464\n",
      "[53]\ttrain-rmse:2.79848\n",
      "[54]\ttrain-rmse:2.74343\n",
      "[55]\ttrain-rmse:2.68954\n",
      "[56]\ttrain-rmse:2.63674\n",
      "[57]\ttrain-rmse:2.58505\n",
      "[58]\ttrain-rmse:2.53433\n",
      "[59]\ttrain-rmse:2.48467\n",
      "[60]\ttrain-rmse:2.43605\n",
      "[61]\ttrain-rmse:2.38843\n",
      "[62]\ttrain-rmse:2.34178\n",
      "[63]\ttrain-rmse:2.29607\n",
      "[64]\ttrain-rmse:2.25129\n",
      "[65]\ttrain-rmse:2.20742\n",
      "[66]\ttrain-rmse:2.16443\n",
      "[67]\ttrain-rmse:2.12235\n",
      "[68]\ttrain-rmse:2.08112\n",
      "[69]\ttrain-rmse:2.04074\n",
      "[70]\ttrain-rmse:2.00121\n",
      "[71]\ttrain-rmse:1.9625\n",
      "[72]\ttrain-rmse:1.92459\n",
      "[73]\ttrain-rmse:1.88745\n",
      "[74]\ttrain-rmse:1.85107\n",
      "[75]\ttrain-rmse:1.81542\n",
      "[76]\ttrain-rmse:1.78052\n",
      "[77]\ttrain-rmse:1.74632\n",
      "[78]\ttrain-rmse:1.71288\n",
      "[79]\ttrain-rmse:1.68005\n",
      "[80]\ttrain-rmse:1.64797\n",
      "[81]\ttrain-rmse:1.61654\n",
      "[82]\ttrain-rmse:1.5858\n",
      "[83]\ttrain-rmse:1.55572\n",
      "[84]\ttrain-rmse:1.52618\n",
      "[85]\ttrain-rmse:1.4973\n",
      "[86]\ttrain-rmse:1.46906\n",
      "[87]\ttrain-rmse:1.4414\n",
      "[88]\ttrain-rmse:1.41434\n",
      "[89]\ttrain-rmse:1.38783\n",
      "[90]\ttrain-rmse:1.36189\n",
      "[91]\ttrain-rmse:1.33648\n",
      "[92]\ttrain-rmse:1.31156\n",
      "[93]\ttrain-rmse:1.2872\n",
      "[94]\ttrain-rmse:1.26337\n",
      "[95]\ttrain-rmse:1.24008\n",
      "[96]\ttrain-rmse:1.21728\n",
      "[97]\ttrain-rmse:1.19497\n",
      "[98]\ttrain-rmse:1.17301\n",
      "[99]\ttrain-rmse:1.15166\n",
      "[100]\ttrain-rmse:1.13072\n",
      "[101]\ttrain-rmse:1.1103\n",
      "[102]\ttrain-rmse:1.09028\n",
      "[103]\ttrain-rmse:1.07067\n",
      "[104]\ttrain-rmse:1.05155\n",
      "[105]\ttrain-rmse:1.03275\n",
      "[106]\ttrain-rmse:1.01446\n",
      "[107]\ttrain-rmse:0.996455\n",
      "[108]\ttrain-rmse:0.978975\n",
      "[109]\ttrain-rmse:0.961836\n",
      "[110]\ttrain-rmse:0.945051\n",
      "[111]\ttrain-rmse:0.928567\n",
      "[112]\ttrain-rmse:0.912604\n",
      "[113]\ttrain-rmse:0.896991\n",
      "[114]\ttrain-rmse:0.881725\n",
      "[115]\ttrain-rmse:0.866839\n",
      "[116]\ttrain-rmse:0.85226\n",
      "[117]\ttrain-rmse:0.838076\n",
      "[118]\ttrain-rmse:0.82397\n",
      "[119]\ttrain-rmse:0.810393\n",
      "[120]\ttrain-rmse:0.797158\n",
      "[121]\ttrain-rmse:0.784182\n",
      "[122]\ttrain-rmse:0.771445\n",
      "[123]\ttrain-rmse:0.759089\n",
      "[124]\ttrain-rmse:0.747044\n",
      "[125]\ttrain-rmse:0.73507\n",
      "[126]\ttrain-rmse:0.723624\n",
      "[127]\ttrain-rmse:0.712352\n",
      "[128]\ttrain-rmse:0.701433\n",
      "[129]\ttrain-rmse:0.690752\n",
      "[130]\ttrain-rmse:0.680355\n",
      "[131]\ttrain-rmse:0.670041\n",
      "[132]\ttrain-rmse:0.660122\n",
      "[133]\ttrain-rmse:0.650436\n",
      "[134]\ttrain-rmse:0.641037\n",
      "[135]\ttrain-rmse:0.63177\n",
      "[136]\ttrain-rmse:0.622847\n",
      "[137]\ttrain-rmse:0.614152\n",
      "[138]\ttrain-rmse:0.605554\n",
      "[139]\ttrain-rmse:0.597279\n",
      "[140]\ttrain-rmse:0.589219\n",
      "[141]\ttrain-rmse:0.581384\n",
      "[142]\ttrain-rmse:0.573755\n",
      "[143]\ttrain-rmse:0.566342\n",
      "[144]\ttrain-rmse:0.558831\n",
      "[145]\ttrain-rmse:0.551795\n",
      "[146]\ttrain-rmse:0.544956\n",
      "[147]\ttrain-rmse:0.538244\n",
      "[148]\ttrain-rmse:0.531767\n",
      "[149]\ttrain-rmse:0.525155\n",
      "[150]\ttrain-rmse:0.519013\n",
      "[151]\ttrain-rmse:0.513061\n",
      "[152]\ttrain-rmse:0.507051\n",
      "[153]\ttrain-rmse:0.501379\n",
      "[154]\ttrain-rmse:0.495757\n",
      "[155]\ttrain-rmse:0.490141\n",
      "[156]\ttrain-rmse:0.484957\n",
      "[157]\ttrain-rmse:0.479941\n",
      "[158]\ttrain-rmse:0.475046\n",
      "[159]\ttrain-rmse:0.47033\n",
      "[160]\ttrain-rmse:0.465459\n",
      "[161]\ttrain-rmse:0.460669\n",
      "[162]\ttrain-rmse:0.455934\n",
      "[163]\ttrain-rmse:0.451713\n",
      "[164]\ttrain-rmse:0.447603\n",
      "[165]\ttrain-rmse:0.443284\n",
      "[166]\ttrain-rmse:0.439452\n",
      "[167]\ttrain-rmse:0.435705\n",
      "[168]\ttrain-rmse:0.431917\n",
      "[169]\ttrain-rmse:0.428422\n",
      "[170]\ttrain-rmse:0.424706\n",
      "[171]\ttrain-rmse:0.421448\n",
      "[172]\ttrain-rmse:0.418131\n",
      "[173]\ttrain-rmse:0.415038\n",
      "[174]\ttrain-rmse:0.412046\n",
      "[175]\ttrain-rmse:0.409132\n",
      "[176]\ttrain-rmse:0.40618\n",
      "[177]\ttrain-rmse:0.403415\n",
      "[178]\ttrain-rmse:0.400448\n",
      "[179]\ttrain-rmse:0.397917\n",
      "[180]\ttrain-rmse:0.395187\n",
      "[181]\ttrain-rmse:0.392789\n",
      "[182]\ttrain-rmse:0.390486\n",
      "[183]\ttrain-rmse:0.388149\n",
      "[184]\ttrain-rmse:0.385957\n",
      "[185]\ttrain-rmse:0.383825\n",
      "[186]\ttrain-rmse:0.381823\n",
      "[187]\ttrain-rmse:0.379868\n",
      "[188]\ttrain-rmse:0.378\n",
      "[189]\ttrain-rmse:0.376107\n",
      "[190]\ttrain-rmse:0.374264\n",
      "[191]\ttrain-rmse:0.372538\n",
      "[192]\ttrain-rmse:0.3709\n",
      "[193]\ttrain-rmse:0.369261\n",
      "[194]\ttrain-rmse:0.367752\n",
      "[195]\ttrain-rmse:0.366281\n",
      "[196]\ttrain-rmse:0.36479\n",
      "[197]\ttrain-rmse:0.363273\n",
      "[198]\ttrain-rmse:0.361904\n",
      "[199]\ttrain-rmse:0.360555\n",
      "[200]\ttrain-rmse:0.358905\n",
      "[201]\ttrain-rmse:0.357686\n",
      "[202]\ttrain-rmse:0.356536\n",
      "[203]\ttrain-rmse:0.355353\n",
      "[204]\ttrain-rmse:0.354289\n",
      "[205]\ttrain-rmse:0.3528\n",
      "[206]\ttrain-rmse:0.351653\n",
      "[207]\ttrain-rmse:0.350648\n",
      "[208]\ttrain-rmse:0.349405\n",
      "[209]\ttrain-rmse:0.348041\n",
      "[210]\ttrain-rmse:0.347158\n",
      "[211]\ttrain-rmse:0.346027\n",
      "[212]\ttrain-rmse:0.344957\n",
      "[213]\ttrain-rmse:0.344168\n",
      "[214]\ttrain-rmse:0.343361\n",
      "[215]\ttrain-rmse:0.342622\n",
      "[216]\ttrain-rmse:0.341848\n",
      "[217]\ttrain-rmse:0.341114\n",
      "[218]\ttrain-rmse:0.340127\n",
      "[219]\ttrain-rmse:0.339375\n",
      "[220]\ttrain-rmse:0.338746\n",
      "[221]\ttrain-rmse:0.338085\n",
      "[222]\ttrain-rmse:0.337502\n",
      "[223]\ttrain-rmse:0.336691\n",
      "[224]\ttrain-rmse:0.336117\n",
      "[225]\ttrain-rmse:0.335583\n",
      "[226]\ttrain-rmse:0.335095\n",
      "[227]\ttrain-rmse:0.334363\n",
      "[228]\ttrain-rmse:0.33385\n",
      "[229]\ttrain-rmse:0.333091\n",
      "[230]\ttrain-rmse:0.332571\n",
      "[231]\ttrain-rmse:0.332152\n",
      "[232]\ttrain-rmse:0.331624\n",
      "[233]\ttrain-rmse:0.330932\n",
      "[234]\ttrain-rmse:0.330516\n",
      "[235]\ttrain-rmse:0.33015\n",
      "[236]\ttrain-rmse:0.329518\n",
      "[237]\ttrain-rmse:0.32919\n",
      "[238]\ttrain-rmse:0.328857\n",
      "[239]\ttrain-rmse:0.328517\n",
      "[240]\ttrain-rmse:0.32818\n",
      "[241]\ttrain-rmse:0.327862\n",
      "[242]\ttrain-rmse:0.327564\n",
      "[243]\ttrain-rmse:0.327223\n",
      "[244]\ttrain-rmse:0.326517\n",
      "[245]\ttrain-rmse:0.32621\n",
      "[246]\ttrain-rmse:0.32597\n",
      "[247]\ttrain-rmse:0.325429\n",
      "[248]\ttrain-rmse:0.325202\n",
      "[249]\ttrain-rmse:0.324638\n",
      "[250]\ttrain-rmse:0.324429\n",
      "[251]\ttrain-rmse:0.324123\n",
      "[252]\ttrain-rmse:0.323834\n",
      "[253]\ttrain-rmse:0.323601\n",
      "[254]\ttrain-rmse:0.323389\n",
      "[255]\ttrain-rmse:0.32318\n",
      "[256]\ttrain-rmse:0.322706\n",
      "[257]\ttrain-rmse:0.322248\n",
      "[258]\ttrain-rmse:0.322037\n",
      "[259]\ttrain-rmse:0.321276\n",
      "[260]\ttrain-rmse:0.32106\n",
      "[261]\ttrain-rmse:0.320903\n",
      "[262]\ttrain-rmse:0.320705\n",
      "[263]\ttrain-rmse:0.320514\n",
      "[264]\ttrain-rmse:0.320006\n",
      "[265]\ttrain-rmse:0.319767\n",
      "[266]\ttrain-rmse:0.319628\n",
      "[267]\ttrain-rmse:0.319406\n",
      "[268]\ttrain-rmse:0.319209\n",
      "[269]\ttrain-rmse:0.319039\n",
      "[270]\ttrain-rmse:0.318266\n",
      "[271]\ttrain-rmse:0.317704\n",
      "[272]\ttrain-rmse:0.317571\n",
      "[273]\ttrain-rmse:0.317387\n",
      "[274]\ttrain-rmse:0.317271\n",
      "[275]\ttrain-rmse:0.317082\n",
      "[276]\ttrain-rmse:0.316951\n",
      "[277]\ttrain-rmse:0.316508\n",
      "[278]\ttrain-rmse:0.316339\n",
      "[279]\ttrain-rmse:0.316163\n",
      "[280]\ttrain-rmse:0.316058\n",
      "[281]\ttrain-rmse:0.315901\n",
      "[282]\ttrain-rmse:0.315533\n",
      "[283]\ttrain-rmse:0.315179\n",
      "[284]\ttrain-rmse:0.314822\n",
      "[285]\ttrain-rmse:0.314712\n",
      "[286]\ttrain-rmse:0.314216\n",
      "[287]\ttrain-rmse:0.313597\n",
      "[288]\ttrain-rmse:0.313031\n",
      "[289]\ttrain-rmse:0.312549\n",
      "[290]\ttrain-rmse:0.312229\n",
      "[291]\ttrain-rmse:0.312114\n",
      "[292]\ttrain-rmse:0.31191\n",
      "[293]\ttrain-rmse:0.311841\n",
      "[294]\ttrain-rmse:0.31171\n",
      "[295]\ttrain-rmse:0.311638\n",
      "[296]\ttrain-rmse:0.311502\n",
      "[297]\ttrain-rmse:0.311183\n",
      "[298]\ttrain-rmse:0.311118\n",
      "[299]\ttrain-rmse:0.310932\n",
      "[300]\ttrain-rmse:0.310862\n",
      "[301]\ttrain-rmse:0.310448\n",
      "[302]\ttrain-rmse:0.310039\n",
      "[303]\ttrain-rmse:0.309939\n",
      "[304]\ttrain-rmse:0.309846\n",
      "[305]\ttrain-rmse:0.309762\n",
      "[306]\ttrain-rmse:0.309699\n",
      "[307]\ttrain-rmse:0.309597\n",
      "[308]\ttrain-rmse:0.309528\n",
      "[309]\ttrain-rmse:0.309445\n",
      "[310]\ttrain-rmse:0.309356\n",
      "[311]\ttrain-rmse:0.309298\n",
      "[312]\ttrain-rmse:0.309026\n",
      "[313]\ttrain-rmse:0.308929\n",
      "[314]\ttrain-rmse:0.308868\n",
      "[315]\ttrain-rmse:0.308577\n",
      "[316]\ttrain-rmse:0.308449\n",
      "[317]\ttrain-rmse:0.308368\n",
      "[318]\ttrain-rmse:0.307877\n",
      "[319]\ttrain-rmse:0.307454\n",
      "[320]\ttrain-rmse:0.307362\n",
      "[321]\ttrain-rmse:0.307282\n",
      "[322]\ttrain-rmse:0.307008\n",
      "[323]\ttrain-rmse:0.306732\n",
      "[324]\ttrain-rmse:0.306342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[325]\ttrain-rmse:0.306073\n",
      "[326]\ttrain-rmse:0.305822\n",
      "[327]\ttrain-rmse:0.30575\n",
      "[328]\ttrain-rmse:0.305378\n",
      "[329]\ttrain-rmse:0.30495\n",
      "[330]\ttrain-rmse:0.304898\n",
      "[331]\ttrain-rmse:0.304549\n",
      "[332]\ttrain-rmse:0.304498\n",
      "[333]\ttrain-rmse:0.304378\n",
      "[334]\ttrain-rmse:0.30427\n",
      "[335]\ttrain-rmse:0.304222\n",
      "[336]\ttrain-rmse:0.304182\n",
      "[337]\ttrain-rmse:0.304126\n",
      "[338]\ttrain-rmse:0.303731\n",
      "[339]\ttrain-rmse:0.303687\n",
      "[340]\ttrain-rmse:0.303607\n",
      "[341]\ttrain-rmse:0.303539\n",
      "[342]\ttrain-rmse:0.303177\n",
      "[343]\ttrain-rmse:0.303134\n",
      "[344]\ttrain-rmse:0.303088\n",
      "[345]\ttrain-rmse:0.302846\n",
      "[346]\ttrain-rmse:0.302798\n",
      "[347]\ttrain-rmse:0.302373\n",
      "[348]\ttrain-rmse:0.30227\n",
      "[349]\ttrain-rmse:0.302185\n",
      "[350]\ttrain-rmse:0.301942\n",
      "[351]\ttrain-rmse:0.301836\n",
      "[352]\ttrain-rmse:0.301615\n",
      "[353]\ttrain-rmse:0.301575\n",
      "[354]\ttrain-rmse:0.301478\n",
      "[355]\ttrain-rmse:0.301108\n",
      "[356]\ttrain-rmse:0.300886\n",
      "[357]\ttrain-rmse:0.300849\n",
      "[358]\ttrain-rmse:0.300808\n",
      "[359]\ttrain-rmse:0.300754\n",
      "[360]\ttrain-rmse:0.300684\n",
      "[361]\ttrain-rmse:0.300636\n",
      "[362]\ttrain-rmse:0.300068\n",
      "[363]\ttrain-rmse:0.300022\n",
      "[364]\ttrain-rmse:0.299983\n",
      "[365]\ttrain-rmse:0.299644\n",
      "[366]\ttrain-rmse:0.299415\n",
      "[367]\ttrain-rmse:0.299345\n",
      "[368]\ttrain-rmse:0.299283\n",
      "[369]\ttrain-rmse:0.298963\n",
      "[370]\ttrain-rmse:0.298931\n",
      "[371]\ttrain-rmse:0.29889\n",
      "[372]\ttrain-rmse:0.298845\n",
      "[373]\ttrain-rmse:0.298556\n",
      "[374]\ttrain-rmse:0.298351\n",
      "[375]\ttrain-rmse:0.298316\n",
      "[376]\ttrain-rmse:0.29795\n",
      "[377]\ttrain-rmse:0.297591\n",
      "[378]\ttrain-rmse:0.297372\n",
      "[379]\ttrain-rmse:0.297335\n",
      "[380]\ttrain-rmse:0.297223\n",
      "[381]\ttrain-rmse:0.297184\n",
      "[382]\ttrain-rmse:0.296844\n",
      "[383]\ttrain-rmse:0.296495\n",
      "[384]\ttrain-rmse:0.296299\n",
      "[385]\ttrain-rmse:0.296257\n",
      "[386]\ttrain-rmse:0.296222\n",
      "[387]\ttrain-rmse:0.295817\n",
      "[388]\ttrain-rmse:0.295496\n",
      "[389]\ttrain-rmse:0.295424\n",
      "[390]\ttrain-rmse:0.295149\n",
      "[391]\ttrain-rmse:0.294949\n",
      "[392]\ttrain-rmse:0.294913\n",
      "[393]\ttrain-rmse:0.294707\n",
      "[394]\ttrain-rmse:0.294652\n",
      "[395]\ttrain-rmse:0.29429\n",
      "[396]\ttrain-rmse:0.294253\n",
      "[397]\ttrain-rmse:0.293922\n",
      "[398]\ttrain-rmse:0.293535\n",
      "[399]\ttrain-rmse:0.293468\n",
      "[400]\ttrain-rmse:0.293436\n",
      "[401]\ttrain-rmse:0.293348\n",
      "[402]\ttrain-rmse:0.293118\n",
      "[403]\ttrain-rmse:0.292793\n",
      "[404]\ttrain-rmse:0.292758\n",
      "[405]\ttrain-rmse:0.292713\n",
      "[406]\ttrain-rmse:0.292682\n",
      "[407]\ttrain-rmse:0.292649\n",
      "[408]\ttrain-rmse:0.292389\n",
      "[409]\ttrain-rmse:0.292031\n",
      "[410]\ttrain-rmse:0.291462\n",
      "[411]\ttrain-rmse:0.291424\n",
      "[412]\ttrain-rmse:0.291097\n",
      "[413]\ttrain-rmse:0.290815\n",
      "[414]\ttrain-rmse:0.290772\n",
      "[415]\ttrain-rmse:0.29072\n",
      "[416]\ttrain-rmse:0.290689\n",
      "[417]\ttrain-rmse:0.290657\n",
      "[418]\ttrain-rmse:0.290594\n",
      "[419]\ttrain-rmse:0.290405\n",
      "[420]\ttrain-rmse:0.290205\n",
      "[421]\ttrain-rmse:0.290162\n",
      "[422]\ttrain-rmse:0.290113\n",
      "[423]\ttrain-rmse:0.289934\n",
      "[424]\ttrain-rmse:0.289609\n",
      "[425]\ttrain-rmse:0.289578\n",
      "[426]\ttrain-rmse:0.289527\n",
      "[427]\ttrain-rmse:0.289501\n",
      "[428]\ttrain-rmse:0.289328\n",
      "[429]\ttrain-rmse:0.288959\n",
      "[430]\ttrain-rmse:0.288929\n",
      "[431]\ttrain-rmse:0.288565\n",
      "[432]\ttrain-rmse:0.288537\n",
      "[433]\ttrain-rmse:0.288507\n",
      "[434]\ttrain-rmse:0.288469\n",
      "[435]\ttrain-rmse:0.288298\n",
      "[436]\ttrain-rmse:0.288265\n",
      "[437]\ttrain-rmse:0.288241\n",
      "[438]\ttrain-rmse:0.288185\n",
      "[439]\ttrain-rmse:0.288023\n",
      "[440]\ttrain-rmse:0.288002\n",
      "[441]\ttrain-rmse:0.287954\n",
      "[442]\ttrain-rmse:0.287928\n",
      "[443]\ttrain-rmse:0.287896\n",
      "[444]\ttrain-rmse:0.287557\n",
      "[445]\ttrain-rmse:0.287168\n",
      "[446]\ttrain-rmse:0.287131\n",
      "[447]\ttrain-rmse:0.28709\n",
      "[448]\ttrain-rmse:0.28675\n",
      "[449]\ttrain-rmse:0.286702\n",
      "[450]\ttrain-rmse:0.286523\n",
      "[451]\ttrain-rmse:0.286497\n",
      "[452]\ttrain-rmse:0.28647\n",
      "[453]\ttrain-rmse:0.286091\n",
      "[454]\ttrain-rmse:0.286056\n",
      "[455]\ttrain-rmse:0.286027\n",
      "[456]\ttrain-rmse:0.286008\n",
      "[457]\ttrain-rmse:0.285749\n",
      "[458]\ttrain-rmse:0.285698\n",
      "[459]\ttrain-rmse:0.285419\n",
      "[460]\ttrain-rmse:0.285398\n",
      "[461]\ttrain-rmse:0.285367\n",
      "[462]\ttrain-rmse:0.285343\n",
      "[463]\ttrain-rmse:0.285288\n",
      "[464]\ttrain-rmse:0.285254\n",
      "[465]\ttrain-rmse:0.285216\n",
      "[466]\ttrain-rmse:0.285188\n",
      "[467]\ttrain-rmse:0.285156\n",
      "[468]\ttrain-rmse:0.284834\n",
      "[469]\ttrain-rmse:0.284671\n",
      "[470]\ttrain-rmse:0.284304\n",
      "[471]\ttrain-rmse:0.284171\n",
      "[472]\ttrain-rmse:0.283909\n",
      "[473]\ttrain-rmse:0.283727\n",
      "[474]\ttrain-rmse:0.283575\n",
      "[475]\ttrain-rmse:0.283555\n",
      "[476]\ttrain-rmse:0.283517\n",
      "[477]\ttrain-rmse:0.283471\n",
      "[478]\ttrain-rmse:0.283449\n",
      "[479]\ttrain-rmse:0.283424\n",
      "[480]\ttrain-rmse:0.283397\n",
      "[481]\ttrain-rmse:0.282985\n",
      "[482]\ttrain-rmse:0.282959\n",
      "[483]\ttrain-rmse:0.282937\n",
      "[484]\ttrain-rmse:0.282904\n",
      "[485]\ttrain-rmse:0.282597\n",
      "[486]\ttrain-rmse:0.282563\n",
      "[487]\ttrain-rmse:0.282299\n",
      "[488]\ttrain-rmse:0.282061\n",
      "[489]\ttrain-rmse:0.282033\n",
      "[490]\ttrain-rmse:0.281993\n",
      "[491]\ttrain-rmse:0.281839\n",
      "[492]\ttrain-rmse:0.281654\n",
      "[493]\ttrain-rmse:0.281597\n",
      "[494]\ttrain-rmse:0.28158\n",
      "[495]\ttrain-rmse:0.281534\n",
      "[496]\ttrain-rmse:0.281336\n",
      "[497]\ttrain-rmse:0.281319\n",
      "[498]\ttrain-rmse:0.281296\n",
      "[499]\ttrain-rmse:0.281267\n",
      "[500]\ttrain-rmse:0.281239\n",
      "[501]\ttrain-rmse:0.281092\n",
      "[502]\ttrain-rmse:0.280962\n",
      "[503]\ttrain-rmse:0.280944\n",
      "[504]\ttrain-rmse:0.280707\n",
      "[505]\ttrain-rmse:0.280655\n",
      "[506]\ttrain-rmse:0.280537\n",
      "[507]\ttrain-rmse:0.280363\n",
      "[508]\ttrain-rmse:0.28002\n",
      "[509]\ttrain-rmse:0.279717\n",
      "[510]\ttrain-rmse:0.279666\n",
      "[511]\ttrain-rmse:0.279393\n",
      "[512]\ttrain-rmse:0.27896\n",
      "[513]\ttrain-rmse:0.278934\n",
      "[514]\ttrain-rmse:0.278658\n",
      "[515]\ttrain-rmse:0.278368\n",
      "[516]\ttrain-rmse:0.278345\n",
      "[517]\ttrain-rmse:0.278315\n",
      "[518]\ttrain-rmse:0.278252\n",
      "[519]\ttrain-rmse:0.278232\n",
      "[520]\ttrain-rmse:0.278209\n",
      "[521]\ttrain-rmse:0.277997\n",
      "[522]\ttrain-rmse:0.277699\n",
      "[523]\ttrain-rmse:0.27755\n",
      "[524]\ttrain-rmse:0.277242\n",
      "[525]\ttrain-rmse:0.276927\n",
      "[526]\ttrain-rmse:0.276602\n",
      "[527]\ttrain-rmse:0.276326\n",
      "[528]\ttrain-rmse:0.276151\n",
      "[529]\ttrain-rmse:0.27613\n",
      "[530]\ttrain-rmse:0.276112\n",
      "[531]\ttrain-rmse:0.275948\n",
      "[532]\ttrain-rmse:0.275672\n",
      "[533]\ttrain-rmse:0.275471\n",
      "[534]\ttrain-rmse:0.275454\n",
      "[535]\ttrain-rmse:0.275094\n",
      "[536]\ttrain-rmse:0.275074\n",
      "[537]\ttrain-rmse:0.274931\n",
      "[538]\ttrain-rmse:0.274706\n",
      "[539]\ttrain-rmse:0.274372\n",
      "[540]\ttrain-rmse:0.274182\n",
      "[541]\ttrain-rmse:0.273915\n",
      "[542]\ttrain-rmse:0.273596\n",
      "[543]\ttrain-rmse:0.273572\n",
      "[544]\ttrain-rmse:0.273525\n",
      "[545]\ttrain-rmse:0.27328\n",
      "[546]\ttrain-rmse:0.272968\n",
      "[547]\ttrain-rmse:0.2728\n",
      "[548]\ttrain-rmse:0.272623\n",
      "[549]\ttrain-rmse:0.272578\n",
      "[550]\ttrain-rmse:0.272329\n",
      "[551]\ttrain-rmse:0.272161\n",
      "[552]\ttrain-rmse:0.272143\n",
      "[553]\ttrain-rmse:0.272077\n",
      "[554]\ttrain-rmse:0.271845\n",
      "[555]\ttrain-rmse:0.27159\n",
      "[556]\ttrain-rmse:0.271551\n",
      "[557]\ttrain-rmse:0.271355\n",
      "[558]\ttrain-rmse:0.271308\n",
      "[559]\ttrain-rmse:0.271134\n",
      "[560]\ttrain-rmse:0.270895\n",
      "[561]\ttrain-rmse:0.270793\n",
      "[562]\ttrain-rmse:0.270743\n",
      "[563]\ttrain-rmse:0.270703\n",
      "[564]\ttrain-rmse:0.270354\n",
      "[565]\ttrain-rmse:0.270181\n",
      "[566]\ttrain-rmse:0.269852\n",
      "[567]\ttrain-rmse:0.269803\n",
      "[568]\ttrain-rmse:0.26978\n",
      "[569]\ttrain-rmse:0.269761\n",
      "[570]\ttrain-rmse:0.269744\n",
      "[571]\ttrain-rmse:0.269701\n",
      "[572]\ttrain-rmse:0.269524\n",
      "[573]\ttrain-rmse:0.26951\n",
      "[574]\ttrain-rmse:0.269459\n",
      "[575]\ttrain-rmse:0.269203\n",
      "[576]\ttrain-rmse:0.269032\n",
      "[577]\ttrain-rmse:0.268829\n",
      "[578]\ttrain-rmse:0.268635\n",
      "[579]\ttrain-rmse:0.268617\n",
      "[580]\ttrain-rmse:0.268603\n",
      "[581]\ttrain-rmse:0.268557\n",
      "[582]\ttrain-rmse:0.268509\n",
      "[583]\ttrain-rmse:0.268488\n",
      "[584]\ttrain-rmse:0.26847\n",
      "[585]\ttrain-rmse:0.268246\n",
      "[586]\ttrain-rmse:0.268007\n",
      "[587]\ttrain-rmse:0.267777\n",
      "[588]\ttrain-rmse:0.267647\n",
      "[589]\ttrain-rmse:0.267627\n",
      "[590]\ttrain-rmse:0.267474\n",
      "[591]\ttrain-rmse:0.267453\n",
      "[592]\ttrain-rmse:0.267436\n",
      "[593]\ttrain-rmse:0.267307\n",
      "[594]\ttrain-rmse:0.267154\n",
      "[595]\ttrain-rmse:0.266687\n",
      "[596]\ttrain-rmse:0.266668\n",
      "[597]\ttrain-rmse:0.266488\n",
      "[598]\ttrain-rmse:0.266209\n",
      "[599]\ttrain-rmse:0.266079\n",
      "[600]\ttrain-rmse:0.266057\n",
      "[601]\ttrain-rmse:0.266016\n",
      "[602]\ttrain-rmse:0.26577\n",
      "[603]\ttrain-rmse:0.265641\n",
      "[604]\ttrain-rmse:0.265617\n",
      "[605]\ttrain-rmse:0.265574\n",
      "[606]\ttrain-rmse:0.26538\n",
      "[607]\ttrain-rmse:0.265357\n",
      "[608]\ttrain-rmse:0.265329\n",
      "[609]\ttrain-rmse:0.265092\n",
      "[610]\ttrain-rmse:0.265055\n",
      "[611]\ttrain-rmse:0.264816\n",
      "[612]\ttrain-rmse:0.264658\n",
      "[613]\ttrain-rmse:0.264627\n",
      "[614]\ttrain-rmse:0.264407\n",
      "[615]\ttrain-rmse:0.264393\n",
      "[616]\ttrain-rmse:0.264274\n",
      "[617]\ttrain-rmse:0.264136\n",
      "[618]\ttrain-rmse:0.264117\n",
      "[619]\ttrain-rmse:0.264104\n",
      "[620]\ttrain-rmse:0.263866\n",
      "[621]\ttrain-rmse:0.26383\n",
      "[622]\ttrain-rmse:0.263806\n",
      "[623]\ttrain-rmse:0.263793\n",
      "[624]\ttrain-rmse:0.26378\n",
      "[625]\ttrain-rmse:0.263502\n",
      "[626]\ttrain-rmse:0.263308\n",
      "[627]\ttrain-rmse:0.263106\n",
      "[628]\ttrain-rmse:0.262944\n",
      "[629]\ttrain-rmse:0.262719\n",
      "[630]\ttrain-rmse:0.262458\n",
      "[631]\ttrain-rmse:0.262443\n",
      "[632]\ttrain-rmse:0.262119\n",
      "[633]\ttrain-rmse:0.261967\n",
      "[634]\ttrain-rmse:0.261882\n",
      "[635]\ttrain-rmse:0.261631\n",
      "[636]\ttrain-rmse:0.261616\n",
      "[637]\ttrain-rmse:0.261496\n",
      "[638]\ttrain-rmse:0.261337\n",
      "[639]\ttrain-rmse:0.261218\n",
      "[640]\ttrain-rmse:0.261206\n",
      "[641]\ttrain-rmse:0.261032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[642]\ttrain-rmse:0.261011\n",
      "[643]\ttrain-rmse:0.260993\n",
      "[644]\ttrain-rmse:0.260739\n",
      "[645]\ttrain-rmse:0.260573\n",
      "[646]\ttrain-rmse:0.260538\n",
      "[647]\ttrain-rmse:0.260516\n",
      "[648]\ttrain-rmse:0.260501\n",
      "[649]\ttrain-rmse:0.260437\n",
      "[650]\ttrain-rmse:0.260336\n",
      "[651]\ttrain-rmse:0.260307\n",
      "[652]\ttrain-rmse:0.260297\n",
      "[653]\ttrain-rmse:0.260274\n",
      "[654]\ttrain-rmse:0.26026\n",
      "[655]\ttrain-rmse:0.260101\n",
      "[656]\ttrain-rmse:0.259928\n",
      "[657]\ttrain-rmse:0.259681\n",
      "[658]\ttrain-rmse:0.259671\n",
      "[659]\ttrain-rmse:0.259503\n",
      "[660]\ttrain-rmse:0.25949\n",
      "[661]\ttrain-rmse:0.259358\n",
      "[662]\ttrain-rmse:0.259339\n",
      "[663]\ttrain-rmse:0.259319\n",
      "[664]\ttrain-rmse:0.259302\n",
      "[665]\ttrain-rmse:0.259131\n",
      "[666]\ttrain-rmse:0.258843\n",
      "[667]\ttrain-rmse:0.258803\n",
      "[668]\ttrain-rmse:0.258567\n",
      "[669]\ttrain-rmse:0.258411\n",
      "[670]\ttrain-rmse:0.25825\n",
      "[671]\ttrain-rmse:0.258235\n",
      "[672]\ttrain-rmse:0.25808\n",
      "[673]\ttrain-rmse:0.257929\n",
      "[674]\ttrain-rmse:0.257902\n",
      "[675]\ttrain-rmse:0.257781\n",
      "[676]\ttrain-rmse:0.257702\n",
      "[677]\ttrain-rmse:0.257688\n",
      "[678]\ttrain-rmse:0.25767\n",
      "[679]\ttrain-rmse:0.257414\n",
      "[680]\ttrain-rmse:0.257212\n",
      "[681]\ttrain-rmse:0.257033\n",
      "[682]\ttrain-rmse:0.257026\n",
      "[683]\ttrain-rmse:0.257011\n",
      "[684]\ttrain-rmse:0.256978\n",
      "[685]\ttrain-rmse:0.256877\n",
      "[686]\ttrain-rmse:0.256859\n",
      "[687]\ttrain-rmse:0.25665\n",
      "[688]\ttrain-rmse:0.256517\n",
      "[689]\ttrain-rmse:0.25627\n",
      "[690]\ttrain-rmse:0.256229\n",
      "[691]\ttrain-rmse:0.256212\n",
      "[692]\ttrain-rmse:0.255993\n",
      "[693]\ttrain-rmse:0.255872\n",
      "[694]\ttrain-rmse:0.255683\n",
      "[695]\ttrain-rmse:0.255652\n",
      "[696]\ttrain-rmse:0.255513\n",
      "[697]\ttrain-rmse:0.255377\n",
      "[698]\ttrain-rmse:0.255118\n",
      "[699]\ttrain-rmse:0.25501\n",
      "[700]\ttrain-rmse:0.254991\n",
      "[701]\ttrain-rmse:0.254963\n",
      "[702]\ttrain-rmse:0.254934\n",
      "[703]\ttrain-rmse:0.254922\n",
      "[704]\ttrain-rmse:0.254885\n",
      "[705]\ttrain-rmse:0.254753\n",
      "[706]\ttrain-rmse:0.254706\n",
      "[707]\ttrain-rmse:0.254509\n",
      "[708]\ttrain-rmse:0.254197\n",
      "[709]\ttrain-rmse:0.25416\n",
      "[710]\ttrain-rmse:0.254021\n",
      "[711]\ttrain-rmse:0.25401\n",
      "[712]\ttrain-rmse:0.253774\n",
      "[713]\ttrain-rmse:0.253652\n",
      "[714]\ttrain-rmse:0.253508\n",
      "[715]\ttrain-rmse:0.25347\n",
      "[716]\ttrain-rmse:0.253425\n",
      "[717]\ttrain-rmse:0.25341\n",
      "[718]\ttrain-rmse:0.253395\n",
      "[719]\ttrain-rmse:0.253264\n",
      "[720]\ttrain-rmse:0.25325\n",
      "[721]\ttrain-rmse:0.253242\n",
      "[722]\ttrain-rmse:0.25313\n",
      "[723]\ttrain-rmse:0.253111\n",
      "[724]\ttrain-rmse:0.253007\n",
      "[725]\ttrain-rmse:0.252875\n",
      "[726]\ttrain-rmse:0.252686\n",
      "[727]\ttrain-rmse:0.252669\n",
      "[728]\ttrain-rmse:0.252415\n",
      "[729]\ttrain-rmse:0.252146\n",
      "[730]\ttrain-rmse:0.252016\n",
      "[731]\ttrain-rmse:0.252001\n",
      "[732]\ttrain-rmse:0.25185\n",
      "[733]\ttrain-rmse:0.251839\n",
      "[734]\ttrain-rmse:0.251825\n",
      "[735]\ttrain-rmse:0.2517\n",
      "[736]\ttrain-rmse:0.251567\n",
      "[737]\ttrain-rmse:0.251309\n",
      "[738]\ttrain-rmse:0.25123\n",
      "[739]\ttrain-rmse:0.251205\n",
      "[740]\ttrain-rmse:0.251189\n",
      "[741]\ttrain-rmse:0.251053\n",
      "[742]\ttrain-rmse:0.250765\n",
      "[743]\ttrain-rmse:0.250751\n",
      "[744]\ttrain-rmse:0.250737\n",
      "[745]\ttrain-rmse:0.250708\n",
      "[746]\ttrain-rmse:0.250697\n",
      "[747]\ttrain-rmse:0.25042\n",
      "[748]\ttrain-rmse:0.250409\n",
      "[749]\ttrain-rmse:0.25028\n",
      "[750]\ttrain-rmse:0.250258\n",
      "[751]\ttrain-rmse:0.250102\n",
      "[752]\ttrain-rmse:0.250086\n",
      "[753]\ttrain-rmse:0.249993\n",
      "[754]\ttrain-rmse:0.249979\n",
      "[755]\ttrain-rmse:0.249868\n",
      "[756]\ttrain-rmse:0.249837\n",
      "[757]\ttrain-rmse:0.249811\n",
      "[758]\ttrain-rmse:0.249572\n",
      "[759]\ttrain-rmse:0.249547\n",
      "[760]\ttrain-rmse:0.249536\n",
      "[761]\ttrain-rmse:0.249423\n",
      "[762]\ttrain-rmse:0.249393\n",
      "[763]\ttrain-rmse:0.249235\n",
      "[764]\ttrain-rmse:0.249222\n",
      "[765]\ttrain-rmse:0.249042\n",
      "[766]\ttrain-rmse:0.249026\n",
      "[767]\ttrain-rmse:0.249012\n",
      "[768]\ttrain-rmse:0.248903\n",
      "[769]\ttrain-rmse:0.248624\n",
      "[770]\ttrain-rmse:0.24861\n",
      "[771]\ttrain-rmse:0.248382\n",
      "[772]\ttrain-rmse:0.248149\n",
      "[773]\ttrain-rmse:0.247916\n",
      "[774]\ttrain-rmse:0.247902\n",
      "[775]\ttrain-rmse:0.247844\n",
      "[776]\ttrain-rmse:0.247818\n",
      "[777]\ttrain-rmse:0.247808\n",
      "[778]\ttrain-rmse:0.247664\n",
      "[779]\ttrain-rmse:0.247511\n",
      "[780]\ttrain-rmse:0.247495\n",
      "[781]\ttrain-rmse:0.247484\n",
      "[782]\ttrain-rmse:0.247409\n",
      "[783]\ttrain-rmse:0.247288\n",
      "[784]\ttrain-rmse:0.247246\n",
      "[785]\ttrain-rmse:0.247047\n",
      "[786]\ttrain-rmse:0.246929\n",
      "[787]\ttrain-rmse:0.246775\n",
      "[788]\ttrain-rmse:0.246504\n",
      "[789]\ttrain-rmse:0.246217\n",
      "[790]\ttrain-rmse:0.246083\n",
      "[791]\ttrain-rmse:0.245907\n",
      "[792]\ttrain-rmse:0.245884\n",
      "[793]\ttrain-rmse:0.245751\n",
      "[794]\ttrain-rmse:0.245657\n",
      "[795]\ttrain-rmse:0.245522\n",
      "[796]\ttrain-rmse:0.245243\n",
      "[797]\ttrain-rmse:0.245045\n",
      "[798]\ttrain-rmse:0.245016\n",
      "[799]\ttrain-rmse:0.244913\n",
      "[800]\ttrain-rmse:0.244896\n",
      "[801]\ttrain-rmse:0.244669\n",
      "[802]\ttrain-rmse:0.244642\n",
      "[803]\ttrain-rmse:0.244542\n",
      "[804]\ttrain-rmse:0.244413\n",
      "[805]\ttrain-rmse:0.244341\n",
      "[806]\ttrain-rmse:0.244323\n",
      "[807]\ttrain-rmse:0.24404\n",
      "[808]\ttrain-rmse:0.244029\n",
      "[809]\ttrain-rmse:0.244013\n",
      "[810]\ttrain-rmse:0.243873\n",
      "[811]\ttrain-rmse:0.243732\n",
      "[812]\ttrain-rmse:0.243588\n",
      "[813]\ttrain-rmse:0.243578\n",
      "[814]\ttrain-rmse:0.243566\n",
      "[815]\ttrain-rmse:0.243485\n",
      "[816]\ttrain-rmse:0.243382\n",
      "[817]\ttrain-rmse:0.243248\n",
      "[818]\ttrain-rmse:0.243142\n",
      "[819]\ttrain-rmse:0.242986\n",
      "[820]\ttrain-rmse:0.242952\n",
      "[821]\ttrain-rmse:0.242941\n",
      "[822]\ttrain-rmse:0.242932\n",
      "[823]\ttrain-rmse:0.242923\n",
      "[824]\ttrain-rmse:0.242912\n",
      "[825]\ttrain-rmse:0.242869\n",
      "[826]\ttrain-rmse:0.242619\n",
      "[827]\ttrain-rmse:0.242612\n",
      "[828]\ttrain-rmse:0.242448\n",
      "[829]\ttrain-rmse:0.24241\n",
      "[830]\ttrain-rmse:0.242261\n",
      "[831]\ttrain-rmse:0.242219\n",
      "[832]\ttrain-rmse:0.242208\n",
      "[833]\ttrain-rmse:0.241986\n",
      "[834]\ttrain-rmse:0.241886\n",
      "[835]\ttrain-rmse:0.24187\n",
      "[836]\ttrain-rmse:0.241849\n",
      "[837]\ttrain-rmse:0.241754\n",
      "[838]\ttrain-rmse:0.241748\n",
      "[839]\ttrain-rmse:0.24174\n",
      "[840]\ttrain-rmse:0.241547\n",
      "[841]\ttrain-rmse:0.241396\n",
      "[842]\ttrain-rmse:0.241212\n",
      "[843]\ttrain-rmse:0.241075\n",
      "[844]\ttrain-rmse:0.240936\n",
      "[845]\ttrain-rmse:0.240847\n",
      "[846]\ttrain-rmse:0.24064\n",
      "[847]\ttrain-rmse:0.240446\n",
      "[848]\ttrain-rmse:0.240439\n",
      "[849]\ttrain-rmse:0.240406\n",
      "[850]\ttrain-rmse:0.240364\n",
      "[851]\ttrain-rmse:0.240347\n",
      "[852]\ttrain-rmse:0.240138\n",
      "[853]\ttrain-rmse:0.239921\n",
      "[854]\ttrain-rmse:0.239705\n",
      "[855]\ttrain-rmse:0.239695\n",
      "[856]\ttrain-rmse:0.239586\n",
      "[857]\ttrain-rmse:0.239567\n",
      "[858]\ttrain-rmse:0.239559\n",
      "[859]\ttrain-rmse:0.239209\n",
      "[860]\ttrain-rmse:0.239189\n",
      "[861]\ttrain-rmse:0.239057\n",
      "[862]\ttrain-rmse:0.239038\n",
      "[863]\ttrain-rmse:0.23894\n",
      "[864]\ttrain-rmse:0.238925\n",
      "[865]\ttrain-rmse:0.238799\n",
      "[866]\ttrain-rmse:0.238594\n",
      "[867]\ttrain-rmse:0.238425\n",
      "[868]\ttrain-rmse:0.238414\n",
      "[869]\ttrain-rmse:0.238407\n",
      "[870]\ttrain-rmse:0.238312\n",
      "[871]\ttrain-rmse:0.238296\n",
      "[872]\ttrain-rmse:0.238194\n",
      "[873]\ttrain-rmse:0.238105\n",
      "[874]\ttrain-rmse:0.237976\n",
      "[875]\ttrain-rmse:0.237784\n",
      "[876]\ttrain-rmse:0.237777\n",
      "[877]\ttrain-rmse:0.237616\n",
      "[878]\ttrain-rmse:0.237581\n",
      "[879]\ttrain-rmse:0.237337\n",
      "[880]\ttrain-rmse:0.237324\n",
      "[881]\ttrain-rmse:0.237118\n",
      "[882]\ttrain-rmse:0.237039\n",
      "[883]\ttrain-rmse:0.236973\n",
      "[884]\ttrain-rmse:0.236905\n",
      "[885]\ttrain-rmse:0.236898\n",
      "[886]\ttrain-rmse:0.236886\n",
      "[887]\ttrain-rmse:0.236791\n",
      "[888]\ttrain-rmse:0.236598\n",
      "[889]\ttrain-rmse:0.23655\n",
      "[890]\ttrain-rmse:0.236538\n",
      "[891]\ttrain-rmse:0.236334\n",
      "[892]\ttrain-rmse:0.236312\n",
      "[893]\ttrain-rmse:0.236293\n",
      "[894]\ttrain-rmse:0.236275\n",
      "[895]\ttrain-rmse:0.236088\n",
      "[896]\ttrain-rmse:0.236074\n",
      "[897]\ttrain-rmse:0.236\n",
      "[898]\ttrain-rmse:0.235941\n",
      "[899]\ttrain-rmse:0.235774\n",
      "[900]\ttrain-rmse:0.235764\n",
      "[901]\ttrain-rmse:0.235669\n",
      "[902]\ttrain-rmse:0.23566\n",
      "[903]\ttrain-rmse:0.235566\n",
      "[904]\ttrain-rmse:0.23554\n",
      "[905]\ttrain-rmse:0.235528\n",
      "[906]\ttrain-rmse:0.235342\n",
      "[907]\ttrain-rmse:0.235335\n",
      "[908]\ttrain-rmse:0.235317\n",
      "[909]\ttrain-rmse:0.235225\n",
      "[910]\ttrain-rmse:0.235094\n",
      "[911]\ttrain-rmse:0.23495\n",
      "[912]\ttrain-rmse:0.234946\n",
      "[913]\ttrain-rmse:0.234775\n",
      "[914]\ttrain-rmse:0.234759\n",
      "[915]\ttrain-rmse:0.234605\n",
      "[916]\ttrain-rmse:0.234501\n",
      "[917]\ttrain-rmse:0.234256\n",
      "[918]\ttrain-rmse:0.234133\n",
      "[919]\ttrain-rmse:0.23398\n",
      "[920]\ttrain-rmse:0.233977\n",
      "[921]\ttrain-rmse:0.233969\n",
      "[922]\ttrain-rmse:0.233945\n",
      "[923]\ttrain-rmse:0.233732\n",
      "[924]\ttrain-rmse:0.233722\n",
      "[925]\ttrain-rmse:0.233596\n",
      "[926]\ttrain-rmse:0.233533\n",
      "[927]\ttrain-rmse:0.233493\n",
      "[928]\ttrain-rmse:0.233483\n",
      "[929]\ttrain-rmse:0.233346\n",
      "[930]\ttrain-rmse:0.233339\n",
      "[931]\ttrain-rmse:0.233329\n",
      "[932]\ttrain-rmse:0.233255\n",
      "[933]\ttrain-rmse:0.233195\n",
      "[934]\ttrain-rmse:0.233181\n",
      "[935]\ttrain-rmse:0.233166\n",
      "[936]\ttrain-rmse:0.233158\n",
      "[937]\ttrain-rmse:0.233148\n",
      "[938]\ttrain-rmse:0.233119\n",
      "[939]\ttrain-rmse:0.233059\n",
      "[940]\ttrain-rmse:0.233055\n",
      "[941]\ttrain-rmse:0.232874\n",
      "[942]\ttrain-rmse:0.232866\n",
      "[943]\ttrain-rmse:0.232714\n",
      "[944]\ttrain-rmse:0.232525\n",
      "[945]\ttrain-rmse:0.232517\n",
      "[946]\ttrain-rmse:0.232316\n",
      "[947]\ttrain-rmse:0.232147\n",
      "[948]\ttrain-rmse:0.232132\n",
      "[949]\ttrain-rmse:0.232123\n",
      "[950]\ttrain-rmse:0.231956\n",
      "[951]\ttrain-rmse:0.231794\n",
      "[952]\ttrain-rmse:0.231782\n",
      "[953]\ttrain-rmse:0.231769\n",
      "[954]\ttrain-rmse:0.231762\n",
      "[955]\ttrain-rmse:0.231746\n",
      "[956]\ttrain-rmse:0.231577\n",
      "[957]\ttrain-rmse:0.231567\n",
      "[958]\ttrain-rmse:0.231538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[959]\ttrain-rmse:0.231535\n",
      "[960]\ttrain-rmse:0.23149\n",
      "[961]\ttrain-rmse:0.231481\n",
      "[962]\ttrain-rmse:0.23145\n",
      "[963]\ttrain-rmse:0.231273\n",
      "[964]\ttrain-rmse:0.231131\n",
      "[965]\ttrain-rmse:0.231121\n",
      "[966]\ttrain-rmse:0.231113\n",
      "[967]\ttrain-rmse:0.231006\n",
      "[968]\ttrain-rmse:0.230999\n",
      "[969]\ttrain-rmse:0.230864\n",
      "[970]\ttrain-rmse:0.230855\n",
      "[971]\ttrain-rmse:0.230827\n",
      "[972]\ttrain-rmse:0.230679\n",
      "[973]\ttrain-rmse:0.230488\n",
      "[974]\ttrain-rmse:0.230329\n",
      "[975]\ttrain-rmse:0.230213\n",
      "[976]\ttrain-rmse:0.230115\n",
      "[977]\ttrain-rmse:0.2301\n",
      "[978]\ttrain-rmse:0.230089\n",
      "[979]\ttrain-rmse:0.229923\n",
      "[980]\ttrain-rmse:0.229893\n",
      "[981]\ttrain-rmse:0.229881\n",
      "[982]\ttrain-rmse:0.229867\n",
      "[983]\ttrain-rmse:0.229754\n",
      "[984]\ttrain-rmse:0.229747\n",
      "[985]\ttrain-rmse:0.229611\n",
      "[986]\ttrain-rmse:0.229486\n",
      "[987]\ttrain-rmse:0.229474\n",
      "[988]\ttrain-rmse:0.229335\n",
      "[989]\ttrain-rmse:0.229318\n",
      "[990]\ttrain-rmse:0.229229\n",
      "[991]\ttrain-rmse:0.229217\n",
      "[992]\ttrain-rmse:0.229063\n",
      "[993]\ttrain-rmse:0.229052\n",
      "[994]\ttrain-rmse:0.228953\n",
      "[995]\ttrain-rmse:0.228855\n",
      "[996]\ttrain-rmse:0.228694\n",
      "[997]\ttrain-rmse:0.228603\n",
      "[998]\ttrain-rmse:0.228598\n",
      "[999]\ttrain-rmse:0.228504\n",
      "Result on validation data:  0.1937494686448902\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "print(\"Fitting XGBoost...\")\n",
    "models.append(XGBoost(X_train, y_train, X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5）评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate combined models...\n",
      "Training error...\n",
      "0.18057345828903387\n",
      "Validation error...\n",
      "0.1937494686448902\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X_val, y_val)\n",
    "print(r_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4使用Embedding数据的XGBoost模型\n",
    "1）把Embedding作为输入XGBoost模型的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using learned embeddings as input\n"
     ]
    }
   ],
   "source": [
    "embeddings_as_input=True\n",
    "if embeddings_as_input:\n",
    "    print(\"Using learned embeddings as input\")\n",
    "    X = embed_features(X, saved_embeddings_fname)\n",
    "    \n",
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting XGBoost...\n",
      "[0]\ttrain-rmse:8.09873\n",
      "[1]\ttrain-rmse:7.93695\n",
      "[2]\ttrain-rmse:7.77837\n",
      "[3]\ttrain-rmse:7.62285\n",
      "[4]\ttrain-rmse:7.4707\n",
      "[5]\ttrain-rmse:7.32131\n",
      "[6]\ttrain-rmse:7.1752\n",
      "[7]\ttrain-rmse:7.03175\n",
      "[8]\ttrain-rmse:6.89139\n",
      "[9]\ttrain-rmse:6.7537\n",
      "[10]\ttrain-rmse:6.61877\n",
      "[11]\ttrain-rmse:6.48656\n",
      "[12]\ttrain-rmse:6.35701\n",
      "[13]\ttrain-rmse:6.22999\n",
      "[14]\ttrain-rmse:6.10559\n",
      "[15]\ttrain-rmse:5.98363\n",
      "[16]\ttrain-rmse:5.86411\n",
      "[17]\ttrain-rmse:5.74706\n",
      "[18]\ttrain-rmse:5.63222\n",
      "[19]\ttrain-rmse:5.5198\n",
      "[20]\ttrain-rmse:5.40954\n",
      "[21]\ttrain-rmse:5.30154\n",
      "[22]\ttrain-rmse:5.19566\n",
      "[23]\ttrain-rmse:5.09197\n",
      "[24]\ttrain-rmse:4.99026\n",
      "[25]\ttrain-rmse:4.89059\n",
      "[26]\ttrain-rmse:4.79296\n",
      "[27]\ttrain-rmse:4.69732\n",
      "[28]\ttrain-rmse:4.60357\n",
      "[29]\ttrain-rmse:4.51169\n",
      "[30]\ttrain-rmse:4.42166\n",
      "[31]\ttrain-rmse:4.33343\n",
      "[32]\ttrain-rmse:4.24688\n",
      "[33]\ttrain-rmse:4.16214\n",
      "[34]\ttrain-rmse:4.07916\n",
      "[35]\ttrain-rmse:3.99778\n",
      "[36]\ttrain-rmse:3.91802\n",
      "[37]\ttrain-rmse:3.83987\n",
      "[38]\ttrain-rmse:3.76333\n",
      "[39]\ttrain-rmse:3.68819\n",
      "[40]\ttrain-rmse:3.61467\n",
      "[41]\ttrain-rmse:3.54259\n",
      "[42]\ttrain-rmse:3.47196\n",
      "[43]\ttrain-rmse:3.40273\n",
      "[44]\ttrain-rmse:3.33492\n",
      "[45]\ttrain-rmse:3.26848\n",
      "[46]\ttrain-rmse:3.20331\n",
      "[47]\ttrain-rmse:3.1395\n",
      "[48]\ttrain-rmse:3.07694\n",
      "[49]\ttrain-rmse:3.01567\n",
      "[50]\ttrain-rmse:2.95558\n",
      "[51]\ttrain-rmse:2.89673\n",
      "[52]\ttrain-rmse:2.83905\n",
      "[53]\ttrain-rmse:2.78254\n",
      "[54]\ttrain-rmse:2.72712\n",
      "[55]\ttrain-rmse:2.67283\n",
      "[56]\ttrain-rmse:2.61962\n",
      "[57]\ttrain-rmse:2.5675\n",
      "[58]\ttrain-rmse:2.51645\n",
      "[59]\ttrain-rmse:2.46641\n",
      "[60]\ttrain-rmse:2.41736\n",
      "[61]\ttrain-rmse:2.36928\n",
      "[62]\ttrain-rmse:2.32219\n",
      "[63]\ttrain-rmse:2.27602\n",
      "[64]\ttrain-rmse:2.23079\n",
      "[65]\ttrain-rmse:2.18648\n",
      "[66]\ttrain-rmse:2.14301\n",
      "[67]\ttrain-rmse:2.10046\n",
      "[68]\ttrain-rmse:2.05876\n",
      "[69]\ttrain-rmse:2.01787\n",
      "[70]\ttrain-rmse:1.97786\n",
      "[71]\ttrain-rmse:1.93864\n",
      "[72]\ttrain-rmse:1.90015\n",
      "[73]\ttrain-rmse:1.86243\n",
      "[74]\ttrain-rmse:1.82551\n",
      "[75]\ttrain-rmse:1.78929\n",
      "[76]\ttrain-rmse:1.75385\n",
      "[77]\ttrain-rmse:1.71909\n",
      "[78]\ttrain-rmse:1.68503\n",
      "[79]\ttrain-rmse:1.65165\n",
      "[80]\ttrain-rmse:1.61896\n",
      "[81]\ttrain-rmse:1.58693\n",
      "[82]\ttrain-rmse:1.55555\n",
      "[83]\ttrain-rmse:1.52479\n",
      "[84]\ttrain-rmse:1.49465\n",
      "[85]\ttrain-rmse:1.46514\n",
      "[86]\ttrain-rmse:1.43617\n",
      "[87]\ttrain-rmse:1.40786\n",
      "[88]\ttrain-rmse:1.38008\n",
      "[89]\ttrain-rmse:1.35285\n",
      "[90]\ttrain-rmse:1.32619\n",
      "[91]\ttrain-rmse:1.30008\n",
      "[92]\ttrain-rmse:1.27445\n",
      "[93]\ttrain-rmse:1.24936\n",
      "[94]\ttrain-rmse:1.2248\n",
      "[95]\ttrain-rmse:1.20073\n",
      "[96]\ttrain-rmse:1.17713\n",
      "[97]\ttrain-rmse:1.15404\n",
      "[98]\ttrain-rmse:1.1314\n",
      "[99]\ttrain-rmse:1.10921\n",
      "[100]\ttrain-rmse:1.08747\n",
      "[101]\ttrain-rmse:1.06617\n",
      "[102]\ttrain-rmse:1.04529\n",
      "[103]\ttrain-rmse:1.02487\n",
      "[104]\ttrain-rmse:1.00486\n",
      "[105]\ttrain-rmse:0.985248\n",
      "[106]\ttrain-rmse:0.966023\n",
      "[107]\ttrain-rmse:0.947197\n",
      "[108]\ttrain-rmse:0.928766\n",
      "[109]\ttrain-rmse:0.910692\n",
      "[110]\ttrain-rmse:0.893004\n",
      "[111]\ttrain-rmse:0.875682\n",
      "[112]\ttrain-rmse:0.85874\n",
      "[113]\ttrain-rmse:0.842111\n",
      "[114]\ttrain-rmse:0.825838\n",
      "[115]\ttrain-rmse:0.809877\n",
      "[116]\ttrain-rmse:0.794258\n",
      "[117]\ttrain-rmse:0.778961\n",
      "[118]\ttrain-rmse:0.763991\n",
      "[119]\ttrain-rmse:0.749287\n",
      "[120]\ttrain-rmse:0.734902\n",
      "[121]\ttrain-rmse:0.720802\n",
      "[122]\ttrain-rmse:0.707001\n",
      "[123]\ttrain-rmse:0.693516\n",
      "[124]\ttrain-rmse:0.680288\n",
      "[125]\ttrain-rmse:0.66731\n",
      "[126]\ttrain-rmse:0.654627\n",
      "[127]\ttrain-rmse:0.6422\n",
      "[128]\ttrain-rmse:0.630053\n",
      "[129]\ttrain-rmse:0.618142\n",
      "[130]\ttrain-rmse:0.606462\n",
      "[131]\ttrain-rmse:0.595023\n",
      "[132]\ttrain-rmse:0.583857\n",
      "[133]\ttrain-rmse:0.572903\n",
      "[134]\ttrain-rmse:0.56218\n",
      "[135]\ttrain-rmse:0.551721\n",
      "[136]\ttrain-rmse:0.541435\n",
      "[137]\ttrain-rmse:0.531359\n",
      "[138]\ttrain-rmse:0.521518\n",
      "[139]\ttrain-rmse:0.511881\n",
      "[140]\ttrain-rmse:0.502474\n",
      "[141]\ttrain-rmse:0.493252\n",
      "[142]\ttrain-rmse:0.484267\n",
      "[143]\ttrain-rmse:0.47544\n",
      "[144]\ttrain-rmse:0.466799\n",
      "[145]\ttrain-rmse:0.458335\n",
      "[146]\ttrain-rmse:0.450055\n",
      "[147]\ttrain-rmse:0.441955\n",
      "[148]\ttrain-rmse:0.43402\n",
      "[149]\ttrain-rmse:0.426259\n",
      "[150]\ttrain-rmse:0.418663\n",
      "[151]\ttrain-rmse:0.411236\n",
      "[152]\ttrain-rmse:0.403976\n",
      "[153]\ttrain-rmse:0.396896\n",
      "[154]\ttrain-rmse:0.389979\n",
      "[155]\ttrain-rmse:0.383182\n",
      "[156]\ttrain-rmse:0.376549\n",
      "[157]\ttrain-rmse:0.370067\n",
      "[158]\ttrain-rmse:0.36372\n",
      "[159]\ttrain-rmse:0.357549\n",
      "[160]\ttrain-rmse:0.351459\n",
      "[161]\ttrain-rmse:0.345542\n",
      "[162]\ttrain-rmse:0.339761\n",
      "[163]\ttrain-rmse:0.334115\n",
      "[164]\ttrain-rmse:0.328587\n",
      "[165]\ttrain-rmse:0.32319\n",
      "[166]\ttrain-rmse:0.317905\n",
      "[167]\ttrain-rmse:0.312754\n",
      "[168]\ttrain-rmse:0.307729\n",
      "[169]\ttrain-rmse:0.302829\n",
      "[170]\ttrain-rmse:0.298016\n",
      "[171]\ttrain-rmse:0.293318\n",
      "[172]\ttrain-rmse:0.288724\n",
      "[173]\ttrain-rmse:0.284266\n",
      "[174]\ttrain-rmse:0.279883\n",
      "[175]\ttrain-rmse:0.275589\n",
      "[176]\ttrain-rmse:0.271418\n",
      "[177]\ttrain-rmse:0.267373\n",
      "[178]\ttrain-rmse:0.263419\n",
      "[179]\ttrain-rmse:0.259555\n",
      "[180]\ttrain-rmse:0.255779\n",
      "[181]\ttrain-rmse:0.252112\n",
      "[182]\ttrain-rmse:0.248487\n",
      "[183]\ttrain-rmse:0.245005\n",
      "[184]\ttrain-rmse:0.241596\n",
      "[185]\ttrain-rmse:0.238268\n",
      "[186]\ttrain-rmse:0.234996\n",
      "[187]\ttrain-rmse:0.231849\n",
      "[188]\ttrain-rmse:0.228791\n",
      "[189]\ttrain-rmse:0.22579\n",
      "[190]\ttrain-rmse:0.222874\n",
      "[191]\ttrain-rmse:0.220041\n",
      "[192]\ttrain-rmse:0.217246\n",
      "[193]\ttrain-rmse:0.214496\n",
      "[194]\ttrain-rmse:0.211839\n",
      "[195]\ttrain-rmse:0.209251\n",
      "[196]\ttrain-rmse:0.206763\n",
      "[197]\ttrain-rmse:0.204303\n",
      "[198]\ttrain-rmse:0.201961\n",
      "[199]\ttrain-rmse:0.199657\n",
      "[200]\ttrain-rmse:0.197404\n",
      "[201]\ttrain-rmse:0.195222\n",
      "[202]\ttrain-rmse:0.19312\n",
      "[203]\ttrain-rmse:0.191011\n",
      "[204]\ttrain-rmse:0.189002\n",
      "[205]\ttrain-rmse:0.187054\n",
      "[206]\ttrain-rmse:0.185122\n",
      "[207]\ttrain-rmse:0.183283\n",
      "[208]\ttrain-rmse:0.181502\n",
      "[209]\ttrain-rmse:0.179744\n",
      "[210]\ttrain-rmse:0.178072\n",
      "[211]\ttrain-rmse:0.176434\n",
      "[212]\ttrain-rmse:0.174855\n",
      "[213]\ttrain-rmse:0.173321\n",
      "[214]\ttrain-rmse:0.171822\n",
      "[215]\ttrain-rmse:0.170341\n",
      "[216]\ttrain-rmse:0.16891\n",
      "[217]\ttrain-rmse:0.167519\n",
      "[218]\ttrain-rmse:0.166166\n",
      "[219]\ttrain-rmse:0.164838\n",
      "[220]\ttrain-rmse:0.163562\n",
      "[221]\ttrain-rmse:0.162319\n",
      "[222]\ttrain-rmse:0.161114\n",
      "[223]\ttrain-rmse:0.159914\n",
      "[224]\ttrain-rmse:0.15879\n",
      "[225]\ttrain-rmse:0.157718\n",
      "[226]\ttrain-rmse:0.156603\n",
      "[227]\ttrain-rmse:0.155603\n",
      "[228]\ttrain-rmse:0.154514\n",
      "[229]\ttrain-rmse:0.153534\n",
      "[230]\ttrain-rmse:0.152612\n",
      "[231]\ttrain-rmse:0.151676\n",
      "[232]\ttrain-rmse:0.1507\n",
      "[233]\ttrain-rmse:0.149833\n",
      "[234]\ttrain-rmse:0.148999\n",
      "[235]\ttrain-rmse:0.148197\n",
      "[236]\ttrain-rmse:0.147421\n",
      "[237]\ttrain-rmse:0.14669\n",
      "[238]\ttrain-rmse:0.145975\n",
      "[239]\ttrain-rmse:0.145201\n",
      "[240]\ttrain-rmse:0.144531\n",
      "[241]\ttrain-rmse:0.143844\n",
      "[242]\ttrain-rmse:0.143179\n",
      "[243]\ttrain-rmse:0.142552\n",
      "[244]\ttrain-rmse:0.141961\n",
      "[245]\ttrain-rmse:0.141374\n",
      "[246]\ttrain-rmse:0.140731\n",
      "[247]\ttrain-rmse:0.140158\n",
      "[248]\ttrain-rmse:0.139605\n",
      "[249]\ttrain-rmse:0.139018\n",
      "[250]\ttrain-rmse:0.13847\n",
      "[251]\ttrain-rmse:0.138001\n",
      "[252]\ttrain-rmse:0.13744\n",
      "[253]\ttrain-rmse:0.13699\n",
      "[254]\ttrain-rmse:0.136538\n",
      "[255]\ttrain-rmse:0.136123\n",
      "[256]\ttrain-rmse:0.135638\n",
      "[257]\ttrain-rmse:0.135254\n",
      "[258]\ttrain-rmse:0.134871\n",
      "[259]\ttrain-rmse:0.134485\n",
      "[260]\ttrain-rmse:0.134136\n",
      "[261]\ttrain-rmse:0.133753\n",
      "[262]\ttrain-rmse:0.133304\n",
      "[263]\ttrain-rmse:0.132869\n",
      "[264]\ttrain-rmse:0.132503\n",
      "[265]\ttrain-rmse:0.13212\n",
      "[266]\ttrain-rmse:0.131821\n",
      "[267]\ttrain-rmse:0.13147\n",
      "[268]\ttrain-rmse:0.131093\n",
      "[269]\ttrain-rmse:0.130807\n",
      "[270]\ttrain-rmse:0.130509\n",
      "[271]\ttrain-rmse:0.130219\n",
      "[272]\ttrain-rmse:0.129953\n",
      "[273]\ttrain-rmse:0.129642\n",
      "[274]\ttrain-rmse:0.129417\n",
      "[275]\ttrain-rmse:0.129127\n",
      "[276]\ttrain-rmse:0.128876\n",
      "[277]\ttrain-rmse:0.128561\n",
      "[278]\ttrain-rmse:0.128344\n",
      "[279]\ttrain-rmse:0.128111\n",
      "[280]\ttrain-rmse:0.127889\n",
      "[281]\ttrain-rmse:0.127651\n",
      "[282]\ttrain-rmse:0.127388\n",
      "[283]\ttrain-rmse:0.127181\n",
      "[284]\ttrain-rmse:0.126978\n",
      "[285]\ttrain-rmse:0.126773\n",
      "[286]\ttrain-rmse:0.126605\n",
      "[287]\ttrain-rmse:0.126384\n",
      "[288]\ttrain-rmse:0.126181\n",
      "[289]\ttrain-rmse:0.126026\n",
      "[290]\ttrain-rmse:0.125831\n",
      "[291]\ttrain-rmse:0.125679\n",
      "[292]\ttrain-rmse:0.125496\n",
      "[293]\ttrain-rmse:0.125315\n",
      "[294]\ttrain-rmse:0.125091\n",
      "[295]\ttrain-rmse:0.124878\n",
      "[296]\ttrain-rmse:0.124718\n",
      "[297]\ttrain-rmse:0.124576\n",
      "[298]\ttrain-rmse:0.124417\n",
      "[299]\ttrain-rmse:0.124279\n",
      "[300]\ttrain-rmse:0.12414\n",
      "[301]\ttrain-rmse:0.124018\n",
      "[302]\ttrain-rmse:0.123912\n",
      "[303]\ttrain-rmse:0.123767\n",
      "[304]\ttrain-rmse:0.123641\n",
      "[305]\ttrain-rmse:0.123501\n",
      "[306]\ttrain-rmse:0.123349\n",
      "[307]\ttrain-rmse:0.123224\n",
      "[308]\ttrain-rmse:0.123136\n",
      "[309]\ttrain-rmse:0.123013\n",
      "[310]\ttrain-rmse:0.122887\n",
      "[311]\ttrain-rmse:0.122735\n",
      "[312]\ttrain-rmse:0.122606\n",
      "[313]\ttrain-rmse:0.122505\n",
      "[314]\ttrain-rmse:0.122365\n",
      "[315]\ttrain-rmse:0.122273\n",
      "[316]\ttrain-rmse:0.122156\n",
      "[317]\ttrain-rmse:0.122054\n",
      "[318]\ttrain-rmse:0.121969\n",
      "[319]\ttrain-rmse:0.121876\n",
      "[320]\ttrain-rmse:0.121767\n",
      "[321]\ttrain-rmse:0.121668\n",
      "[322]\ttrain-rmse:0.121558\n",
      "[323]\ttrain-rmse:0.121443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[324]\ttrain-rmse:0.121339\n",
      "[325]\ttrain-rmse:0.121212\n",
      "[326]\ttrain-rmse:0.121147\n",
      "[327]\ttrain-rmse:0.121077\n",
      "[328]\ttrain-rmse:0.120964\n",
      "[329]\ttrain-rmse:0.12085\n",
      "[330]\ttrain-rmse:0.120763\n",
      "[331]\ttrain-rmse:0.120668\n",
      "[332]\ttrain-rmse:0.120591\n",
      "[333]\ttrain-rmse:0.120479\n",
      "[334]\ttrain-rmse:0.120391\n",
      "[335]\ttrain-rmse:0.120297\n",
      "[336]\ttrain-rmse:0.120215\n",
      "[337]\ttrain-rmse:0.120148\n",
      "[338]\ttrain-rmse:0.120064\n",
      "[339]\ttrain-rmse:0.119979\n",
      "[340]\ttrain-rmse:0.119907\n",
      "[341]\ttrain-rmse:0.11981\n",
      "[342]\ttrain-rmse:0.119754\n",
      "[343]\ttrain-rmse:0.119681\n",
      "[344]\ttrain-rmse:0.119592\n",
      "[345]\ttrain-rmse:0.11951\n",
      "[346]\ttrain-rmse:0.119465\n",
      "[347]\ttrain-rmse:0.119404\n",
      "[348]\ttrain-rmse:0.119306\n",
      "[349]\ttrain-rmse:0.119234\n",
      "[350]\ttrain-rmse:0.119158\n",
      "[351]\ttrain-rmse:0.119033\n",
      "[352]\ttrain-rmse:0.11899\n",
      "[353]\ttrain-rmse:0.118934\n",
      "[354]\ttrain-rmse:0.118832\n",
      "[355]\ttrain-rmse:0.118786\n",
      "[356]\ttrain-rmse:0.118743\n",
      "[357]\ttrain-rmse:0.1187\n",
      "[358]\ttrain-rmse:0.118642\n",
      "[359]\ttrain-rmse:0.118561\n",
      "[360]\ttrain-rmse:0.118516\n",
      "[361]\ttrain-rmse:0.118467\n",
      "[362]\ttrain-rmse:0.118395\n",
      "[363]\ttrain-rmse:0.118332\n",
      "[364]\ttrain-rmse:0.118256\n",
      "[365]\ttrain-rmse:0.118211\n",
      "[366]\ttrain-rmse:0.118134\n",
      "[367]\ttrain-rmse:0.118055\n",
      "[368]\ttrain-rmse:0.118014\n",
      "[369]\ttrain-rmse:0.117971\n",
      "[370]\ttrain-rmse:0.117921\n",
      "[371]\ttrain-rmse:0.117845\n",
      "[372]\ttrain-rmse:0.1178\n",
      "[373]\ttrain-rmse:0.117773\n",
      "[374]\ttrain-rmse:0.117698\n",
      "[375]\ttrain-rmse:0.117603\n",
      "[376]\ttrain-rmse:0.117559\n",
      "[377]\ttrain-rmse:0.117498\n",
      "[378]\ttrain-rmse:0.117458\n",
      "[379]\ttrain-rmse:0.11737\n",
      "[380]\ttrain-rmse:0.117291\n",
      "[381]\ttrain-rmse:0.117259\n",
      "[382]\ttrain-rmse:0.117224\n",
      "[383]\ttrain-rmse:0.117197\n",
      "[384]\ttrain-rmse:0.117167\n",
      "[385]\ttrain-rmse:0.117088\n",
      "[386]\ttrain-rmse:0.117043\n",
      "[387]\ttrain-rmse:0.116994\n",
      "[388]\ttrain-rmse:0.116914\n",
      "[389]\ttrain-rmse:0.11684\n",
      "[390]\ttrain-rmse:0.116798\n",
      "[391]\ttrain-rmse:0.116705\n",
      "[392]\ttrain-rmse:0.116675\n",
      "[393]\ttrain-rmse:0.116647\n",
      "[394]\ttrain-rmse:0.116606\n",
      "[395]\ttrain-rmse:0.116563\n",
      "[396]\ttrain-rmse:0.116531\n",
      "[397]\ttrain-rmse:0.116486\n",
      "[398]\ttrain-rmse:0.116457\n",
      "[399]\ttrain-rmse:0.116364\n",
      "[400]\ttrain-rmse:0.116264\n",
      "[401]\ttrain-rmse:0.116217\n",
      "[402]\ttrain-rmse:0.116173\n",
      "[403]\ttrain-rmse:0.116089\n",
      "[404]\ttrain-rmse:0.116053\n",
      "[405]\ttrain-rmse:0.116025\n",
      "[406]\ttrain-rmse:0.115964\n",
      "[407]\ttrain-rmse:0.115895\n",
      "[408]\ttrain-rmse:0.115851\n",
      "[409]\ttrain-rmse:0.115807\n",
      "[410]\ttrain-rmse:0.115783\n",
      "[411]\ttrain-rmse:0.115766\n",
      "[412]\ttrain-rmse:0.115748\n",
      "[413]\ttrain-rmse:0.115678\n",
      "[414]\ttrain-rmse:0.115603\n",
      "[415]\ttrain-rmse:0.115556\n",
      "[416]\ttrain-rmse:0.115506\n",
      "[417]\ttrain-rmse:0.115461\n",
      "[418]\ttrain-rmse:0.115435\n",
      "[419]\ttrain-rmse:0.115413\n",
      "[420]\ttrain-rmse:0.115363\n",
      "[421]\ttrain-rmse:0.115325\n",
      "[422]\ttrain-rmse:0.115297\n",
      "[423]\ttrain-rmse:0.115256\n",
      "[424]\ttrain-rmse:0.115223\n",
      "[425]\ttrain-rmse:0.115189\n",
      "[426]\ttrain-rmse:0.115155\n",
      "[427]\ttrain-rmse:0.115134\n",
      "[428]\ttrain-rmse:0.115106\n",
      "[429]\ttrain-rmse:0.115069\n",
      "[430]\ttrain-rmse:0.115029\n",
      "[431]\ttrain-rmse:0.115003\n",
      "[432]\ttrain-rmse:0.114943\n",
      "[433]\ttrain-rmse:0.11486\n",
      "[434]\ttrain-rmse:0.114839\n",
      "[435]\ttrain-rmse:0.114797\n",
      "[436]\ttrain-rmse:0.114774\n",
      "[437]\ttrain-rmse:0.114745\n",
      "[438]\ttrain-rmse:0.114657\n",
      "[439]\ttrain-rmse:0.114574\n",
      "[440]\ttrain-rmse:0.114489\n",
      "[441]\ttrain-rmse:0.114464\n",
      "[442]\ttrain-rmse:0.114421\n",
      "[443]\ttrain-rmse:0.114395\n",
      "[444]\ttrain-rmse:0.114362\n",
      "[445]\ttrain-rmse:0.114294\n",
      "[446]\ttrain-rmse:0.114258\n",
      "[447]\ttrain-rmse:0.114223\n",
      "[448]\ttrain-rmse:0.114187\n",
      "[449]\ttrain-rmse:0.114162\n",
      "[450]\ttrain-rmse:0.114113\n",
      "[451]\ttrain-rmse:0.114084\n",
      "[452]\ttrain-rmse:0.114054\n",
      "[453]\ttrain-rmse:0.113997\n",
      "[454]\ttrain-rmse:0.113948\n",
      "[455]\ttrain-rmse:0.113907\n",
      "[456]\ttrain-rmse:0.113873\n",
      "[457]\ttrain-rmse:0.113834\n",
      "[458]\ttrain-rmse:0.113816\n",
      "[459]\ttrain-rmse:0.11376\n",
      "[460]\ttrain-rmse:0.113745\n",
      "[461]\ttrain-rmse:0.113683\n",
      "[462]\ttrain-rmse:0.113649\n",
      "[463]\ttrain-rmse:0.113572\n",
      "[464]\ttrain-rmse:0.113509\n",
      "[465]\ttrain-rmse:0.113461\n",
      "[466]\ttrain-rmse:0.113438\n",
      "[467]\ttrain-rmse:0.113409\n",
      "[468]\ttrain-rmse:0.113393\n",
      "[469]\ttrain-rmse:0.113334\n",
      "[470]\ttrain-rmse:0.113303\n",
      "[471]\ttrain-rmse:0.113261\n",
      "[472]\ttrain-rmse:0.113218\n",
      "[473]\ttrain-rmse:0.113181\n",
      "[474]\ttrain-rmse:0.113161\n",
      "[475]\ttrain-rmse:0.113088\n",
      "[476]\ttrain-rmse:0.113038\n",
      "[477]\ttrain-rmse:0.113002\n",
      "[478]\ttrain-rmse:0.112945\n",
      "[479]\ttrain-rmse:0.112881\n",
      "[480]\ttrain-rmse:0.112861\n",
      "[481]\ttrain-rmse:0.11282\n",
      "[482]\ttrain-rmse:0.112793\n",
      "[483]\ttrain-rmse:0.112724\n",
      "[484]\ttrain-rmse:0.112689\n",
      "[485]\ttrain-rmse:0.112667\n",
      "[486]\ttrain-rmse:0.112644\n",
      "[487]\ttrain-rmse:0.112584\n",
      "[488]\ttrain-rmse:0.11253\n",
      "[489]\ttrain-rmse:0.112489\n",
      "[490]\ttrain-rmse:0.11247\n",
      "[491]\ttrain-rmse:0.112435\n",
      "[492]\ttrain-rmse:0.112408\n",
      "[493]\ttrain-rmse:0.112384\n",
      "[494]\ttrain-rmse:0.112367\n",
      "[495]\ttrain-rmse:0.112337\n",
      "[496]\ttrain-rmse:0.112317\n",
      "[497]\ttrain-rmse:0.11228\n",
      "[498]\ttrain-rmse:0.112234\n",
      "[499]\ttrain-rmse:0.112195\n",
      "[500]\ttrain-rmse:0.112157\n",
      "[501]\ttrain-rmse:0.11213\n",
      "[502]\ttrain-rmse:0.112108\n",
      "[503]\ttrain-rmse:0.112091\n",
      "[504]\ttrain-rmse:0.112029\n",
      "[505]\ttrain-rmse:0.111971\n",
      "[506]\ttrain-rmse:0.111938\n",
      "[507]\ttrain-rmse:0.111887\n",
      "[508]\ttrain-rmse:0.111844\n",
      "[509]\ttrain-rmse:0.111821\n",
      "[510]\ttrain-rmse:0.111771\n",
      "[511]\ttrain-rmse:0.111744\n",
      "[512]\ttrain-rmse:0.111681\n",
      "[513]\ttrain-rmse:0.111616\n",
      "[514]\ttrain-rmse:0.111603\n",
      "[515]\ttrain-rmse:0.111556\n",
      "[516]\ttrain-rmse:0.11153\n",
      "[517]\ttrain-rmse:0.111511\n",
      "[518]\ttrain-rmse:0.111483\n",
      "[519]\ttrain-rmse:0.111459\n",
      "[520]\ttrain-rmse:0.111382\n",
      "[521]\ttrain-rmse:0.111331\n",
      "[522]\ttrain-rmse:0.11129\n",
      "[523]\ttrain-rmse:0.111274\n",
      "[524]\ttrain-rmse:0.111233\n",
      "[525]\ttrain-rmse:0.111174\n",
      "[526]\ttrain-rmse:0.111163\n",
      "[527]\ttrain-rmse:0.111109\n",
      "[528]\ttrain-rmse:0.111084\n",
      "[529]\ttrain-rmse:0.111055\n",
      "[530]\ttrain-rmse:0.111047\n",
      "[531]\ttrain-rmse:0.111028\n",
      "[532]\ttrain-rmse:0.110955\n",
      "[533]\ttrain-rmse:0.110909\n",
      "[534]\ttrain-rmse:0.110885\n",
      "[535]\ttrain-rmse:0.110859\n",
      "[536]\ttrain-rmse:0.110808\n",
      "[537]\ttrain-rmse:0.110775\n",
      "[538]\ttrain-rmse:0.110758\n",
      "[539]\ttrain-rmse:0.110714\n",
      "[540]\ttrain-rmse:0.110647\n",
      "[541]\ttrain-rmse:0.11063\n",
      "[542]\ttrain-rmse:0.110578\n",
      "[543]\ttrain-rmse:0.110533\n",
      "[544]\ttrain-rmse:0.110516\n",
      "[545]\ttrain-rmse:0.110501\n",
      "[546]\ttrain-rmse:0.110488\n",
      "[547]\ttrain-rmse:0.110466\n",
      "[548]\ttrain-rmse:0.110447\n",
      "[549]\ttrain-rmse:0.110423\n",
      "[550]\ttrain-rmse:0.110407\n",
      "[551]\ttrain-rmse:0.110391\n",
      "[552]\ttrain-rmse:0.11036\n",
      "[553]\ttrain-rmse:0.110311\n",
      "[554]\ttrain-rmse:0.110287\n",
      "[555]\ttrain-rmse:0.110258\n",
      "[556]\ttrain-rmse:0.11018\n",
      "[557]\ttrain-rmse:0.110136\n",
      "[558]\ttrain-rmse:0.110121\n",
      "[559]\ttrain-rmse:0.110113\n",
      "[560]\ttrain-rmse:0.110091\n",
      "[561]\ttrain-rmse:0.110066\n",
      "[562]\ttrain-rmse:0.110048\n",
      "[563]\ttrain-rmse:0.11\n",
      "[564]\ttrain-rmse:0.10997\n",
      "[565]\ttrain-rmse:0.109943\n",
      "[566]\ttrain-rmse:0.109886\n",
      "[567]\ttrain-rmse:0.109857\n",
      "[568]\ttrain-rmse:0.109815\n",
      "[569]\ttrain-rmse:0.109775\n",
      "[570]\ttrain-rmse:0.109743\n",
      "[571]\ttrain-rmse:0.109715\n",
      "[572]\ttrain-rmse:0.109698\n",
      "[573]\ttrain-rmse:0.109662\n",
      "[574]\ttrain-rmse:0.109642\n",
      "[575]\ttrain-rmse:0.109626\n",
      "[576]\ttrain-rmse:0.109593\n",
      "[577]\ttrain-rmse:0.109577\n",
      "[578]\ttrain-rmse:0.109559\n",
      "[579]\ttrain-rmse:0.109537\n",
      "[580]\ttrain-rmse:0.109497\n",
      "[581]\ttrain-rmse:0.109453\n",
      "[582]\ttrain-rmse:0.109411\n",
      "[583]\ttrain-rmse:0.109401\n",
      "[584]\ttrain-rmse:0.109371\n",
      "[585]\ttrain-rmse:0.109341\n",
      "[586]\ttrain-rmse:0.109316\n",
      "[587]\ttrain-rmse:0.109258\n",
      "[588]\ttrain-rmse:0.109236\n",
      "[589]\ttrain-rmse:0.109222\n",
      "[590]\ttrain-rmse:0.109179\n",
      "[591]\ttrain-rmse:0.109149\n",
      "[592]\ttrain-rmse:0.109126\n",
      "[593]\ttrain-rmse:0.109087\n",
      "[594]\ttrain-rmse:0.109046\n",
      "[595]\ttrain-rmse:0.109023\n",
      "[596]\ttrain-rmse:0.108994\n",
      "[597]\ttrain-rmse:0.108954\n",
      "[598]\ttrain-rmse:0.108908\n",
      "[599]\ttrain-rmse:0.108877\n",
      "[600]\ttrain-rmse:0.108812\n",
      "[601]\ttrain-rmse:0.108747\n",
      "[602]\ttrain-rmse:0.108718\n",
      "[603]\ttrain-rmse:0.108694\n",
      "[604]\ttrain-rmse:0.108682\n",
      "[605]\ttrain-rmse:0.108657\n",
      "[606]\ttrain-rmse:0.108642\n",
      "[607]\ttrain-rmse:0.108629\n",
      "[608]\ttrain-rmse:0.108582\n",
      "[609]\ttrain-rmse:0.108559\n",
      "[610]\ttrain-rmse:0.108493\n",
      "[611]\ttrain-rmse:0.108459\n",
      "[612]\ttrain-rmse:0.108429\n",
      "[613]\ttrain-rmse:0.108383\n",
      "[614]\ttrain-rmse:0.108364\n",
      "[615]\ttrain-rmse:0.108339\n",
      "[616]\ttrain-rmse:0.108285\n",
      "[617]\ttrain-rmse:0.108264\n",
      "[618]\ttrain-rmse:0.108221\n",
      "[619]\ttrain-rmse:0.108204\n",
      "[620]\ttrain-rmse:0.108192\n",
      "[621]\ttrain-rmse:0.108177\n",
      "[622]\ttrain-rmse:0.108137\n",
      "[623]\ttrain-rmse:0.108117\n",
      "[624]\ttrain-rmse:0.108084\n",
      "[625]\ttrain-rmse:0.108071\n",
      "[626]\ttrain-rmse:0.108058\n",
      "[627]\ttrain-rmse:0.108049\n",
      "[628]\ttrain-rmse:0.108008\n",
      "[629]\ttrain-rmse:0.107994\n",
      "[630]\ttrain-rmse:0.107978\n",
      "[631]\ttrain-rmse:0.107918\n",
      "[632]\ttrain-rmse:0.107877\n",
      "[633]\ttrain-rmse:0.107844\n",
      "[634]\ttrain-rmse:0.10781\n",
      "[635]\ttrain-rmse:0.107772\n",
      "[636]\ttrain-rmse:0.107713\n",
      "[637]\ttrain-rmse:0.1077\n",
      "[638]\ttrain-rmse:0.107673\n",
      "[639]\ttrain-rmse:0.107658\n",
      "[640]\ttrain-rmse:0.107644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[641]\ttrain-rmse:0.10763\n",
      "[642]\ttrain-rmse:0.107605\n",
      "[643]\ttrain-rmse:0.107581\n",
      "[644]\ttrain-rmse:0.107567\n",
      "[645]\ttrain-rmse:0.107555\n",
      "[646]\ttrain-rmse:0.107543\n",
      "[647]\ttrain-rmse:0.107506\n",
      "[648]\ttrain-rmse:0.107481\n",
      "[649]\ttrain-rmse:0.107471\n",
      "[650]\ttrain-rmse:0.107445\n",
      "[651]\ttrain-rmse:0.107426\n",
      "[652]\ttrain-rmse:0.107394\n",
      "[653]\ttrain-rmse:0.107374\n",
      "[654]\ttrain-rmse:0.107362\n",
      "[655]\ttrain-rmse:0.107322\n",
      "[656]\ttrain-rmse:0.107305\n",
      "[657]\ttrain-rmse:0.107268\n",
      "[658]\ttrain-rmse:0.107224\n",
      "[659]\ttrain-rmse:0.107201\n",
      "[660]\ttrain-rmse:0.107184\n",
      "[661]\ttrain-rmse:0.107157\n",
      "[662]\ttrain-rmse:0.107136\n",
      "[663]\ttrain-rmse:0.107113\n",
      "[664]\ttrain-rmse:0.107102\n",
      "[665]\ttrain-rmse:0.107057\n",
      "[666]\ttrain-rmse:0.107034\n",
      "[667]\ttrain-rmse:0.10699\n",
      "[668]\ttrain-rmse:0.106963\n",
      "[669]\ttrain-rmse:0.106942\n",
      "[670]\ttrain-rmse:0.106927\n",
      "[671]\ttrain-rmse:0.106892\n",
      "[672]\ttrain-rmse:0.106883\n",
      "[673]\ttrain-rmse:0.106864\n",
      "[674]\ttrain-rmse:0.106847\n",
      "[675]\ttrain-rmse:0.106828\n",
      "[676]\ttrain-rmse:0.106815\n",
      "[677]\ttrain-rmse:0.106797\n",
      "[678]\ttrain-rmse:0.106761\n",
      "[679]\ttrain-rmse:0.106739\n",
      "[680]\ttrain-rmse:0.106726\n",
      "[681]\ttrain-rmse:0.106708\n",
      "[682]\ttrain-rmse:0.106681\n",
      "[683]\ttrain-rmse:0.106661\n",
      "[684]\ttrain-rmse:0.106635\n",
      "[685]\ttrain-rmse:0.106623\n",
      "[686]\ttrain-rmse:0.106591\n",
      "[687]\ttrain-rmse:0.106569\n",
      "[688]\ttrain-rmse:0.106545\n",
      "[689]\ttrain-rmse:0.106501\n",
      "[690]\ttrain-rmse:0.106491\n",
      "[691]\ttrain-rmse:0.106481\n",
      "[692]\ttrain-rmse:0.106458\n",
      "[693]\ttrain-rmse:0.106434\n",
      "[694]\ttrain-rmse:0.106415\n",
      "[695]\ttrain-rmse:0.106398\n",
      "[696]\ttrain-rmse:0.106356\n",
      "[697]\ttrain-rmse:0.106333\n",
      "[698]\ttrain-rmse:0.106313\n",
      "[699]\ttrain-rmse:0.106288\n",
      "[700]\ttrain-rmse:0.106268\n",
      "[701]\ttrain-rmse:0.106254\n",
      "[702]\ttrain-rmse:0.106222\n",
      "[703]\ttrain-rmse:0.106193\n",
      "[704]\ttrain-rmse:0.106172\n",
      "[705]\ttrain-rmse:0.106161\n",
      "[706]\ttrain-rmse:0.10613\n",
      "[707]\ttrain-rmse:0.10609\n",
      "[708]\ttrain-rmse:0.106046\n",
      "[709]\ttrain-rmse:0.106025\n",
      "[710]\ttrain-rmse:0.105993\n",
      "[711]\ttrain-rmse:0.105986\n",
      "[712]\ttrain-rmse:0.105974\n",
      "[713]\ttrain-rmse:0.105959\n",
      "[714]\ttrain-rmse:0.105946\n",
      "[715]\ttrain-rmse:0.105915\n",
      "[716]\ttrain-rmse:0.105893\n",
      "[717]\ttrain-rmse:0.105882\n",
      "[718]\ttrain-rmse:0.105856\n",
      "[719]\ttrain-rmse:0.105837\n",
      "[720]\ttrain-rmse:0.105806\n",
      "[721]\ttrain-rmse:0.105775\n",
      "[722]\ttrain-rmse:0.105746\n",
      "[723]\ttrain-rmse:0.105689\n",
      "[724]\ttrain-rmse:0.105677\n",
      "[725]\ttrain-rmse:0.105668\n",
      "[726]\ttrain-rmse:0.105652\n",
      "[727]\ttrain-rmse:0.105634\n",
      "[728]\ttrain-rmse:0.105617\n",
      "[729]\ttrain-rmse:0.105608\n",
      "[730]\ttrain-rmse:0.105578\n",
      "[731]\ttrain-rmse:0.105564\n",
      "[732]\ttrain-rmse:0.105539\n",
      "[733]\ttrain-rmse:0.10552\n",
      "[734]\ttrain-rmse:0.105492\n",
      "[735]\ttrain-rmse:0.105475\n",
      "[736]\ttrain-rmse:0.105457\n",
      "[737]\ttrain-rmse:0.105432\n",
      "[738]\ttrain-rmse:0.105426\n",
      "[739]\ttrain-rmse:0.1054\n",
      "[740]\ttrain-rmse:0.105383\n",
      "[741]\ttrain-rmse:0.105371\n",
      "[742]\ttrain-rmse:0.10536\n",
      "[743]\ttrain-rmse:0.105325\n",
      "[744]\ttrain-rmse:0.10527\n",
      "[745]\ttrain-rmse:0.105249\n",
      "[746]\ttrain-rmse:0.10523\n",
      "[747]\ttrain-rmse:0.105205\n",
      "[748]\ttrain-rmse:0.105176\n",
      "[749]\ttrain-rmse:0.105156\n",
      "[750]\ttrain-rmse:0.105143\n",
      "[751]\ttrain-rmse:0.105112\n",
      "[752]\ttrain-rmse:0.105096\n",
      "[753]\ttrain-rmse:0.10506\n",
      "[754]\ttrain-rmse:0.105018\n",
      "[755]\ttrain-rmse:0.105003\n",
      "[756]\ttrain-rmse:0.104988\n",
      "[757]\ttrain-rmse:0.104965\n",
      "[758]\ttrain-rmse:0.104933\n",
      "[759]\ttrain-rmse:0.104916\n",
      "[760]\ttrain-rmse:0.104894\n",
      "[761]\ttrain-rmse:0.104883\n",
      "[762]\ttrain-rmse:0.104866\n",
      "[763]\ttrain-rmse:0.104853\n",
      "[764]\ttrain-rmse:0.1048\n",
      "[765]\ttrain-rmse:0.104788\n",
      "[766]\ttrain-rmse:0.104774\n",
      "[767]\ttrain-rmse:0.104761\n",
      "[768]\ttrain-rmse:0.104756\n",
      "[769]\ttrain-rmse:0.104733\n",
      "[770]\ttrain-rmse:0.104715\n",
      "[771]\ttrain-rmse:0.104705\n",
      "[772]\ttrain-rmse:0.104686\n",
      "[773]\ttrain-rmse:0.104675\n",
      "[774]\ttrain-rmse:0.104664\n",
      "[775]\ttrain-rmse:0.104657\n",
      "[776]\ttrain-rmse:0.104649\n",
      "[777]\ttrain-rmse:0.104604\n",
      "[778]\ttrain-rmse:0.104594\n",
      "[779]\ttrain-rmse:0.104577\n",
      "[780]\ttrain-rmse:0.104566\n",
      "[781]\ttrain-rmse:0.104555\n",
      "[782]\ttrain-rmse:0.104544\n",
      "[783]\ttrain-rmse:0.104533\n",
      "[784]\ttrain-rmse:0.104512\n",
      "[785]\ttrain-rmse:0.104502\n",
      "[786]\ttrain-rmse:0.10449\n",
      "[787]\ttrain-rmse:0.104471\n",
      "[788]\ttrain-rmse:0.104465\n",
      "[789]\ttrain-rmse:0.104428\n",
      "[790]\ttrain-rmse:0.104412\n",
      "[791]\ttrain-rmse:0.104391\n",
      "[792]\ttrain-rmse:0.104331\n",
      "[793]\ttrain-rmse:0.104319\n",
      "[794]\ttrain-rmse:0.104294\n",
      "[795]\ttrain-rmse:0.104272\n",
      "[796]\ttrain-rmse:0.10426\n",
      "[797]\ttrain-rmse:0.104241\n",
      "[798]\ttrain-rmse:0.10423\n",
      "[799]\ttrain-rmse:0.104218\n",
      "[800]\ttrain-rmse:0.104209\n",
      "[801]\ttrain-rmse:0.104188\n",
      "[802]\ttrain-rmse:0.104166\n",
      "[803]\ttrain-rmse:0.104141\n",
      "[804]\ttrain-rmse:0.10413\n",
      "[805]\ttrain-rmse:0.104115\n",
      "[806]\ttrain-rmse:0.104106\n",
      "[807]\ttrain-rmse:0.1041\n",
      "[808]\ttrain-rmse:0.104094\n",
      "[809]\ttrain-rmse:0.104074\n",
      "[810]\ttrain-rmse:0.104062\n",
      "[811]\ttrain-rmse:0.104046\n",
      "[812]\ttrain-rmse:0.104029\n",
      "[813]\ttrain-rmse:0.104023\n",
      "[814]\ttrain-rmse:0.104005\n",
      "[815]\ttrain-rmse:0.103991\n",
      "[816]\ttrain-rmse:0.10398\n",
      "[817]\ttrain-rmse:0.103964\n",
      "[818]\ttrain-rmse:0.103947\n",
      "[819]\ttrain-rmse:0.103922\n",
      "[820]\ttrain-rmse:0.103907\n",
      "[821]\ttrain-rmse:0.103884\n",
      "[822]\ttrain-rmse:0.103874\n",
      "[823]\ttrain-rmse:0.103861\n",
      "[824]\ttrain-rmse:0.103841\n",
      "[825]\ttrain-rmse:0.103825\n",
      "[826]\ttrain-rmse:0.103801\n",
      "[827]\ttrain-rmse:0.103783\n",
      "[828]\ttrain-rmse:0.103774\n",
      "[829]\ttrain-rmse:0.103752\n",
      "[830]\ttrain-rmse:0.103739\n",
      "[831]\ttrain-rmse:0.10372\n",
      "[832]\ttrain-rmse:0.103711\n",
      "[833]\ttrain-rmse:0.103698\n",
      "[834]\ttrain-rmse:0.10369\n",
      "[835]\ttrain-rmse:0.10368\n",
      "[836]\ttrain-rmse:0.103665\n",
      "[837]\ttrain-rmse:0.103646\n",
      "[838]\ttrain-rmse:0.103634\n",
      "[839]\ttrain-rmse:0.103616\n",
      "[840]\ttrain-rmse:0.10359\n",
      "[841]\ttrain-rmse:0.103571\n",
      "[842]\ttrain-rmse:0.103558\n",
      "[843]\ttrain-rmse:0.103549\n",
      "[844]\ttrain-rmse:0.103528\n",
      "[845]\ttrain-rmse:0.103501\n",
      "[846]\ttrain-rmse:0.103477\n",
      "[847]\ttrain-rmse:0.103465\n",
      "[848]\ttrain-rmse:0.10345\n",
      "[849]\ttrain-rmse:0.103421\n",
      "[850]\ttrain-rmse:0.103414\n",
      "[851]\ttrain-rmse:0.103405\n",
      "[852]\ttrain-rmse:0.10339\n",
      "[853]\ttrain-rmse:0.103368\n",
      "[854]\ttrain-rmse:0.103362\n",
      "[855]\ttrain-rmse:0.10335\n",
      "[856]\ttrain-rmse:0.103339\n",
      "[857]\ttrain-rmse:0.103331\n",
      "[858]\ttrain-rmse:0.103312\n",
      "[859]\ttrain-rmse:0.103305\n",
      "[860]\ttrain-rmse:0.103292\n",
      "[861]\ttrain-rmse:0.103283\n",
      "[862]\ttrain-rmse:0.103275\n",
      "[863]\ttrain-rmse:0.103253\n",
      "[864]\ttrain-rmse:0.103247\n",
      "[865]\ttrain-rmse:0.103221\n",
      "[866]\ttrain-rmse:0.103196\n",
      "[867]\ttrain-rmse:0.103182\n",
      "[868]\ttrain-rmse:0.103134\n",
      "[869]\ttrain-rmse:0.103106\n",
      "[870]\ttrain-rmse:0.103077\n",
      "[871]\ttrain-rmse:0.103065\n",
      "[872]\ttrain-rmse:0.103039\n",
      "[873]\ttrain-rmse:0.103027\n",
      "[874]\ttrain-rmse:0.103017\n",
      "[875]\ttrain-rmse:0.103004\n",
      "[876]\ttrain-rmse:0.102989\n",
      "[877]\ttrain-rmse:0.102947\n",
      "[878]\ttrain-rmse:0.102922\n",
      "[879]\ttrain-rmse:0.102907\n",
      "[880]\ttrain-rmse:0.102898\n",
      "[881]\ttrain-rmse:0.102884\n",
      "[882]\ttrain-rmse:0.102846\n",
      "[883]\ttrain-rmse:0.102825\n",
      "[884]\ttrain-rmse:0.102802\n",
      "[885]\ttrain-rmse:0.102794\n",
      "[886]\ttrain-rmse:0.102768\n",
      "[887]\ttrain-rmse:0.102753\n",
      "[888]\ttrain-rmse:0.102737\n",
      "[889]\ttrain-rmse:0.102715\n",
      "[890]\ttrain-rmse:0.1027\n",
      "[891]\ttrain-rmse:0.102687\n",
      "[892]\ttrain-rmse:0.102629\n",
      "[893]\ttrain-rmse:0.102606\n",
      "[894]\ttrain-rmse:0.102588\n",
      "[895]\ttrain-rmse:0.102584\n",
      "[896]\ttrain-rmse:0.102578\n",
      "[897]\ttrain-rmse:0.102542\n",
      "[898]\ttrain-rmse:0.102527\n",
      "[899]\ttrain-rmse:0.102514\n",
      "[900]\ttrain-rmse:0.102501\n",
      "[901]\ttrain-rmse:0.102473\n",
      "[902]\ttrain-rmse:0.102457\n",
      "[903]\ttrain-rmse:0.102443\n",
      "[904]\ttrain-rmse:0.102432\n",
      "[905]\ttrain-rmse:0.10242\n",
      "[906]\ttrain-rmse:0.102413\n",
      "[907]\ttrain-rmse:0.102406\n",
      "[908]\ttrain-rmse:0.102391\n",
      "[909]\ttrain-rmse:0.102382\n",
      "[910]\ttrain-rmse:0.102373\n",
      "[911]\ttrain-rmse:0.102369\n",
      "[912]\ttrain-rmse:0.102354\n",
      "[913]\ttrain-rmse:0.102334\n",
      "[914]\ttrain-rmse:0.102326\n",
      "[915]\ttrain-rmse:0.102319\n",
      "[916]\ttrain-rmse:0.102301\n",
      "[917]\ttrain-rmse:0.102292\n",
      "[918]\ttrain-rmse:0.10228\n",
      "[919]\ttrain-rmse:0.102274\n",
      "[920]\ttrain-rmse:0.10227\n",
      "[921]\ttrain-rmse:0.102238\n",
      "[922]\ttrain-rmse:0.102226\n",
      "[923]\ttrain-rmse:0.102222\n",
      "[924]\ttrain-rmse:0.10221\n",
      "[925]\ttrain-rmse:0.102194\n",
      "[926]\ttrain-rmse:0.102172\n",
      "[927]\ttrain-rmse:0.102164\n",
      "[928]\ttrain-rmse:0.102156\n",
      "[929]\ttrain-rmse:0.102144\n",
      "[930]\ttrain-rmse:0.102133\n",
      "[931]\ttrain-rmse:0.102107\n",
      "[932]\ttrain-rmse:0.102085\n",
      "[933]\ttrain-rmse:0.102071\n",
      "[934]\ttrain-rmse:0.102061\n",
      "[935]\ttrain-rmse:0.102045\n",
      "[936]\ttrain-rmse:0.10203\n",
      "[937]\ttrain-rmse:0.102019\n",
      "[938]\ttrain-rmse:0.102007\n",
      "[939]\ttrain-rmse:0.101993\n",
      "[940]\ttrain-rmse:0.101974\n",
      "[941]\ttrain-rmse:0.10196\n",
      "[942]\ttrain-rmse:0.101941\n",
      "[943]\ttrain-rmse:0.10191\n",
      "[944]\ttrain-rmse:0.101905\n",
      "[945]\ttrain-rmse:0.101885\n",
      "[946]\ttrain-rmse:0.101875\n",
      "[947]\ttrain-rmse:0.101861\n",
      "[948]\ttrain-rmse:0.101851\n",
      "[949]\ttrain-rmse:0.101833\n",
      "[950]\ttrain-rmse:0.101817\n",
      "[951]\ttrain-rmse:0.101803\n",
      "[952]\ttrain-rmse:0.101791\n",
      "[953]\ttrain-rmse:0.101779\n",
      "[954]\ttrain-rmse:0.101769\n",
      "[955]\ttrain-rmse:0.101759\n",
      "[956]\ttrain-rmse:0.101749\n",
      "[957]\ttrain-rmse:0.101727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[958]\ttrain-rmse:0.101718\n",
      "[959]\ttrain-rmse:0.101702\n",
      "[960]\ttrain-rmse:0.101693\n",
      "[961]\ttrain-rmse:0.101687\n",
      "[962]\ttrain-rmse:0.101678\n",
      "[963]\ttrain-rmse:0.10167\n",
      "[964]\ttrain-rmse:0.101658\n",
      "[965]\ttrain-rmse:0.101643\n",
      "[966]\ttrain-rmse:0.101625\n",
      "[967]\ttrain-rmse:0.101614\n",
      "[968]\ttrain-rmse:0.101587\n",
      "[969]\ttrain-rmse:0.101555\n",
      "[970]\ttrain-rmse:0.101549\n",
      "[971]\ttrain-rmse:0.101541\n",
      "[972]\ttrain-rmse:0.101515\n",
      "[973]\ttrain-rmse:0.101489\n",
      "[974]\ttrain-rmse:0.101481\n",
      "[975]\ttrain-rmse:0.101458\n",
      "[976]\ttrain-rmse:0.101453\n",
      "[977]\ttrain-rmse:0.101444\n",
      "[978]\ttrain-rmse:0.101438\n",
      "[979]\ttrain-rmse:0.101429\n",
      "[980]\ttrain-rmse:0.101413\n",
      "[981]\ttrain-rmse:0.101398\n",
      "[982]\ttrain-rmse:0.101379\n",
      "[983]\ttrain-rmse:0.101374\n",
      "[984]\ttrain-rmse:0.101366\n",
      "[985]\ttrain-rmse:0.101357\n",
      "[986]\ttrain-rmse:0.101344\n",
      "[987]\ttrain-rmse:0.101332\n",
      "[988]\ttrain-rmse:0.10132\n",
      "[989]\ttrain-rmse:0.101304\n",
      "[990]\ttrain-rmse:0.101298\n",
      "[991]\ttrain-rmse:0.101279\n",
      "[992]\ttrain-rmse:0.101265\n",
      "[993]\ttrain-rmse:0.101255\n",
      "[994]\ttrain-rmse:0.101247\n",
      "[995]\ttrain-rmse:0.101237\n",
      "[996]\ttrain-rmse:0.101223\n",
      "[997]\ttrain-rmse:0.10121\n",
      "[998]\ttrain-rmse:0.101187\n",
      "[999]\ttrain-rmse:0.101179\n",
      "Result on validation data:  0.09668812940871037\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "print(\"Fitting XGBoost...\")\n",
    "models.append(XGBoost(X_train, y_train, X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3）评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate combined models...\n",
      "Training error...\n",
      "0.07297976184932689\n",
      "Validation error...\n",
      "0.09668812940871037\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X_val, y_val)\n",
    "print(r_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
