{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 章 用Embedding提升机器学习性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 项目概述\n",
    "\t采用Embedding提升模型性能，对传统机器学习算法，如典型的XGBoost算法，采用相同数据集，相同算法，但对输入数据预处理不同，一种是通常处理方法，另一种应用Embedding。同样对神经网络也进行比较，使用相同模型结构，但输入数据采用不同策略，一种是对分类特征进行One-hot编码，另一种是采用Embedding处理。这些方法比较结果如下表所示\n",
    " \n",
    "|算法|RMSE|RMSE(with EE)|\n",
    "|:-|:-|:-|\n",
    "|XGBoost|0.176|\t0.098|\n",
    "|NN|0.101|0.095|\n",
    "\n",
    "由上表可以看出，神经网络NN的性能优于传统机器学习，使用EE（EntityEmbedding）的算法优于不使用EE的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.1 数据集简介\n",
    "\t本章使用的数据集为1115家德国罗斯曼商店的历史销售数据。任务是预测测试集的“销售”列。数据集中的某些商店已暂时关闭以进行翻新。涉及数据文件如下：  \n",
    "\train.csv：包括销售在内的历史数据；  \n",
    "\ttest.csv：历史数据（不包括销售）；  \n",
    "\tsample_submission.csv：格式正确的样本提交文件；  \n",
    "\tstore.csv：有关商店的补充信息。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.2 导入数据\n",
    "1）导入需要的库  \n",
    "导入train.csv,store.csv,store_states.csv等文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  #把内存信息序列化写入磁盘\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）定义两个函数，csv2dicts用于把csv文件转换为字典，set_nan_as_string函数把空值用'0'填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把csv文件转换为字典\n",
    "def csv2dicts(csvfile):\n",
    "    data = []\n",
    "    keys = []\n",
    "    for row_index, row in enumerate(csvfile):\n",
    "        #把第一行标题打印出来\n",
    "        if row_index == 0:\n",
    "            keys = row\n",
    "            print(row)\n",
    "            continue\n",
    "        \n",
    "        data.append({key: value for key, value in zip(keys, row)})\n",
    "    return data\n",
    "\n",
    "#如果值为空，则用'0'填充\n",
    "def set_nan_as_string(data, replace_str='0'):\n",
    "    for i, x in enumerate(data):\n",
    "        for key, value in x.items():\n",
    "            if value == '':\n",
    "                x[key] = replace_str\n",
    "        data[i] = x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3）导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday']\n",
      "[{'Store': '1115', 'DayOfWeek': '2', 'Date': '2013-01-01', 'Sales': '0', 'Customers': '0', 'Open': '0', 'Promo': '0', 'StateHoliday': 'a', 'SchoolHoliday': '1'}, {'Store': '1114', 'DayOfWeek': '2', 'Date': '2013-01-01', 'Sales': '0', 'Customers': '0', 'Open': '0', 'Promo': '0', 'StateHoliday': 'a', 'SchoolHoliday': '1'}, {'Store': '1113', 'DayOfWeek': '2', 'Date': '2013-01-01', 'Sales': '0', 'Customers': '0', 'Open': '0', 'Promo': '0', 'StateHoliday': 'a', 'SchoolHoliday': '1'}]\n"
     ]
    }
   ],
   "source": [
    "train_data = r\".\\data\\train.csv\"\n",
    "store_data = r\".\\data\\store.csv\"\n",
    "store_states = r'.\\data\\store_states.csv'\n",
    "\n",
    "#把处理后的训练数据写入文件\n",
    "with open(train_data) as csvfile:\n",
    "    data = csv.reader(csvfile, delimiter=',')\n",
    "    with open('train_data.pickle', 'wb') as f:\n",
    "        data = csv2dicts(data)\n",
    "        #头尾倒过来\n",
    "        data = data[::-1]\n",
    "        #序列化，把数据保存到文件中\n",
    "        pickle.dump(data, f, -1)\n",
    "        print(data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4）处理store_data，store_states数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store', 'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
      "['Store', 'State']\n",
      "[{'Store': '1', 'StoreType': 'c', 'Assortment': 'a', 'CompetitionDistance': '1270', 'CompetitionOpenSinceMonth': '9', 'CompetitionOpenSinceYear': '2008', 'Promo2': '0', 'Promo2SinceWeek': '0', 'Promo2SinceYear': '0', 'PromoInterval': '0', 'State': 'HE'}, {'Store': '2', 'StoreType': 'a', 'Assortment': 'a', 'CompetitionDistance': '570', 'CompetitionOpenSinceMonth': '11', 'CompetitionOpenSinceYear': '2007', 'Promo2': '1', 'Promo2SinceWeek': '13', 'Promo2SinceYear': '2010', 'PromoInterval': 'Jan,Apr,Jul,Oct', 'State': 'TH'}]\n"
     ]
    }
   ],
   "source": [
    "#把处理后的store_data，store_states数据写入文件store_data.pickle\n",
    "with open(store_data) as csvfile, open(store_states) as csvfile2:\n",
    "    data = csv.reader(csvfile, delimiter=',')\n",
    "    state_data = csv.reader(csvfile2, delimiter=',')\n",
    "    with open('store_data.pickle', 'wb') as f:\n",
    "        data = csv2dicts(data)\n",
    "        state_data = csv2dicts(state_data)\n",
    "        set_nan_as_string(data)\n",
    "        #把state加到store_data数据集中，然后保存生成的数据        \n",
    "        for index, val in enumerate(data):\n",
    "            state = state_data[index]\n",
    "            val['State'] = state['State']\n",
    "            data[index] = val\n",
    "        pickle.dump(data, f, -1)\n",
    "        print(data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.3 预处理数据\n",
    "1）导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）使用pickle读取pickle文件数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取pickle文件\n",
    "with open('train_data.pickle', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    num_records = len(train_data)\n",
    "with open('store_data.pickle', 'rb') as f:\n",
    "    store_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3）对销售时间字段进行拆分和转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对时间特征进行拆分和转换，是否促销promo等特征转换为整数\n",
    "def feature_list(record):\n",
    "    dt = datetime.strptime(record['Date'], '%Y-%m-%d')\n",
    "    store_index = int(record['Store'])\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    day = dt.day\n",
    "    day_of_week = int(record['DayOfWeek'])\n",
    "    try:\n",
    "        store_open = int(record['Open'])\n",
    "    except:\n",
    "        store_open = 1\n",
    "\n",
    "    promo = int(record['Promo'])\n",
    "    #同时返回state对应的简称\n",
    "    return [store_open,\n",
    "            store_index,\n",
    "            day_of_week,\n",
    "            promo,\n",
    "            year,\n",
    "            month,\n",
    "            day,\n",
    "            store_data[store_index - 1]['State']\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4）对train_data进行一些简单清理或过滤操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "销售记录数:  844338\n",
      "最小销售量:46，最大销售量:41551\n"
     ]
    }
   ],
   "source": [
    "#生成训练数据\n",
    "train_data_X = []\n",
    "train_data_y = []\n",
    "\n",
    "for record in train_data:\n",
    "    if record['Sales'] != '0' and record['Open'] != '':\n",
    "        fl = feature_list(record)\n",
    "        train_data_X.append(fl)\n",
    "        train_data_y.append(int(record['Sales']))\n",
    "print(\"销售记录数: \", len(train_data_y))\n",
    "\n",
    "print(\"最小销售量:{}，最大销售量:{}\".format(min(train_data_y), max(train_data_y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5）数值化各特征，把结果保存到文件feature_train_data.pickle中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 109   1   0   0   0   0   7] 5961\n"
     ]
    }
   ],
   "source": [
    "full_X = np.array(train_data_X)\n",
    "#full_X = np.array(full_X)\n",
    "train_data_X = np.array(train_data_X)\n",
    "les = []\n",
    "#对每列进行处理，把类别转换为数值\n",
    "for i in range(train_data_X.shape[1]):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(full_X[:, i])\n",
    "    les.append(le)\n",
    "    train_data_X[:, i] = le.transform(train_data_X[:, i])\n",
    "\n",
    "#处理后的数据写入pickle文件\n",
    "with open('les.pickle', 'wb') as f:\n",
    "    pickle.dump(les, f, -1)\n",
    "\n",
    "#把训练数据转换为整数\n",
    "train_data_X = train_data_X.astype(int)\n",
    "train_data_y = np.array(train_data_y)\n",
    "\n",
    "#保存数据到feature_train_data.pickle文件\n",
    "with open('feature_train_data.pickle', 'wb') as f:\n",
    "    pickle.dump((train_data_X, train_data_y), f, -1)\n",
    "    print(train_data_X[0], train_data_y[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.4 定义公共函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要定义公共函数，主要分为以下几个步骤。  \n",
    "1）首先导入必要的库或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pickle  \n",
    "numpy.random.seed(123)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Reshape,Flatten\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#屏蔽警告信息\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）设置一些超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "shuffle_data = False\n",
    "one_hot_as_input = False\n",
    "embeddings_as_input = False\n",
    "save_embeddings = True\n",
    "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3）定义几个公共函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('feature_train_data.pickle', 'rb')\n",
    "(X, y) = pickle.load(f)\n",
    "\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)\n",
    "\n",
    "if shuffle_data:\n",
    "    print(\"Using shuffled data\")\n",
    "    sh = numpy.arange(X.shape[0])\n",
    "    numpy.random.shuffle(sh)\n",
    "    X = X[sh]\n",
    "    y = y[sh]\n",
    "\n",
    "if embeddings_as_input:\n",
    "    print(\"Using learned embeddings as input\")\n",
    "    X = embed_features(X, saved_embeddings_fname)\n",
    "\n",
    "if one_hot_as_input:\n",
    "    print(\"Using one-hot encoding as input\")\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X)\n",
    "\n",
    "def sample(X, y, n):\n",
    "    '''random samples'''\n",
    "    num_row = X.shape[0]\n",
    "    indices = numpy.random.randint(num_row, size=n)\n",
    "    return X[indices, :], y[indices]\n",
    "\n",
    "def evaluate_models(models, X, y):\n",
    "    assert(min(y) > 0)\n",
    "    guessed_sales = numpy.array([model.guess(X) for model in models])\n",
    "    mean_sales = guessed_sales.mean(axis=0)\n",
    "    relative_err = numpy.absolute((y - mean_sales) / y)\n",
    "    result = numpy.sum(relative_err) / len(y)\n",
    "    return result\n",
    "\n",
    "#分别取出各特征,取出X中前8列数据，除第1列，\n",
    "def split_features(X):\n",
    "    X_list = []\n",
    "    #获取X第2列数据\n",
    "    store_index = X[..., [1]]\n",
    "    X_list.append(store_index)\n",
    "    #获取X第3列数据,以下类推\n",
    "    day_of_week = X[..., [2]]\n",
    "    X_list.append(day_of_week)\n",
    "\n",
    "    promo = X[..., [3]]\n",
    "    X_list.append(promo)\n",
    "\n",
    "    year = X[..., [4]]\n",
    "    X_list.append(year)\n",
    "\n",
    "    month = X[..., [5]]\n",
    "    X_list.append(month)\n",
    "\n",
    "    day = X[..., [6]]\n",
    "    X_list.append(day)\n",
    "\n",
    "    State = X[..., [7]]\n",
    "    X_list.append(State)\n",
    "\n",
    "    return X_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
