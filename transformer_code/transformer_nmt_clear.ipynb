{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "\n",
    "# Transformer代码实现\n",
    "\n",
    "参考：\n",
    "\n",
    "[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer)\n",
    "\n",
    "[PyTorch官方教程：Sequence-to-Sequence Modeling with nn.Transformer and TorchText](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "\n",
    "[TF官方教程：理解语言的 Transformer 模型](https://tensorflow.google.cn/tutorials/text/transformer)\n",
    "\n",
    "## 目录\n",
    "\n",
    "1. 数据预处理\n",
    "\n",
    "2. Transformer模型概况\n",
    "\n",
    "3. 位置编码（positional encoding）\n",
    "\n",
    "4. 编码器（encoder）\n",
    "\n",
    "    * 自注意力机制（self attention）\n",
    "\n",
    "    * 层归一化（layer normalization）、残差连接\n",
    "    \n",
    "    * 前馈网络（feed forward）\n",
    "\n",
    "    * 编码器整体结构\n",
    "    \n",
    "5. 解码器（decoder）\n",
    "\n",
    "6. 注意力掩码机制\n",
    "\n",
    "7. Transformer模型\n",
    "\n",
    "8. 模型训练\n",
    "\n",
    "9. 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import Counter\n",
    "from langconv import Converter\n",
    "from nltk import word_tokenize\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化参数设置\n",
    "PAD = 0                             # padding占位符的索引\n",
    "UNK = 1                             # 未登录词标识符的索引\n",
    "BATCH_SIZE = 128                    # 批次大小\n",
    "EPOCHS = 20                         # 训练轮数\n",
    "LAYERS = 6                          # transformer中encoder、decoder层数\n",
    "H_NUM = 8                           # 多头注意力个数\n",
    "D_MODEL = 256                       # 输入、输出词向量维数\n",
    "D_FF = 1024                         # feed forward全连接层维数\n",
    "DROPOUT = 0.1                       # dropout比例\n",
    "MAX_LENGTH = 60                     # 语句最大长度\n",
    "\n",
    "TRAIN_FILE = 'nmt/en-cn/train.txt'  # 训练集\n",
    "DEV_FILE = \"nmt/en-cn/dev.txt\"      # 验证集\n",
    "SAVE_FILE = 'save/model.pt'         # 模型保存路径\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据预处理\n",
    "\n",
    "注意：使用中文语料训练Transformer模型时，中文语句一般**以字为单位进行切分**，即无需对中文语句分词。\n",
    "\n",
    "注意：**同一批次中seq_len相同，不同批次间seq_len可能变化。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=PAD):\n",
    "    \"\"\"\n",
    "    按批次（batch）对数据填充、长度对齐\n",
    "    \"\"\"\n",
    "    # 计算该批次各条样本语句长度\n",
    "    L = [len(x) for x in X]\n",
    "    # 获取该批次样本中语句长度最大值\n",
    "    ML = max(L)\n",
    "    # 遍历该批次样本，如果语句长度小于最大长度，则用padding填充\n",
    "    return np.array([\n",
    "        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n",
    "    ])\n",
    "\n",
    "def cht_to_chs(sent):\n",
    "    sent = Converter(\"zh-hans\").convert(sent)\n",
    "    sent.encode(\"utf-8\")\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    def __init__(self, train_file, dev_file):\n",
    "        # 读取数据、分词\n",
    "        self.train_en, self.train_cn = self.load_data(train_file)\n",
    "        self.dev_en, self.dev_cn = self.load_data(dev_file)\n",
    "        # 构建词表\n",
    "        self.en_word_dict, self.en_total_words, self.en_index_dict = \\\n",
    "            self.build_dict(self.train_en)\n",
    "        self.cn_word_dict, self.cn_total_words, self.cn_index_dict = \\\n",
    "            self.build_dict(self.train_cn)\n",
    "        # 单词映射为索引\n",
    "        self.train_en, self.train_cn = self.word2id(self.train_en, self.train_cn, self.en_word_dict, self.cn_word_dict)\n",
    "        self.dev_en, self.dev_cn = self.word2id(self.dev_en, self.dev_cn, self.en_word_dict, self.cn_word_dict)\n",
    "        # 划分批次、填充、掩码\n",
    "        self.train_data = self.split_batch(self.train_en, self.train_cn, BATCH_SIZE)\n",
    "        self.dev_data = self.split_batch(self.dev_en, self.dev_cn, BATCH_SIZE)\n",
    "\n",
    "    def load_data(self, path):\n",
    "        \"\"\"\n",
    "        读取英文、中文数据\n",
    "        对每条样本分词并构建包含起始符和终止符的单词列表\n",
    "        形式如：en = [['BOS', 'i', 'love', 'you', 'EOS'], ['BOS', 'me', 'too', 'EOS'], ...]\n",
    "                cn = [['BOS', '我', '爱', '你', 'EOS'], ['BOS', '我', '也', '是', 'EOS'], ...]\n",
    "        \"\"\"\n",
    "        en = []\n",
    "        cn = []\n",
    "        with open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f.readlines():\n",
    "                sent_en, sent_cn = line.strip().split(\"\\t\")\n",
    "                sent_en = sent_en.lower()\n",
    "                sent_cn = cht_to_chs(sent_cn)\n",
    "                sent_en = [\"BOS\"] + word_tokenize(sent_en) + [\"EOS\"]\n",
    "                # 中文按字符切分\n",
    "                sent_cn = [\"BOS\"] + [char for char in sent_cn] + [\"EOS\"]\n",
    "                en.append(sent_en)\n",
    "                cn.append(sent_cn)\n",
    "        return en, cn\n",
    "    \n",
    "    def build_dict(self, sentences, max_words=5e4):\n",
    "        \"\"\"\n",
    "        构造分词后的列表数据\n",
    "        构建单词-索引映射（key为单词，value为id值）\n",
    "        \"\"\"\n",
    "        # 统计数据集中单词词频\n",
    "        word_count = Counter([word for sent in sentences for word in sent])\n",
    "        # 按词频保留前max_words个单词构建词典\n",
    "        # 添加UNK和PAD两个单词\n",
    "        ls = word_count.most_common(int(max_words))\n",
    "        total_words = len(ls) + 2\n",
    "        word_dict = {w[0]: index + 2 for index, w in enumerate(ls)}\n",
    "        word_dict['UNK'] = UNK\n",
    "        word_dict['PAD'] = PAD\n",
    "        # 构建id2word映射\n",
    "        index_dict = {v: k for k, v in word_dict.items()}\n",
    "        return word_dict, total_words, index_dict\n",
    "\n",
    "    def word2id(self, en, cn, en_dict, cn_dict, sort=True):\n",
    "        \"\"\"\n",
    "        将英文、中文单词列表转为单词索引列表\n",
    "        `sort=True`表示以英文语句长度排序，以便按批次填充时，同批次语句填充尽量少\n",
    "        \"\"\"\n",
    "        length = len(en)\n",
    "        # 单词映射为索引\n",
    "        out_en_ids = [[en_dict.get(word, UNK) for word in sent] for sent in en]\n",
    "        out_cn_ids = [[cn_dict.get(word, UNK) for word in sent] for sent in cn]\n",
    "        # 按照语句长度排序\n",
    "        def len_argsort(seq):\n",
    "            \"\"\"\n",
    "            传入一系列语句数据(分好词的列表形式)，\n",
    "            按照语句长度排序后，返回排序后原来各语句在数据中的索引下标\n",
    "            \"\"\"\n",
    "            return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "        # 按相同顺序对中文、英文样本排序\n",
    "        if sort:\n",
    "            # 以英文语句长度排序\n",
    "            sorted_index = len_argsort(out_en_ids)\n",
    "            out_en_ids = [out_en_ids[idx] for idx in sorted_index]\n",
    "            out_cn_ids = [out_cn_ids[idx] for idx in sorted_index]\n",
    "        return out_en_ids, out_cn_ids\n",
    "\n",
    "    def split_batch(self, en, cn, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        划分批次\n",
    "        `shuffle=True`表示对各批次顺序随机打乱\n",
    "        \"\"\"\n",
    "        # 每隔batch_size取一个索引作为后续batch的起始索引\n",
    "        idx_list = np.arange(0, len(en), batch_size)\n",
    "        # 起始索引随机打乱\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx_list)\n",
    "        # 存放所有批次的语句索引\n",
    "        batch_indexs = []\n",
    "        for idx in idx_list:\n",
    "            \"\"\"\n",
    "            形如[array([4, 5, 6, 7]), \n",
    "                 array([0, 1, 2, 3]), \n",
    "                 array([8, 9, 10, 11]),\n",
    "                 ...]\n",
    "            \"\"\"\n",
    "            # 起始索引最大的批次可能发生越界，要限定其索引\n",
    "            batch_indexs.append(np.arange(idx, min(idx + batch_size, len(en))))\n",
    "        # 构建批次列表\n",
    "        batches = []\n",
    "        for batch_index in batch_indexs:\n",
    "            # 按当前批次的样本索引采样\n",
    "            batch_en = [en[index] for index in batch_index]\n",
    "            batch_cn = [cn[index] for index in batch_index]\n",
    "            # 对当前批次中所有语句填充、对齐长度\n",
    "            # 维度为：batch_size * 当前批次中语句的最大长度\n",
    "            batch_cn = seq_padding(batch_cn)\n",
    "            batch_en = seq_padding(batch_en)\n",
    "            # 将当前批次添加到批次列表\n",
    "            # Batch类用于实现注意力掩码\n",
    "            batches.append(Batch(batch_en, batch_cn))\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 示例\n",
    "# data_mini = PrepareData(\"./nmt/en-cn/train_mini.txt\", \"./nmt/en-cn/dev_mini.txt\")\n",
    "# print(\"*** batch 0, en ***\")\n",
    "# print(data_mini.train_data[0][0])\n",
    "# print(\"*** batch 0, chs ***\")\n",
    "# print(data_mini.train_data[0][1])\n",
    "# print(\"*** en word to index ***\")\n",
    "# print(data_mini.en_word_dict)\n",
    "# print(\"*** chs word to index ***\")\n",
    "# print(data_mini.cn_word_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer模型概况\n",
    "  \n",
    "**Transformer**和**LSTM**的最大区别：LSTM采用迭代（自回归）训练方式，而Transformer可以并行训练，极大加快计算效率。Transformer使用位置编码（positional encoding）理解语言的顺序，使用自注意力机制和全连接层前向计算。   \n",
    "  \n",
    "Transformer模型由**编码器**和**解码器**组成，\n",
    "\n",
    "* **编码器（encoder）**\n",
    "\n",
    "将自然语言序列映射成为**隐藏层表示**（下图中九宫格），即自然语言序列的数学表达\n",
    "\n",
    "* **解码器（decoder）**\n",
    "\n",
    "将隐藏层表示映射为自然语言序列，进而解决各种任务，如情感分类、命名实体识别、语义关系抽取、摘要生成、机器翻译等。\n",
    "\n",
    "![](./img/transformer.jpg)\n",
    "\n",
    "![](./img/intuition.jpg)\n",
    "\n",
    "* **词向量（embedding）**  \n",
    "  \n",
    "使用可学习的词向量表（input & output embeddings）将输入、输出索引映射为$d_{model}$-维词向量。词向量表可随机初始化，也可加载预训练词向量，如word2vec、glove等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        # Embedding层\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        # Embedding维数\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 返回x的词向量（需要乘以math.sqrt(d_model)）\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 位置编码\n",
    "\n",
    "由于训练Transformer模型时，序列输入采用并行方式，因此缺少单词的位置信息，通过在Transformer的输入中加入单词位置编码信息，使Transformer能够识别语句中单词的位置关系。   \n",
    "  \n",
    "**位置编码（positional encoding）**：位置编码向量与词向量维度相同，$\\text{max_seq_len} \\times \\text{embedding_dim}$。   \n",
    "\n",
    "  \n",
    "Transformer原文中使用正、余弦函数的线性变换对单词位置编码：   \n",
    "  \n",
    "$$\\text{PE}_{pos, 2i} = \\sin \\left( \\frac{pos}{10000^{2i / d_{\\text{model}}}} \\right) \\\\ \n",
    "\\text{PE}_{(pos,2i + 1)} = \\cos \\left( \\frac{pos}{10000^{2i / d_{\\text{model}}}} \\right)\\tag{eq.1}$$  \n",
    "  \n",
    "其中，$pos \\in [0, \\text{max_seq_len})$表示单词在语句中的位置，$i \\in [0, \\text{embedding_dim})$表示词向量维度。位置编码函数的波长在$[2 \\pi, 10000 \\times 2 \\pi]$区间内变化，语句中每一个单词位置沿词向量维度由周期不同的正、余弦函数交替取值组合，生成独一纹理信息，从而使模型学到位置间的依赖关系和自然语言的时序特性。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 位置编码矩阵，维度[max_len, embedding_dim]\n",
    "        pe = torch.zeros(max_len, d_model, device=DEVICE)\n",
    "        # 单词位置\n",
    "        position = torch.arange(0.0, max_len, device=DEVICE)\n",
    "        position.unsqueeze_(1)\n",
    "        # 使用exp和log实现幂运算\n",
    "        div_term = torch.exp(torch.arange(0.0, d_model, 2, device=DEVICE) * (- math.log(1e4) / d_model))\n",
    "        div_term.unsqueeze_(0)\n",
    "        # 计算单词位置沿词向量维度的纹理值\n",
    "        pe[:, 0 : : 2] = torch.sin(torch.mm(position, div_term))\n",
    "        pe[:, 1 : : 2] = torch.cos(torch.mm(position, div_term))\n",
    "        # 增加批次维度，[1, max_len, embedding_dim]\n",
    "        pe.unsqueeze_(0)\n",
    "        # 将位置编码矩阵注册为buffer(不参加训练)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将一个批次中语句所有词向量与位置编码相加\n",
    "        # 注意，位置编码不参与训练，因此设置requires_grad=False\n",
    "        x += Variable(self.pe[:, : x.size(1), :], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emb_dim = 64\n",
    "max_seq_len = 100\n",
    "seq_len = 20\n",
    "\n",
    "pe = PositionalEncoding(emb_dim, 0, max_seq_len)\n",
    "positional_encoding = pe(torch.zeros(1, seq_len, emb_dim, device=DEVICE))\n",
    "plt.figure()\n",
    "sns.heatmap(positional_encoding.squeeze().to(\"cpu\"))\n",
    "plt.xlabel(\"i\")\n",
    "plt.ylabel(\"pos\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "y = positional_encoding.to(\"cpu\").numpy()\n",
    "plt.plot(np.arange(seq_len), y[0, :, 0 : 64 : 8], \".\")\n",
    "plt.legend([\"dim %d\" % p for p in [0, 7, 15, 31, 63]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 编码器\n",
    "  \n",
    "Transformer的编码器能够独立完成一些主流自然语言处理任务, 如情感分类、语义关系分析、命名实体识别等。Transformer编码器单元结构：\n",
    "\n",
    "![](./img/encoder.jpg)\n",
    "\n",
    "### 4.1 自注意力机制\n",
    "\n",
    "![](./img/attention_0.jpg)\n",
    "\n",
    "![](./img/attention_1.jpg)\n",
    "\n",
    "![](./img/attention_2.jpg)\n",
    "\n",
    "* **$\\sqrt{d_{k}}$归一化**  \n",
    "  \n",
    "假设$\\mathbf{q}$、$\\mathbf{k}$为均值0、方差1的独立随机变量，其点积注意力$\\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d_k}{\\mathbf{q}_{i} \\mathbf{k}_{i}}$的均值和方差分别为0、$d_{k}$。通过$\\sqrt{d_{k}}$缩放，使softmax结果更稳定（防止单词-单词间的点积注意力差异太大），便于反向传播时梯度平衡。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"\"\"\n",
    "    克隆基本单元，克隆的单元之间参数不共享\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([\n",
    "        copy.deepcopy(module) for _ in range(N)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention（方程（4））\n",
    "    \"\"\"\n",
    "    # q、k、v向量长度为d_k\n",
    "    d_k = query.size(-1)\n",
    "    # 矩阵乘法实现q、k点积注意力，sqrt(d_k)归一化\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    # 注意力掩码机制\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "    # 注意力矩阵softmax归一化\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    # dropout\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    # 注意力对v加权\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention（编码器（2））\n",
    "    \"\"\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        \"\"\"\n",
    "        `h`：注意力头的数量\n",
    "        `d_model`：词向量维数\n",
    "        \"\"\"\n",
    "        # 确保整除\n",
    "        assert d_model % h == 0\n",
    "        # q、k、v向量维数\n",
    "        self.d_k = d_model // h\n",
    "        # 头的数量\n",
    "        self.h = h\n",
    "        # WQ、WK、WV矩阵及多头注意力拼接变换矩阵WO\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        # 批次大小\n",
    "        nbatches = query.size(0)\n",
    "        # WQ、WK、WV分别对词向量线性变换，并将结果拆成h块\n",
    "        query, key, value = [\n",
    "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for l, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        # 注意力加权\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        # 多头注意力加权拼接\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        # 对多头注意力加权拼接结果线性变换\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 层归一化、残差连接\n",
    "\n",
    "* **层归一化**\n",
    "\n",
    "层归一化将神经网络中隐藏层归一化为标准正态分布，以加快训练速度\n",
    "\n",
    "![](./img/layer_norm.jpg)\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mu^{l} & = \\frac{1}{H} \\sum_{i = 1}^{H} x_{i}^{l} \\\\  \n",
    "\\sigma^{l} & = \\sqrt{\\frac{1}{H} \\sum_{i = 1}^{H} (x_{i}^{l} - \\mu^{l})^{2}} \\\\\n",
    "\\text{LayerNorm}(\\mathbf{x}^{l}) & = \\alpha \\odot \\frac{\\mathbf{x}^{l} - \\mu^{l}}{\\sqrt{(\\sigma^{l})^{2} + \\epsilon}} + \\beta\n",
    "\\end{aligned} \\tag{5}$$\n",
    "\n",
    "其中，$\\odot$表示Hadamard积，即两个向量对应元素相乘。$\\alpha$、$\\beta$为可训练参数，以弥补归一化过程损失信息。通常$\\alpha$、$\\beta$分别初始化为1、0。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # α、β分别初始化为1、0\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        # 平滑项\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 沿词向量方向计算均值和方差\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        # 沿词向量和语句序列方向计算均值和方差\n",
    "        # mean = x.mean(dim=[-2, -1], keepdim=True)\n",
    "        # std = x.std(dim=[-2, -1], keepdim=True)\n",
    "        # 归一化\n",
    "        x = (x - mean) / torch.sqrt(std ** 2 + self.eps)\n",
    "        return self.a_2 * x + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **残差连接**\n",
    "  \n",
    "残差连接：训练时，梯度通过捷径直接反向传播至前层\n",
    "\n",
    "$$\\mathbf{x} + \\text{SubLayer}(\\mathbf{x}) \\tag{6}$$\n",
    "\n",
    "其中，$\\text{SubLayer}$表示`Add & Norm`前层模块，如`Multi-Head Attention`、`Feed Forward`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    通过层归一化和残差连接，连接Multi-Head Attention和Feed Forward\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # 层归一化\n",
    "        x_ = self.norm(x)\n",
    "        x_ = sublayer(x_)\n",
    "        x_ = self.dropout(x_)\n",
    "        # 残差连接\n",
    "        return x + x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 前馈网络\n",
    "\n",
    "前馈网络（Feed Forward）为两层线性映射及激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.w_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Transformer编码器整体结构\n",
    "\n",
    "Transformer编码器基本单元由两个子层组成：第一个子层实现多头**自注意力（self-attention）**机制（Multi-Head Attention）；第二个子层实现全连接前馈网络。计算过程如下：\n",
    "  \n",
    "1. **词向量与位置编码**\n",
    "\n",
    "$$X = \\text{EmbeddingLookup}(X) + \\text{PositionalEncoding} \\tag{2}$$\n",
    "\n",
    "$$X \\in \\mathbb{R}^{\\text{batch_size} \\times \\text{seq_len} \\times \\text{embedding_dim}}$$\n",
    "\n",
    "2. **自注意力机制**\n",
    "\n",
    "$$Q = \\text{Linear}(X) = X W_{Q}$$\n",
    "\n",
    "$$K = \\text{Linear}(X) = XW_{K} \\tag{3}$$\n",
    "\n",
    "$$V = \\text{Linear}(X) = XW_{V}$$\n",
    "\n",
    "$$X_{\\text{attention}} = \\text{SelfAttention}(Q, K, V) \\tag{4}$$\n",
    "\n",
    "3. **层归一化、残差连接**\n",
    "\n",
    "$$X_{\\text{attention}} = \\text{LayerNorm}(X_{\\text{attention}}) \\tag{5}$$\n",
    "\n",
    "$$X_{\\text{attention}} = X + X_{\\text{attention}} \\tag{6}$$  \n",
    "\n",
    "4. **前馈网络**\n",
    "\n",
    "$$X_{\\text{hidden}} = \\text{Linear}(\\text{Activate}(\\text{Linear}(X_{\\text{attention}}))) \\tag{7}$$\n",
    "\n",
    "5. **层归一化、残差连接**\n",
    "\n",
    "$$X_{\\text{hidden}} = \\text{LayerNorm}(X_{\\text{hidden}})$$\n",
    "\n",
    "$$X_{\\text{hidden}} = X_{\\text{attention}} + X_{\\text{hidden}}$$\n",
    "\n",
    "$$X_{\\text{hidden}} \\in \\mathbb{R}^{\\text{batch_size} \\times \\text{seq_len} \\times \\text{embedding_dim}}$$\n",
    "\n",
    "Transformer编码器由$N = 6$个编码器基本单元组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        # SublayerConnection作用连接multi和ffn\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        # d_model\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 将embedding层进行Multi head Attention\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        # attn的结果直接作为下一层输入\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        \"\"\"\n",
    "        layer = EncoderLayer\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # 复制N个编码器基本单元\n",
    "        self.layers = clones(layer, N)\n",
    "        # 层归一化\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        循环编码器基本单元N次\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 解码器\n",
    "\n",
    "![](./img/decoder.jpg)\n",
    "\n",
    "解码器同样由$N$层解码器基本单元堆叠而成，与编码器基本单元不同的是：在编码器基本单元的多头**自注意力**机制及前馈网络之间插入一个**上下文注意力（context-attention）**机制（Multi-Head Attention）层，用解码器基本单元的自注意力机制输出作为$q$查询编码器的输出，以便解码时，解码器获得编码器的所有输出，即上下文注意力机制的$K$、$V$来自编码器的输出，$Q$来自解码器前一时刻的输出。\n",
    "\n",
    "编码器和解码器注意力的区别：\n",
    "\n",
    "![](./img/attention.png)\n",
    "\n",
    "解码器基本单元的输入、输出：\n",
    "\n",
    "* 输入：编码器的输出、解码器前一时刻的输出\n",
    "\n",
    "* 输出：对应当前时刻输出单词的概率分布\n",
    "\n",
    "此外，解码器的输出（最后一个解码器基本单元的输出）需要经线性变换和softmax函数映射为下一时刻预测单词的概率分布。  \n",
    "\n",
    "解码器**解码过程**：给定编码器输出（编码器输入语句所有单词的词向量）和解码器前一时刻输出（单词），预测当前时刻单词的概率分布。\n",
    "\n",
    "注意：训练过程中，编、解码器均可以并行计算（训练语料中已知前一时刻单词）；推理过程中，编码器可以并行计算，解码器需要像RNN一样依次预测输出单词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        循环解码器基本单元N次\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        # 自注意力机制\n",
    "        self.self_attn = self_attn\n",
    "        # 上下文注意力机制\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # memory为编码器输出隐表示\n",
    "        m = memory\n",
    "        # 自注意力机制，q、k、v均来自解码器隐表示\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # 上下文注意力机制：q为来自解码器隐表示，而k、v为编码器隐表示\n",
    "        x = self.sublayer[1](x, lambda x: self.self_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    解码器输出经线性变换和softmax函数映射为下一时刻预测单词的概率分布\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        # decode后的结果，先进入一个全连接层变为词典大小的向量\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 然后再进行log_softmax操作(在softmax结果上再做多一次log运算)\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 注意力掩码机制\n",
    "\n",
    "### 6.1 编码器注意力掩码\n",
    "\n",
    "编码器注意力掩码的目地：使批次中较短语句的填充部分不参与注意力计算。\n",
    "  \n",
    "![](./img/attention_mask2.jpg)\n",
    "\n",
    "模型训练通常按批次进行，同一批次中的语句长度可能不同，因此需要按语句最大长度对短语句进行0填充以补齐长度。语句填充部分属于无效信息，不应参与前向传播，考虑softmax函数特性，\n",
    "\n",
    "$$\\text{softmax}(\\mathbf {z})_{i} = {\\frac{\\exp(z_{i})}{\\sum _{j = 1}^{K} \\exp(z_{j})}}$$\n",
    "\n",
    "当$z_{i}$为填充时，可令$z_{i} = - \\infty$（一般取很大的负数）使其无效。\n",
    "\n",
    "$$z_{\\text{pad}} = - \\infty, \\exp(z_{\\text{pad}}) = 0$$\n",
    "\n",
    "![](./img/attention_mask.jpg)\n",
    "\n",
    "编码器注意力掩码生成伪代码：\n",
    "\n",
    "```python\n",
    "# True表示有效；False表示无效（填充位）\n",
    "mask = src != pad\n",
    "# 将无效位置为负无穷\n",
    "scores = scores.masked_fill(~mask, -1e9)\n",
    "```\n",
    "\n",
    "### 6.2 解码器注意力掩码\n",
    "\n",
    "\n",
    "解码器注意力掩码相对于编码器略微复杂，不仅需要将填充部分屏蔽掉，还需要对当前及后续序列进行屏蔽（`subsequent_mask`），即解码器在预测当前时刻单词时，不能知道当前及后续单词内容，因此注意力掩码需要将当前时刻之后的注意力分数全部置为$- \\infty$，然后再计算$softmax$，防止发生数据泄露。\n",
    "\n",
    "subsequent_mask的矩阵形式为一个下三角矩阵，在主对角线右上位置全部为False\n",
    "\n",
    "![](./img/subsequent_mask.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    # 设定subsequent_mask矩阵的shape\n",
    "    attn_shape = (1, size, size)\n",
    "    # 生成一个右上角(不含主对角线)为全1，左下角(含主对角线)为全0的subsequent_mask矩阵\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    # 返回一个右上角(不含主对角线)为全False，左下角(含主对角线)为全True的subsequent_mask矩阵\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(subsequent_mask(20)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"\n",
    "    批次类\n",
    "        1. 输入序列（源）\n",
    "        2. 输出序列（目标）\n",
    "        3. 构造掩码\n",
    "    \"\"\"\n",
    "    def __init__(self, src, trg=None, pad=PAD):\n",
    "        # 将输入、输出单词id表示的数据规范成整数类型\n",
    "        src = torch.from_numpy(src).to(DEVICE).long()\n",
    "        trg = torch.from_numpy(trg).to(DEVICE).long()\n",
    "        self.src = src\n",
    "        # 对于当前输入的语句非空部分进行判断，bool序列\n",
    "        # 并在seq length前面增加一维，形成维度为 1×seq length 的矩阵\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        # 如果输出目标不为空，则需要对解码器使用的目标语句进行掩码\n",
    "        if trg is not None:\n",
    "            # 解码器使用的目标输入部分\n",
    "            self.trg = trg[:, : -1]\n",
    "            # 解码器训练时应预测输出的目标结果\n",
    "            self.trg_y = trg[:, 1 :]\n",
    "            # 将目标输入部分进行注意力掩码\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "            # 将应输出的目标结果中实际的词数进行统计\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    # 掩码操作\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transformer模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # encoder的结果作为decoder的memory参数传入，进行decode\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h = 8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    # 实例化Attention对象\n",
    "    attn = MultiHeadedAttention(h, d_model).to(DEVICE)\n",
    "    # 实例化FeedForward对象\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout).to(DEVICE)\n",
    "    # 实例化PositionalEncoding对象\n",
    "    position = PositionalEncoding(d_model, dropout).to(DEVICE)\n",
    "    # 实例化Transformer模型对象\n",
    "    model = Transformer(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout).to(DEVICE), N).to(DEVICE),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout).to(DEVICE), N).to(DEVICE),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab).to(DEVICE), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab).to(DEVICE), c(position)),\n",
    "        Generator(d_model, tgt_vocab)).to(DEVICE)\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            # 这里初始化采用的是nn.init.xavier_uniform\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 模型训练\n",
    "\n",
    "**标签平滑**\n",
    "\n",
    "训练过程中，采用KL散度损失实现标签平滑（$\\epsilon_{ls} = 0.1$）策略，提高模型鲁棒性、准确性和BLEU分数。\n",
    "\n",
    "标签平滑：输出概率分布由one-hot方式转为真实标签的概率置为`confidence`，其它所有非真实标签概率平分`1 - confidence`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    标签平滑\n",
    "    \"\"\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label smoothing的例子\n",
    "crit = LabelSmoothing(5, 0, 0.4)  # 设定一个ϵ=0.4\n",
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]])\n",
    "v = crit(Variable(predict.log()), \n",
    "         Variable(torch.LongTensor([2, 1, 0])))\n",
    "\n",
    "# Show the target distributions expected by the system.\n",
    "print(crit.true_dist)\n",
    "plt.imshow(crit.true_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**计算损失**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"\n",
    "    简单的计算损失和进行参数反向传播更新训练的函数\n",
    "    \"\"\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.data.item() * norm.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**优化器**\n",
    "\n",
    "Adam优化器，$\\beta_1=0.9、\\beta_2=0.98$ 和 $\\epsilon = 10^{−9}$，并使用warmup策略调整学习率：  \n",
    "  \n",
    "$$ lr = d_{\\text{model}}^{−0.5} \\min(\\text{step_num}^{−0.5}, \\text{step_num} \\times \\text{warmup_steps}^{−1.5})$$  \n",
    "\n",
    "使用固定步数$\\text{warmup_steps}$**先使学习率的线性增长（热身）**，而后随着$\\text{step_num}$的增加以$\\text{step_num}$的反平方根成比例**逐渐减小学习率**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要调节是在 $rate$ 这个函数中，其中\n",
    "- $model\\_size$ 即为 $d_{model}$\n",
    "- $warmup$ 即为 $warmup\\_steps$\n",
    "- $factor$ 可以理解为初始的学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下对该优化器在**不同模型大小（$model\\_size$）**和**不同超参数（$marmup$）值**的情况下的学习率（$lrate$）曲线进行示例。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练迭代**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们创建一个通用的训练和评分功能来跟踪损失。 我们传入一个上面定义的损失计算函数，它也处理参数更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data, model, loss_compute, epoch):\n",
    "    start = time.time()\n",
    "    total_tokens = 0.\n",
    "    total_loss = 0.\n",
    "    tokens = 0.\n",
    "\n",
    "    for i , batch in enumerate(data):\n",
    "        out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch %d Batch: %d Loss: %f Tokens per Sec: %fs\" % (epoch, i - 1, loss / batch.ntokens, (tokens.float() / elapsed / 1000.)))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "def train(data, model, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    训练并保存模型\n",
    "    \"\"\"\n",
    "    # 初始化模型在dev集上的最优Loss为一个较大值\n",
    "    best_dev_loss = 1e5\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # 模型训练\n",
    "        model.train()\n",
    "        run_epoch(data.train_data, model, SimpleLossCompute(model.generator, criterion, optimizer), epoch)\n",
    "        model.eval()\n",
    "\n",
    "        # 在dev集上进行loss评估\n",
    "        print('>>>>> Evaluate')\n",
    "        dev_loss = run_epoch(data.dev_data, model, SimpleLossCompute(model.generator, criterion, None), epoch)\n",
    "        print('<<<<< Evaluate loss: %f' % dev_loss)\n",
    "        \n",
    "        # 如果当前epoch的模型在dev集上的loss优于之前记录的最优loss则保存当前模型，并更新最优loss值\n",
    "        if dev_loss < best_dev_loss:\n",
    "            torch.save(model.state_dict(), SAVE_FILE)\n",
    "            best_dev_loss = dev_loss\n",
    "            print('****** Save model done... ******')       \n",
    "            \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "data = PrepareData(TRAIN_FILE, DEV_FILE)\n",
    "src_vocab = len(data.en_word_dict)\n",
    "tgt_vocab = len(data.cn_word_dict)\n",
    "print(\"src_vocab %d\" % src_vocab)\n",
    "print(\"tgt_vocab %d\" % tgt_vocab)\n",
    "\n",
    "# 初始化模型\n",
    "model = make_model(\n",
    "                    src_vocab, \n",
    "                    tgt_vocab, \n",
    "                    LAYERS, \n",
    "                    D_MODEL, \n",
    "                    D_FF,\n",
    "                    H_NUM,\n",
    "                    DROPOUT\n",
    "                )\n",
    "\n",
    "# 训练\n",
    "print(\">>>>>>> start train\")\n",
    "train_start = time.time()\n",
    "criterion = LabelSmoothing(tgt_vocab, padding_idx = 0, smoothing= 0.0)\n",
    "optimizer = NoamOpt(D_MODEL, 1, 2000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9,0.98), eps=1e-9))\n",
    "\n",
    "train(data, model, criterion, optimizer)\n",
    "print(f\"<<<<<<< finished train, cost {time.time()-train_start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 模型预测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    传入一个训练好的模型，对指定数据进行预测\n",
    "    \"\"\"\n",
    "    # 先用encoder进行encode\n",
    "    memory = model.encode(src, src_mask)\n",
    "    # 初始化预测内容为1×1的tensor，填入开始符('BOS')的id，并将type设置为输入数据类型(LongTensor)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    # 遍历输出的长度下标\n",
    "    for i in range(max_len-1):\n",
    "        # decode得到隐层表示\n",
    "        out = model.decode(memory, \n",
    "                           src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        # 将隐藏表示转为对词典各词的log_softmax概率分布表示\n",
    "        prob = model.generator(out[:, -1])\n",
    "        # 获取当前位置最大概率的预测词id\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        # 将当前位置预测的字符id与之前的预测内容拼接起来\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "\n",
    "def evaluate(data, model):\n",
    "    \"\"\"\n",
    "    在data上用训练好的模型进行预测，打印模型翻译结果\n",
    "    \"\"\"\n",
    "    # 梯度清零\n",
    "    with torch.no_grad():\n",
    "        # 在data的英文数据长度上遍历下标\n",
    "        for i in range(len(data.dev_en)):\n",
    "            # 打印待翻译的英文语句\n",
    "            en_sent = \" \".join([data.en_index_dict[w] for w in data.dev_en[i]])\n",
    "            print(\"\\n\" + en_sent)\n",
    "            \n",
    "            # 打印对应的中文语句答案\n",
    "            cn_sent = \" \".join([data.cn_index_dict[w] for w in data.dev_cn[i]])\n",
    "            print(\"\".join(cn_sent))\n",
    "            \n",
    "            # 将当前以单词id表示的英文语句数据转为tensor，并放如DEVICE中\n",
    "            src = torch.from_numpy(np.array(data.dev_en[i])).long().to(DEVICE)\n",
    "            # 增加一维\n",
    "            src = src.unsqueeze(0)\n",
    "            # 设置attention mask\n",
    "            src_mask = (src != 0).unsqueeze(-2)\n",
    "            # 用训练好的模型进行decode预测\n",
    "            out = greedy_decode(model, src, src_mask, max_len=MAX_LENGTH, start_symbol=data.cn_word_dict[\"BOS\"])\n",
    "            # 初始化一个用于存放模型翻译结果语句单词的列表\n",
    "            translation = []\n",
    "            # 遍历翻译输出字符的下标（注意：开始符\"BOS\"的索引0不遍历）\n",
    "            for j in range(1, out.size(1)):\n",
    "                # 获取当前下标的输出字符\n",
    "                sym = data.cn_index_dict[out[0, j].item()]\n",
    "                # 如果输出字符不为'EOS'终止符，则添加到当前语句的翻译结果列表\n",
    "                if sym != 'EOS':\n",
    "                    translation.append(sym)\n",
    "                # 否则终止遍历\n",
    "                else:\n",
    "                    break\n",
    "            # 打印模型翻译输出的中文语句结果\n",
    "            print(\"translation: %s\" % \" \".join(translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "# 加载模型\n",
    "model.load_state_dict(torch.load(SAVE_FILE))\n",
    "# 开始预测\n",
    "print(\">>>>>>> start evaluate\")\n",
    "evaluate_start  = time.time()\n",
    "evaluate(data, model)\n",
    "print(f\"<<<<<<< finished evaluate, cost {time.time()-evaluate_start:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
